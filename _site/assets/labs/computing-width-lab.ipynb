{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Today, we're going to calculate rates of convergence for our\n",
    "nonparametric regression estimators. In the current homework, we're\n",
    "tackling a few cases with a pen-and-paper approach. Today, we're going\n",
    "to do it computationally for Monotone Regression and Bounded Variation Regression. \n",
    "If you like, you should be able to replicate the approach to tackle lipschitz regression,\n",
    "convex regression, sobolev regression, or anything else we've implemented. \n",
    "Width calculation code just isn't that different from least squares code, so\n",
    "it's easy to swap in new models we've already got regression code for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "library(tidyverse)\n",
    "library(CVXR)\n",
    "})\n",
    "CVXR::add_to_solver_blacklist('OSQP')\n",
    "\n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t\t\t\t\tpanel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\tpanel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t      panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Recall what we've shown about the relationship between error bounds and\n",
    "the gaussian width of neighborhoods in our model. Letting $\\mu^\\star$ be\n",
    "the best approximation to $\\mu$ in our model $\\mathcal{M}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\lVert \\hat \\mu - \\mu^\\star \\rVert_{L_2(P_n)} < s + 2\\sqrt{\\frac{2\\{1+2\\log(2n)\\}}{\\delta n}} &\\ \\ \\ \\text{ w.p. \\ $1-\\delta$ \\ if } \\ \\ \\  s^2 \\ge 2\\sigma \\text{w}(\\mathcal{M}_s) \\\\\n",
    "\\text{where} & \\ \\mathcal{M}_s = \\left\\{ m \\in \\mathcal{M}  \\ : \\ \\lVert m - \\mu^\\star \\rVert_{L_2(P_n)} \\le s \\right\\} \\\\ \n",
    "\\text{and}   & \\ \\text{w}(\\mathcal{V}) = \\operatorname{E} \\max_{v \\in \\mathcal{V}}\\frac{1}{n}\\sum_{i=1}^n g_i v_i \\quad \\text{ for a gaussian vector } \\ g \\sim N(0, I). \n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Essential Approach\n",
    "\n",
    "The reason we're bothering with gaussian width at all is that linear\n",
    "combinations of gaussians concentrate around their mean, so with high\n",
    "probability, gaussian width and 'sample gaussian width' are close.\n",
    "\n",
    "$$ \n",
    "\\hat{\\text{w}}(\\mathcal{V}) := \\max_{v \\in \\mathcal{V}}\\frac{1}{n}\\sum_{i=1}^n g_i v_i \\quad \\text{ satisfies } \\quad \\hat{\\text{w}}(\\mathcal{V}) \\approx \\text{w}(\\mathcal{V}). \n",
    "$$\n",
    "\n",
    "This means that if we want to find a radius $s$ satisfying our\n",
    "inequality above, we can more or less just find a solution to the\n",
    "inequality $s^2 \\ge 2 \\sigma \\hat{\\text{w}}(\\mathcal{M}_s)$. And\n",
    "because we want the best error bound we can get, we want the smallest\n",
    "$s$ satisfying this. Or something close to it.\n",
    "\n",
    "If we can maximize the linear function $l(v) = g^T v$ over vectors in\n",
    "the set $\\mathcal{M}_s$ efficiently, it's easy to do this. We make\n",
    "a list of values of $s$ to try, sample a gaussian vector\n",
    "$g \\sim N(0,1)$, calculate\n",
    "$\\hat{\\text{w}}(\\mathcal{M}_s)=\\max_{v \\in \\mathcal{M}_s} g^T v/n$\n",
    "at each $s$ in our list, and choose the smallest for which\n",
    "$s^2 \\ge 2\\sigma \\hat{\\text{w}}(\\mathcal{M}_s)$. To visualize\n",
    "what's going on, we can plot the curves $f(s)=s^2$ and\n",
    "$g(s)=2\\sigma \\hat{\\text{w}}(\\mathcal{M}_s)$ and look for the\n",
    "point $s$ at which they cross. We saw one of these plots at the end of\n",
    "our last lecture and today you'll be making your own.\n",
    "\n",
    "**Caveat**. $\\hat{\\text{w}}(\\mathcal{V}) \\approx \\text{w}(\\mathcal{V})$ in large samples essentially because that's when sample averages are close to expected values. To get a better estimate of $\\text{w}(\\mathcal{V})$ in small samples, we can average multiple copies of $\\hat{\\text{w}}(\\mathcal{V})$, i.e. copies we get for multiple noise vectors $g$.\n",
    "\n",
    "# A First Pass\n",
    "\n",
    "## Calculating Width\n",
    "\n",
    "We're going to do this for monotone regression. This will take less work\n",
    "than you might think. Let's compare what the function `monotonereg` we\n",
    "wrote a while back does to what we need to do now.\n",
    "\n",
    "$$ \n",
    "\\hat\\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m } \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i) \\}^2  \\quad \\text{ vs. } \\quad \n",
    "  \\hat{\\text{w}}(\\mathcal{M}_s) = \\max_{\\substack{\\text{increasing} \\ m \\\\ \\lVert m \\rVert_{L_2(P_n)} \\le s}} g^T m / n \n",
    "$$\n",
    "\n",
    "There are basically three differences. \n",
    "\n",
    "1. We're maximizing a linear function of $m$ instead of minimizing a quadratic one. \n",
    "2. We're interested in the *value* of the maximum instead of the *argument* at\n",
    "which it occurs. \n",
    "3. We have an additional constraint on the sample\n",
    "two-norm of our curve $m$. \n",
    "\n",
    "If you we make these changes to the monotone\n",
    "regression code we've been using, we're good. Here's what that looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "monotone           = function(m,X) { diff(m) >= 0 }\n",
    "bounded.variation  = function(m,X) { sum(abs(diff(m))) <= 1 }\n",
    "neighborhood.width = function(in.model, X, mustarX, s, g) {\n",
    "    # find the unique elements of X and the inverse mapping\n",
    "    unique.X = invert.unique(X)\n",
    "    uX = X[unique.X$elements]\n",
    "    n = length(X)\n",
    "  \n",
    "    # Step 1.\n",
    "    # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "    # and permute and duplicate these into a vector mX with n elements in correspondence with X_1 ... X_n \n",
    "    m = Variable(length(unique.X$elements))\n",
    "    mX = m[unique.X$inverse]\n",
    "\n",
    "    inner.product = sum(g * (mX-mustarX)) / n\n",
    "    neighborhood.constraint = sum((mX-mustarX)^2)/n <= s^2\n",
    "    model.constraint = in.model(m, uX) \n",
    "    constraints = list(model.constraint, neighborhood.constraint)\n",
    "\n",
    "    # solve and return the maximal value\n",
    "    solved = solve(Problem(Maximize(inner.product), constraints))\n",
    "    the.max = solved$value\n",
    "    the.argmax = solved$getValue(mX)\n",
    "    attr(the.max, 'argmax') = the.argmax \n",
    "    the.max\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will calculate $\\text{w}(\\mathcal{M}_s)$ of neighborhoods of $\\mu_\\star$ in both of our models.\n",
    "It'll do it for a number of radii $s$ and samples sizes $n$. \n",
    "\n",
    "This'll take a couple minutes to run. It takes about 8 minutes on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(2)\n",
    "sigma=1/4\n",
    "ns = c(2,25,50,100,200)\n",
    "radii=seq(0,1,length.out=40)\n",
    "mustar = function(x) { x } \n",
    "reps = 10\n",
    "models = list(bv=bounded.variation, monotone=monotone)\n",
    "\n",
    "\n",
    "X=runif(max(ns))\n",
    "g=sigma*rnorm(max(ns)*reps)\n",
    "dim(g) = c(max(ns), reps)\n",
    "grid = expand.grid(n=ns, s=radii, rep=1:reps, model=names(models))\n",
    "\n",
    "widths = grid |> pmap(function(n,s,rep,model) { \n",
    "  model = models[[model]]\n",
    "  model |> neighborhood.width(X[1:n], mustar(X[1:n]), s, g[1:n,rep])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Width\n",
    "\n",
    "To visualize the width calculation, you can run this code. \n",
    "\n",
    "In the background, you'll see:\n",
    "\n",
    "- The functions in the model, i.e. the pairs $\\{x=m(X_1), y=m(X_2)\\}$ for $m \\in \\mathcal{M}$, will be shown in blue. \n",
    "- The set of functions (pairs) within a distance $s$ of $\\mu_\\star$, will be shown in red.\n",
    "- The neighborhood $\\mathcal{M}_s$ will be where the red and blue regions overlap.\n",
    "\n",
    "In the foreground, you'll see:\n",
    "\n",
    "- The vector $2\\varepsilon$ in orange.\n",
    "- The function $m$ maximizing $\\langle \\varepsilon, m-\\mu_\\star \\rangle$ in blue.\n",
    "- A breakdown of $2\\varepsilon$ into components parallel and perpendicular to $m$.\n",
    "  - The orange dot on the dashed blue line indicates the parallel component.\n",
    "  - The dashed orange line shows the perpendicular component.\n",
    "\n",
    "There are sets of these arrows corresponding to two draws of our random vector $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x = seq(-10,10,length.out=1000)\n",
    "projection = function(epsilon, dm) { \n",
    "  epsilon.proj = dm * sum(dm*epsilon)/sum(dm^2)\n",
    "  epsilon.proj\n",
    "}\n",
    "model_geom = function(model) { \n",
    "  if(model == 'monotone') {\n",
    "    geom_ribbon(aes(x=x, ymin=x, ymax=Inf), \n",
    "                fill='blue', alpha=.1, color='lightgray')\n",
    "  } else { # it's bv\n",
    "    geom_ribbon(aes(x=x, ymin=x-1, ymax=x+1), \n",
    "                fill='blue', alpha=.1, color='lightgray')\n",
    "  }\n",
    "}\n",
    "plot.neighborhood.width = function(model, mustarX, argmaxes, gs, s) {\n",
    "  background = ggplot() + \n",
    "    model_geom(model) + \n",
    "    geom_point(aes(x=mustarX[1], y=mustarX[2]), color='black', alpha=.3) + \n",
    "    geom_polygon(aes(x=mustarX[1]+s*cos(pi*x), y=mustarX[2]+s*sin(pi*x)), \n",
    "                 fill='red', color='lightgray', alpha=.1, linewidth=.2)\n",
    "\n",
    "    arrow.style = arrow(length = unit(0.2, \"cm\"), ends='last', type='closed')\n",
    "    all.arrows = 1:ncol(argmaxes) |> map(function(rr) { \n",
    "      argmax = argmaxes[,rr]\n",
    "      g = 2*gs[,rr]  \n",
    "      pg = projection(g, argmax-mustarX) \n",
    "\n",
    "      arrows = list(\n",
    "        annotate('segment',\n",
    "          x=mustarX[1], xend=mustarX[1]+g[1], \n",
    "          y=mustarX[2], yend=mustarX[2]+g[2], \n",
    "          arrow = arrow.style, color='orange', linewidth=1, alpha=.5),\n",
    "        annotate('segment', \n",
    "          x=mustarX[1], xend=argmax[1], \n",
    "          y=mustarX[2], yend=argmax[2], \n",
    "          arrow = arrow.style, color='blue', linewidth=1, alpha=.5),\n",
    "        annotate('line',\n",
    "          x=mustarX[1] + x*(argmax[1]-mustarX[1]),\n",
    "          y=mustarX[2] + x*(argmax[2]-mustarX[2]),\n",
    "          color='blue', linetype='dashed', linewidth=.4, alpha=.5),\n",
    "        annotate('segment', \n",
    "          x=mustarX[1]+g[1], xend=mustarX[1] + pg[1], \n",
    "          y=mustarX[2]+g[2], yend=mustarX[2] + pg[2], \n",
    "          color='orange', linetype='dashed', linewidth=.4, alpha=.5),\n",
    "        annotate('point', \n",
    "          x=mustarX[1] + pg[1], \n",
    "          y=mustarX[2] + pg[2], \n",
    "          color='orange')\n",
    "      )\n",
    "    }) |> list_c()\n",
    "    background + all.arrows + labs(x='', y='') + ggtitle(model)\n",
    "}\n",
    "\n",
    "\n",
    "plot.width.at.radius = function(model, s) { \n",
    "  ws = which(grid$n==2 & grid$s==s & grid$model==model)\n",
    "  argmaxes = do.call(cbind, map(widths[ws], function(w) { attr(w, 'argmax') }))\n",
    "  reps = grid[ws,]$rep\n",
    "  rr = c(3,6)\n",
    "  coords = coord_cartesian(xlim=c(-2,2), ylim=c(-2,2))\n",
    "  plot.neighborhood.width(model, mustar(X)[1:2], \n",
    "                          argmaxes[1:2,rr, drop=FALSE],\n",
    "                          g[1:2,reps[rr],drop=FALSE], s*sqrt(2)) + coords  \n",
    "}\n",
    "\n",
    "plot.radii = radii[seq(2, length(radii), by=4)]\n",
    "for(model in  names(models)) { \n",
    "  for(s in plot.radii) { \n",
    "    print(model |> plot.width.at.radius(s))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossing Points and Error Bounds\n",
    "\n",
    "To visualize radius $s$ at which $s^2 \\ge 2\\sigma\\text{w}(\\mathcal{M}_s)$ and how it varies with sample size, you can run this.\n",
    "For each model and sample size $n$, it'll plot ...\n",
    "\n",
    "  - $f(s)=s^2$ in blue.\n",
    "  - $g(s)=2\\sigma\\text{w}(\\mathcal{M}_s)$ in red.\n",
    "\n",
    "\n",
    "The point $s_\\star$ at which these curves cross is the smallest $s$ for which $s^2 \\ge 2\\sigma\\text{w}(\\mathcal{M}_s)$.\n",
    "That's the best choice of $s$ to plug into our bound formula $\\lVert \\hat\\mu - \\mu_\\star \\rVert \\le s + \\sqrt{\\frac{2\\{1+2\\log(2n)\\}}{\\delta n}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "delta = .8\n",
    "\n",
    "crossing.plot = grid |>\n",
    "  add_column(w=unlist(widths)) |>\n",
    "  filter(grid$n > 2) |>\n",
    "  ggplot() +\n",
    "    geom_point(aes(x=s, y=2*sigma*w), alpha=.2, size=.5, color='red') + \n",
    "    stat_summary(aes(x=s, y=2*sigma*w), fun=mean, geom='line', color='red') +\n",
    "    geom_line(aes(x=s, y=s^2), color='blue') + \n",
    "    facet_grid(rows=vars(n), cols=vars(model))\n",
    "crossing.plot + coord_cartesian(xlim=c(0,.15), ylim=c(0,.025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these radii?  We can roughly calculate the crossing point by \n",
    "looking for the smallest radius $s$ for which $s^2 \\ge 2\\sigma\\text{w}(\\mathcal{M}_s)$\n",
    "*in the set of radii we've tried*. Having done this, we can plot the resulting probability $1-\\delta$ error bound $s+2\\sigma\\sqrt{\\frac{2\\{1+2\\log(2n)\\}}{\\delta n }}$ as a function of $n$.\n",
    "I've used $\\delta=.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summaries = grid |>\n",
    "  add_column(w=unlist(widths)) |>\n",
    "  group_by(n,s,model) |>\n",
    "  summarize(w=mean(w), .groups='drop')\n",
    "\n",
    "best.s = summaries |>\n",
    "  group_by(n,model) |>\n",
    "  summarize(s = min(s[s^2 >= 2*sigma*w]), .groups='drop')\n",
    "\n",
    "efron.stein = function(n,delta=.2) { sqrt(2*(1+2*log(2*n))/(delta*n)) }\n",
    "ggplot(best.s) +\n",
    "  geom_point(aes(x=n, y=s+2*sigma*efron.stein(n), color=model)) +\n",
    "  geom_line(aes(x=n,  y=s+2*sigma*efron.stein(n), color=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious, you can compare these bounds to the errors we see empirically, \n",
    "i.e. to error curves like the ones we drew in the convergence rates lab and subsequent homeworks. \n",
    "As a quick approximation to doing this, I'll estimate the rate $s \\propto n^{-b}$ at which these bounds\n",
    "decrease to zero. For both models, we get something like $n^{-1/3}$, like we did for the empirical errors earlier in the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for(modelname in names(models)) { \n",
    "  rate.model = nls(formula = s ~ a*n^(-b), \n",
    "                   start=list(a=1, b=1/2), \n",
    "                   data=best.s |> filter(model==modelname & n > 2))\n",
    "  print(model)\n",
    "  print(summary(rate.model))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost of Simplifying our Crossing Condition \n",
    "\n",
    "The bound we've been using, stated below, isn't quite the one we proved in lecture. \n",
    "$$\n",
    "\\lVert \\hat \\mu - \\mu \\rVert \\le s + 2\\sigma \\sqrt{\\frac{2\\{1+2\\log(2n)\\}}{\\delta n}} \n",
    "\\quad \\text{ for } \\quad s^2 \\ge 2\\sigma\\text{w}(\\mathcal{M}_s).\n",
    "$$\n",
    "What we did prove is this.\n",
    "$$ \n",
    "\\lVert \\hat\\mu - \\mu \\rVert \\le \\tilde s \\quad \\text{ for } \\quad \\tilde s^2 \\ge  \\textcolor{blue}{2\\sigma\\left\\{\\text{w}(\\mathcal{M}_{\\tilde s}) + \\tilde{s} \\sqrt{\\frac{2\\{1+2\\log(2n)\\}}{\\delta n}}\\right\\}}.\n",
    "$$\n",
    "That the second implies the first is a homework question. In the 'crossing plot' below, I've added a dashed red line indicating \n",
    "the $\\textcolor{blue}{\\text{right side of the latter inequality}}$. I've also added a purple dashed line where $\\sqrt{\\frac{2\\log(1/\\delta)}{n}}$ replaces the square root factor on the right side. This is what we'd be working with if we'd used the [Borell-TIS](https://en.wikipedia.org/wiki/Borellâ€“TIS_inequality) inequality, which is specific to gaussian noise and a bit harder to prove, in place of Efron-Stein.^[For a quick survey of inequalities like this, see [this post](https://terrytao.wordpress.com/2010/01/03/254a-notes-1-concentration-of-measure/) on Terry Tao's blog. The Borell-TIS inequality is a consequence of what Tao calls the\n",
    "*Gaussian concentration inequality for Lipschitz functions*. For a very detailed survey, see Boucheron, Lugosi, and Massart's book *Concentration inequalities: A nonasymptotic theory of independence*.] \n",
    "\n",
    "We can see that the crossing point $\\tilde s$ for the second bound tends to be well to the right of the one for $s$.\n",
    "The shift is, however, much smaller when we use the sharper Borell-TIS inequality in place of Efron-Stein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "borell.tis  = function(n,delta=.2) { sqrt(2*log(1/delta)/n) }\n",
    "\n",
    "crossing.plot + \n",
    "  stat_summary(aes(x=s, y=2*sigma*(w+s*efron.stein(n))) , \n",
    "    fun=mean, geom='line', linetype='dashed', color='red') +\n",
    "  stat_summary(aes(x=s, y=2*sigma*(w+s*borell.tis(n))), \n",
    "    fun=mean, geom='line', linetype='dashed', color='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the $\\tilde s$ bounds compare to the 'additive' $s + \\ldots$ bounds? Let's plot them as a function of $n$. What we see is that the $\\tilde s$ bounds tend to be substantially better.  \n",
    "\n",
    "**Something to think about.**  We use the 'additive version' of our bound because it makes things simpler. It lets us think about the impact of the model $\\mathcal{M}$ we're using and the probability $\\delta$ with which we tolerate out bound being invalid as two separate things. Is the cost we're paying necessary to break things down like this? Or did we not derive the best possible 'additive version', or more generally the best possible version that lets us break things down like this, in our homework exercise? If you get somewhere with this,\n",
    "let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "quietmin = function(x) { min(c(Inf, x)) }\n",
    "best.s = summaries |>\n",
    "  group_by(n,model) |>\n",
    "  summarize(efron.stein = quietmin(s[s^2 >= 2*sigma*(w+s*efron.stein(n))]),\n",
    "            borell.tis  = quietmin(s[s^2 >= 2*sigma*(w+s*borell.tis(n))]),\n",
    "            expected    = quietmin(s[s^2 >= 2*sigma*w]),\n",
    "            .groups = 'drop') |>\n",
    "  mutate(additive.efron.stein = s + 2*sigma*s*efron.stein(n,delta),\n",
    "         additive.borell.tis  = s + 2*sigma*s*borell.tis(n,delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "best.s |> pivot_longer(cols = c('expected', 'efron.stein', 'borell.tis', 'additive.efron.stein', 'additive.borell.tis'),\n",
    "                       names_to = 'type', values_to = 's') |>\n",
    "          filter(n > 2 & model == 'monotone' & is.finite(s)) |>\n",
    "  ggplot() +\n",
    "    geom_point(aes(x=n, y=s, color=type)) + \n",
    "    geom_line(aes(x=n, y=s, color=type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
