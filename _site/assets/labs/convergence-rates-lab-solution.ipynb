{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Folklore says that if we use more data, we'll make better estimates. In\n",
    "today's lab, we'll qualify and quantify this statement. We're going to\n",
    "look at how a few measures of the accuracy of our estimated curves vary\n",
    "as a function of sample size. And to summarize what we see, we'll\n",
    "estimate *rates of convergence*.\n",
    "\n",
    "We'll use a few libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "})\n",
    "\n",
    "# OSQP claims some feasible problems aren't, so we'll tell CVXR not to use it\n",
    "CVXR::add_to_solver_blacklist('OSQP')  \n",
    "\n",
    "# And we'll style our plots   \n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t\t\t\t\tpanel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\tpanel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t      panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll start by looking at data sampled around a few curves $\\mu$ on\n",
    "$[0,1]$.\n",
    "\n",
    "1.  A step, $\\mu(x) = 1(x \\ge .5)$.\n",
    "2.  A line, $\\mu(x)=x$.\n",
    "3.  A step into a line, $\\mu(x) = x 1(x \\ge .5)$.\n",
    "4.  A sine, $\\mu(x)=\\sin(\\pi x)$.\n",
    "\n",
    "Here's some code for evaluating our curves. We've put them in a list, so\n",
    "`mu$step(x)` is $\\mu(x)$ for the step function and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = list(step     = function(x) { 1*(x >= .5) },\n",
    "\t        line     = function(x) { x },\n",
    "\t        stepline = function(x) { x*(x >= .5) }, \n",
    "\t        sin      = function(x) { sin(pi*x) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll sample $X_i$ from the uniform distribution on $[0,1]$ and then\n",
    "sample $Y_i$ around $\\mu(X_i)$, taking $Y_i=\\mu(X_i) + \\varepsilon_i$\n",
    "where $\\varepsilon_i$ is independent of $X_i$ with mean zero. \n",
    "\n",
    "  - We'll start out by working with Gaussian noise. \n",
    "  - But if we have time, we'll also take a look at what changes when we have ...\n",
    "    - lighter-tailed uniformly-distributed noise \n",
    "    - heavier-tailed $t_3$-distributed noise \n",
    "\n",
    "    Both of these will be scaled to have the same standard deviation $\\sigma=\\sqrt{\\operatorname*{Var}{\\varepsilon_i}}$.\n",
    "\n",
    "Here's some code for sampling noise from these distributions. Again,\n",
    "we've put them in a list, but now that list is the output of a function\n",
    "of the noise standard deviation $\\sigma$, so `noise(.5)$gaussian(n)`\n",
    "gives us $n$ draws from the gaussian distribution with mean zero and\n",
    "standard deviation $.5$. We'll set our default choice for $\\sigma$\n",
    "to $.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#| label: noise-distributions\n",
    "noise = function(sigma) { \n",
    "    list(gaussian = function(n) { sigma*rnorm(n) },\n",
    "\t       uniform  = function(n) { sigma*sqrt(3)*runif(n,-1,1) },\n",
    "\t       t        = function(n) { (sigma/sqrt(3))*rt(n, 3) }) }\n",
    "\n",
    "sigma = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our curves and the data we're sampling around them\n",
    "with these three different noise distributions. There are, of course,\n",
    "differences from one noise distribution to another, but qualitatively\n",
    "things look pretty similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "X = runif(n)\n",
    "x = seq(0,1,by=.001)\n",
    "\n",
    "for(mm in 1:length(mu)) {\n",
    "  observations = purrr::map_dfr(noise(sigma), .id='noise', function(rnoise) { \n",
    "    data.frame(X=X, Y=mu[[mm]](X) + rnoise(n))\n",
    "  })   \n",
    "  curve = purrr::map_dfr(noise(sigma), .id='noise', function(rnoise) {\n",
    "    data.frame(x=x, y=mu[[mm]](x))\n",
    "  }) # we do this so faceting works: \n",
    "     # the values of mu(x) must be repeated for each noise distribution.\n",
    "  plt = ggplot() + \n",
    "          geom_point(aes(x=X, y=Y), alpha=.2, data=observations) + \n",
    "          geom_line(aes(x=x, y=y), data=curve) + \n",
    "          facet_grid(cols=vars(noise))\n",
    "  print(plt  + xlab('') + ylab(''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's hard to have intuition for how accurately we'll be able to\n",
    "estimate a curve in a quantitative sense, we get somewhere by asking a\n",
    "more qualitative question. We can ask whether, based on the observations\n",
    "$X_i,Y_i=\\mu(X_i)+\\varepsilon_i$, we can identify which of our four\n",
    "curves we're looking at. To get a sense of how accuracy in this sense\n",
    "relates to sample size, give this exercise a shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Try varying sample size $n$ in the code above. Based on what you see, at\n",
    "roughly what sample size do you start getting a clear sense of the shape\n",
    "of the curve $\\mu(x)$ from looking at the data? You don't have to be all\n",
    "that precise: try doubling sample sizes $n=50,100,200,400,\\ldots$. \n",
    "\n",
    "Your answer may vary by the curve $\\mu$ and the noise distribution. Briefly\n",
    "describe the trends. No need to make a big table for all curves and\n",
    "noise distributions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "I think it's around $n=100$. By $n=100$, I think I can tell the\n",
    "difference between all the curves except the step and the step-line. By\n",
    "$n=200$, I think I can see that difference as well. Things are a little\n",
    "easier with uniformly distributed noise, because I can see a sort of\n",
    "'tube' of data with edges that are more or less parallel to the curve\n",
    "$\\mu$, and maybe also with $t$-distributed noise, as there's a little\n",
    "more of it close to the curve $\\mu$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Now let's test your intuition. We'll display, in random order, plots\n",
    "like the ones above for our four different curves $\\mu(x)$. See if you\n",
    "can work out which is which.\n",
    "\n",
    "Run the block below to plot our four curves in random order with meaningless\n",
    "numbers 1-4 as labels. Write down which of our curves you think you're\n",
    "seeing for each number. Then, run the block below that, which will show\n",
    "the same plot with informative labels, to check your answers.\n",
    "\n",
    "Change the first block to try different values of $n$ and report whether\n",
    "your intuition from the last exercise was right. And repeat this for the\n",
    "three noise distributions. To do this, change 'gaussian' in\n",
    "`rnoise=noise(sigma)$gaussian` to 'uniform' or 't'.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "rnoise = noise(sigma)$gaussian\n",
    "X = runif(n)\n",
    " \n",
    "mu.order = sample(1:4)\n",
    "observations = mu.order |> map(function(mm) { \n",
    "  data.frame(X=X, Y=mu[[mm]](X)+rnoise(n), curve=mm)\n",
    "}) |> list_rbind()\n",
    "plt = ggplot() + geom_point(aes(x=X, y=Y,), alpha=.2, data=observations)\n",
    "plt + facet_wrap(~curve, nrow=2) + xlab('') + ylab('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "informative_labeller = function(factors) { list(names(mu))[factors$curve] }\n",
    "plt + facet_wrap(~curve, nrow=2, labeller=informative_labeller) + xlab('') + ylab('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "There's not really a right answer to this, but I'll give mine. \n",
    "\n",
    "\n",
    "With gaussian noise, I think $n=100$ was right, but just barely. \n",
    "  \n",
    "  - with $n=100$, I was able to label the plots correctly my first try, but not with a lot of\n",
    "confidence. \n",
    "  - $n=50$ was impossible and $n=200$ was pretty easy, although at $n=200$ I still wasn't sure about the step vs. step-line. \n",
    "I wasn't all that confident about the step vs. step-line. \n",
    "  - At $n=400$, even that was pretty easy.\n",
    "\n",
    "With t-distributed noise, it was a bit more possible to guess at $n=50$,\n",
    "but otherwise pretty similar to the gaussian case.\n",
    "\n",
    "For uniformly distributed noise, I could guess correctly although with\n",
    "not a lot of confidence in step vs. step-line at $n=50$, and by $n=200$\n",
    "everything was very easy.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten a sense of what sample sizes are required to\n",
    "visually distinguish our four curves, let's start fitting them using monotone regression. \n",
    "We'll see how our choice of model impacts our ability to estimate them accurately.\n",
    "\n",
    "In addition to linear regression, which chooses the best fitting curve\n",
    "from the set of all lines, and monotone regression, which chooses from\n",
    "the set of all increasing curves, we'll use a model-selection approach\n",
    "that plays the same guessing game you've been playing. Just like you,\n",
    "it'll choose from the set of four curves we're sampling our data around.\n",
    "This is, of course, unusable in practice; it's never going to be\n",
    "plausible that you know that the data's conditional mean\n",
    "$\\mu(x)=E[Y_i \\mid X_i=x]$ is one of a few curves you know a-priori. But\n",
    "comparing this to the other methods will give you a sense of how much\n",
    "harder the task of estimating a curve via something like monotone\n",
    "regression is than the task you were working on visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Code\n",
    "\n",
    "We'll be comparing linear regression to monotone regression, so you'll\n",
    "want monotone regression code. To minimize the time we wait for code to\n",
    "run, we'll use R's built-in implementation, `isoreg`, which uses a fast\n",
    "specialized algorithm called PAVA to solve the monotonicity-constrained\n",
    "least squares problem. I'll give you a code that wraps it so it behaves\n",
    "exactly like the function `monotonereg` we wrote in lab. I'll\n",
    "also give you an implementation of `selectionreg`, which selects one of\n",
    "our four curves as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg = function(X,Y, decreasing=FALSE) {\n",
    "  isoreg.model = if(decreasing) { isoreg(-X,Y) } else { isoreg(X,Y) }\n",
    "  mu.hat = array(dim=length(X))\n",
    "  mu.hat[isoreg.model$ord] = isoreg.model$yf\n",
    "  \n",
    "  model = list(X=X, mu.hat=mu.hat)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.piecewise.constant = function(model, newdata=data.frame(X=model$X)) {\n",
    "  increasing.order = order(model$X) \n",
    "  X = model$X[increasing.order]\n",
    "  mu.hat = model$mu.hat[increasing.order]\n",
    "  \n",
    "  # for each new data point x[k]\n",
    "  # find the closest observed X[i[k]] left of x[k]\n",
    "  # i.e., i[k] is the largest integer i for which X[i] <= x[k] \n",
    "  # this covers cases 1 and 2\n",
    "  i = findInterval(newdata$X, X) \n",
    "  # if there is no X[i] < x[k], findInterval sets i[k]=0\n",
    "  # to cover case 3, we want X[i] for i=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  mu.hat[i]\n",
    "}\n",
    "predict.monotonereg = predict.piecewise.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "selectionreg = function(X,Y) {\n",
    "  mses = sapply(mu, function(m) { mean((Y-m(X))^2) })\n",
    "  model = list(X=X, mu=names(mu)[which.min(mses)])\n",
    "  attr(model, 'class') = 'selectionreg'\n",
    "  model\n",
    "}\n",
    "predict.selectionreg = function(model, newdata=data.frame(X=model$X)) {\n",
    "  mu[[model$mu]](newdata$X)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, having our code's notation match our mathematical notation makes\n",
    "it easier to read. To help with that, here's a function that takes a\n",
    "model like we'd get by calling `lm`, `monotonereg`, or `selectionreg`\n",
    "and gives us back a function $\\hat\\mu$ that makes predictions\n",
    "$\\hat\\mu(x)$ using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prediction.function = function(model) { \n",
    "  function(x) { predict(model, newdata=data.frame(X=x)) } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X = runif(n)\n",
    "Y = mu$step(X) + noise(sigma)$gaussian(n)\n",
    "\n",
    "mu.hat = prediction.function(monotonereg(X,Y))\n",
    "mu.hat(c(0,.5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x=seq(0,1,by=.001)\n",
    "ggplot() + geom_point(aes(x=X, y=Y), alpha=.2) + \n",
    "           geom_line(aes(x=x, y=mu.hat(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squared Errors\n",
    "\n",
    "To get a sense of how sample size impacts the accuracy of our estimator,\n",
    "we'll vary sample size and track a few measures of its error. We'll look\n",
    "at three error measures.\n",
    "\n",
    "1.  Sample mean squared error, the estimator's typical level of error on\n",
    "    the sample used to fit the curve.\n",
    "    $$ \\lVert\\hat\\mu-\\mu\\rVert_{L_2(P_n)}^2\n",
    "                     = \\frac{1}{n}\\sum_{i=1}^n \\{ \\hat\\mu(X_i) - \\mu(X_i)\\}^2. $$\n",
    "\n",
    "2.  Population mean squared error, its typical level error on points\n",
    "    drawn independently from the same distribution as that sample. For\n",
    "    $\\tilde X$ independent of $X_1 \\ldots X_n$ with the same distribution,\n",
    "    $$ \\lVert\\hat\\mu-\\mu\\rVert_{L_2(P)}^2 \n",
    "                     = E\\left[ \\{ \\hat \\mu(\\tilde X) - \\mu(\\tilde X) \\}^2 \\mid X_1 \\ldots X_n\\right]. $$\n",
    "    \n",
    "    - Since we're sampling $X_i$ from the uniform distribution, that's\n",
    "      $\\int_0^1\\{ \\hat \\mu(x) - \\mu(x) \\}^2 dx$. We won't calculate this integral exactly.\n",
    "    - To approximate it, we'll use the simplest numerical integration scheme there is. We'll\n",
    "    just use the average of $\\{\\hat\\mu(x)-\\mu(x)\\}^2$ for an evenly-spaced grid of points $x \\in [0,1]$.\n",
    "\n",
    "3.  Squared error at a single point $x$,\n",
    "    $$\n",
    "    \\{ \\hat \\mu(x) - \\mu(x) \\}^2\n",
    "    $$. \n",
    "    We'll look at the point $x=0$ at the left edge of $X_i$'s distribution. \n",
    "     \n",
    "     - This is the sort of thing you wind up working with in RDD estimates.[^1]\n",
    "\n",
    "[^1]: Think of it like this. You've taken the data to the right of the\n",
    "    enrollment cutoff in our class size example (enrollment $> 40$),\n",
    "    defined $X_i$ by shifting and scaling enrollment so the cutoff\n",
    "    occurs at $x=0$ so the cutoff occurs at $x=0$ and all $X_i$ lie in\n",
    "    into the range $[0,1]$ ($X_i=(\\text{enrollment}_i-40)/40$), fit the\n",
    "    data, and are now evaluating it at $X_i=0$ to get the estimate\n",
    "    $\\hat \\mu_{\\text{right}}(40)$. We'll often want to scale the data\n",
    "    into the unit interval like this for other reasons, so this is a\n",
    "    more natural way of thinking about things than it may sound now.\n",
    "\n",
    "We'll do this for three estimators.\n",
    "\n",
    "1. the least squares line (we use the `R` built-in `lm`) \n",
    "2. the monotone regression estimator (we use the function `monotonereg` defined above)\n",
    "3. the estimator that selects one of our four choices of $\\mu$ (we use `selectionreg` defined\n",
    "above).\n",
    "\n",
    "This kind of code is kind of a pain to write, so I'm going to give you\n",
    "everything you need. But you'll be using this code in a few homeworks including this week's, so you may\n",
    "want to take a look at the last section of this notebook, where I describe how the code works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Run the code below to plot these three error measures at a range of\n",
    "sample sizes. As it is, it plots them at sample sizes\n",
    "$n \\in \\{25,50,100,200\\}$. Feel free to add some more. Briefly describe\n",
    "what you see. \n",
    "\n",
    "Later on, I'll ask more precise questions, so don't worry\n",
    "about being complete.\n",
    ":::\n",
    "\n",
    "\n",
    "Here, to start, is some code for computing our three error measures\n",
    "given a function $\\hat\\mu(x)$. Note that so we can substitute one for\n",
    "another in our code, they all take the sample $X=X_1 \\ldots X_n$ as an\n",
    "argument even though only sample MSE actually needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sample.mse     = function(mu.hat, mu, X) { \n",
    "  mean((mu.hat(X) - mu(X))^2) \n",
    "}\n",
    "population.mse = function(mu.hat, mu, X) { \n",
    "  x=seq(0,1,by=.001)\n",
    "  mean((mu.hat(x) - mu(x))^2) \n",
    "}   \n",
    "point.se = function(mu.hat, mu, X) { \n",
    "  (mu.hat(0) - mu(0))^2 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a code for making a table of these 3 errors, for our 3\n",
    "regression methods, at our 4 different sample sizes. Naturally, it has\n",
    "$3 \\times 3 \\times 4 = 36$ rows, one for each combination of these\n",
    "things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "errors = list(sample=sample.mse, population=population.mse, point=point.se) \n",
    "models = list(lines = function(X,Y) { lm(Y~X, data.frame(X=X,Y=Y)) },\n",
    "              monotone = monotonereg,\n",
    "              selection  = selectionreg)\n",
    "\n",
    "tabulate.errors = function(X,Y, mu, ns) { \n",
    "  map_dfr(ns, function(n) {\n",
    "    map_dfr(models, .id='model', function(model.fit) {\n",
    "\t    model = model.fit(X[1:n], Y[1:n])\n",
    "\t    map_dfr(errors, .id='error.measure', function(error) {\n",
    "        data.frame(n=n, error=error(prediction.function(model), mu, X[1:n]))\n",
    "\t    })\n",
    "    })\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll run it to produce tables for all combinations of the four curves\n",
    "$\\mu(x)$ and three noise distributions described above, then stack them all together. If you want, you can look at the\n",
    "table, but it's probably easier to plot what's in it and look at the\n",
    "plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ns = c(25,50,100,200)\n",
    "X = runif(max(ns))\n",
    "\n",
    "tab = map_dfr(mu, .id='mu', function(mu) {\n",
    "        map_dfr(noise(sigma), .id='noise', function(rnoise) {\n",
    "          Y = mu(X) + rnoise(length(X))\n",
    "          tabulate.errors(X, Y, mu, ns)\n",
    "        })\n",
    "    })\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's a plot of population and sample MSE. We'll just look at what\n",
    "happens with gaussian noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "noise.type = 'gaussian'\n",
    "noise.tab = tab[tab$noise==noise.type, ]\n",
    "\n",
    "ggplot(noise.tab[noise.tab$error.measure != 'point',], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "  geom_line() + geom_point() + facet_grid(cols=vars(mu), )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And here's a plot of squared error at the point $x=0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(noise.tab[noise.tab$error.measure == 'point', ], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "    geom_line() + geom_point() + facet_grid(cols=vars(mu), rows=vars(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "What jumps out at me first is that selection is easiest, then fitting a\n",
    "line, then fitting an increasing curve. By easier, I mean the errors are\n",
    "lower when the model can fit $\\mu$.\n",
    "\n",
    "Then, looking at sample and population mean squared error, what I see is\n",
    "that no matter what regression model we're using, error tends to improve\n",
    "as sample size does. At least up to a point. If we're trying to fit a\n",
    "step function with a line, error more or less stops improving in larger\n",
    "sample sizes because no line can fit the step. The same thing happens\n",
    "when we're trying to fit an increasing curve to the sine.\n",
    "\n",
    "I'm also seeing that sample and population MSE are usually pretty close,\n",
    "and that MSE at the point $x=0$ tends to be higher, and doesn't\n",
    "necessarily improve as sample size increases.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Ignore the squared error at the point $x=0$ for now. Answer these\n",
    "questions for the linear model, the the monotone (increasing) model, and\n",
    "the select one-of-four model. \n",
    "\n",
    "1. Is the qualitative behavior you see roughly the same for all curves? \n",
    "2. If not, for which curve or curves is it different? \n",
    "3. How are they different? Why?\n",
    "\n",
    "**Tip.** If you see different behaviors in the error plots but don't\n",
    "know why, plot the fitted curve $\\hat \\mu$ on top of the data and look\n",
    "at it.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "Here are my thoughts. For the linear model, I see two types of behavior\n",
    "depending on the curve $\\mu$. For the line, as you'd expect, everything\n",
    "goes well. Its error looks to be decreasing to zero as sample size\n",
    "increases. For the others, error tends to decrease a little at very\n",
    "small sample sizes, then stops. We're seeing this because no line fits\n",
    "these other curves; the line we're fitting is going to converge to the\n",
    "line that fits $\\mu$ best, but it's not going to fit very well.\n",
    "\n",
    "We see similar behaviors for monotone regression, but breakdown of\n",
    "curves $\\mu$ is different. Error decreases to zero for all but the\n",
    "$\\sin$ curve, as that is the only one that is not monotone. And for this\n",
    "$\\sin$ curve, the monotone fit's error decreases a bit further than the\n",
    "line's because there is a substantial portion of it that the monotone\n",
    "model can fit; $\\sin(\\pi x)$ is increasing on the interval $[0,1/2]$ but\n",
    "approximately linear only in relatively small intervals (i.e. it is\n",
    "differentiable, but that's it).\n",
    "\n",
    "For the select-one-of-four approach, we often get zero error outright.\n",
    "That is, when our computer plays the guessing game we played earlier,\n",
    "sometimes it guesses right, in which case all our errors will be zero.\n",
    "At smaller sample sizes it, just like us, can guess wrong---in that case\n",
    "the errors we see are distances between the selected curve $\\hat\\mu$ and\n",
    "the actual one $\\mu$.\n",
    "\n",
    "Let's take a look at what's going on. Note that because the data is\n",
    "random, we'll get different results each time we run this code. It might\n",
    "be helpful to run it a couple times to get a sense of what can happen\n",
    "and what's typical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n=50\n",
    "X=runif(n)\n",
    "x=seq(0,1,by=.001)\n",
    "\n",
    "fit.and.plot = function(mu) { \n",
    "  Y = mu(X) + noise(sigma)$gaussian(length(X))\n",
    "  prediction.geoms = pmap(list(models, names(models)), \n",
    "                          function(fit.model, model.name) { \n",
    "    muhat = fit.model(X, Y) |> prediction.function()\n",
    "    plot.data = data.frame(x=x, muhat.x=muhat(x), model=model.name)\n",
    "    geom_line(aes(x=x, y=muhat.x, color=model.name), linewidth=2, alpha=.3, data=plot.data)\n",
    "  })\n",
    "  ggplot() + geom_point(aes(x=X,y=Y), alpha=.2) + \n",
    "             geom_line(aes(x=x, y=mu(x)), linewidth=.5, alpha=1) +\n",
    "             prediction.geoms\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Here's what happens when we fit our three models to our four signals. One thing to notice \n",
    "is that our computer is pretty good at the select-one-of-four guessing game we played, so 'selection' gets \n",
    "zero error in a lot of random samples, especially for large $n$. For $n=25$, it makes errors now and then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "walk(mu, function(signal) {\n",
    "  fit.and.plot(signal) |> print()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-exercise}\n",
    "Again, ignore squared error at the point $x=0$. Is either sample or\n",
    "population mean squared error typically larger or is their relationship\n",
    "inconsistent? Why do you think that's the case?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "The following claim is inaccurate, but it's what I saw when I first ran\n",
    "my code, and I made up a reason for it. It may not match what you see\n",
    "when you run the code because you'll get a different random sample than\n",
    "I did. We'll see later in the semester that the claim is accurate\n",
    "sometimes; whether it is or not tends to depend on how much noise there\n",
    "is, i.e., on the noise standard deviation $\\sigma$.\n",
    "\n",
    "Here's the inaccurate claim. The population mean squared error tends to\n",
    "be a little larger. That's the case because the data doesn't really tell\n",
    "us anything about the value of $\\mu(x)$ off the sample $X_1 \\ldots X_n$.\n",
    "We're just filling in the gaps in a way we think is reasonable. It tends\n",
    "to work, especially as the sample gets big enough that the gaps between\n",
    "points are small, but we should expect to do at least slightly better on\n",
    "the sample, where we have more information.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Now let's think about why we've been ignoring squared error at $x=0$.\n",
    "The errors we have plotted are *random*. They depend on the sample\n",
    "$\\mathcal{S}=\\{(X_i,Y_i) : i \\le n\\}$ used to fit $\\hat\\mu$.[^2] Rerun\n",
    "the code in this section to draw a new sample and make your plots. Do it\n",
    "a few times. Is the qualitative behavior you've already described\n",
    "consistent from sample to sample? What about squared error at $x=0$? In\n",
    "which cases is it inconsistent? Why?\n",
    "\n",
    "\n",
    "**Tip.** Each time you resample, plot the fitted curves $\\hat \\mu$ on\n",
    "top of the data and look at them.\n",
    ":::\n",
    "\n",
    "[^2]: And, in the case of sample mean squared error, also more directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "What I've already described is consistent. Squared error at $x=0$ is\n",
    "inconsistent for the increasing model because, vaguely speaking, what\n",
    "the curve looks like at $x=0$ is sensitive to the leftmost couple\n",
    "points. This explains why it doesn't necessarily improve as sample size\n",
    "increases: the number of observations that influence the left endpoint\n",
    "effectively isn't increasing.\n",
    "\n",
    "To think precisely about this, suppose we're fitting mean zero noise. We\n",
    "know that, as we go away from zero, our fit curve is going to settle at\n",
    "$\\hat\\mu(x) \\approx 0$; it can't match the oscillations of the noise\n",
    "because it's increasing, so this is the best it can do to fit the\n",
    "majority of our observations. But if, entirely by chance, the first\n",
    "couple points are increasing, it'll fit them perfectly on its way toward\n",
    "that horizontal line. That reduces squared error relative to starting\n",
    "near the horizontal line and doesn't interfere with the increasingness\n",
    "constraint. If, on the other hand, they're decreasing, it'll more or\n",
    "less start on the horizontal line, as to try to fit them better would\n",
    "run afoul of the constraint. So we see fluctuations in $\\hat\\mu(X_1)$\n",
    "roughly on the scale of the noise, and therefore if we're using\n",
    "piecewise-constant interpolation for prediction, the same fluctuations\n",
    "in $\\hat\\mu(0)$. This is more or less what happens even though we're not\n",
    "actually fitting mean zero noise because, near $x=0$, we essentially\n",
    "are: because our curves $\\mu(x)$ are continuous at zero, the noise\n",
    "varies much more than $\\mu(x)$ does in a neighborhood of zero.\n",
    "\n",
    "On the other hand, squared error at $x=0$ is fairly consistent for the\n",
    "line, as for the line $\\hat \\mu(0)$ is determined not by the first\n",
    "couple points but by the data everywhere. Same deal for the\n",
    "select-one-of-four estimator. If we'd included two step functions, one\n",
    "jumping at jump at $.5$ and the other at $.499$, we'd find ourselves in\n",
    "a situation a little more like what we're seeing for the increasing\n",
    "estimator. Although as sample size got large enough, say on the order of\n",
    "$10,000$, things might still be a bit better for this estimator: we'd\n",
    "start having at least $10$ or so points between $.499$ to help us tell\n",
    "which one it is.\n",
    "\n",
    "I ran this a few times to look at the fitted increasing curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n=200\n",
    "X=runif(n)\n",
    "x=seq(0,1,by=.01)\n",
    "\n",
    "walk(mu, function(mu) {\n",
    "    Y = mu(X) + noise(sigma)$gaussian(length(X))\n",
    "    muhat = prediction.function(monotonereg(X,Y))\n",
    "    p = ggplot() + geom_point(aes(x=X,y=Y), alpha=.2) + \n",
    "                   geom_line(aes(x=x, y=muhat(x)), color='blue')\n",
    "    print(p)\n",
    "}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Expected Squared Error\n",
    "\n",
    "In this section, we're going to summarize the distribution of the mean\n",
    "squared errors we plotted in the previous one. Because each error is a\n",
    "function of the sample, $\\text{error}(\\mathcal{S})$, it makes sense to\n",
    "talk about its mean $\\text{E}[ \\text{error}(\\mathcal{S})]$ and its\n",
    "variance $\\text{Var}[ \\text{error}(\\mathcal{S})]$. We can, of course,\n",
    "estimate those by the sample mean and sample variance, \n",
    "\n",
    "$$\n",
    "\\hat{\\text{E}}[ \\text{error}(\\mathcal{S})] := \\frac{1}{R}\\sum_{r=1}^R \\text{error}(\\mathcal{S}_r)\n",
    "\\quad \\text{ and } \\quad\n",
    "\\widehat{\\text{Var}}[ \\text{error}(\\mathcal{S})] := \\frac{1}{R}\\sum_{r=1}^R \\left\\{\\text{error}(\\mathcal{S}_r) - \\hat{\\text{E}}[ \\text{error}(\\mathcal{S})] \\right\\}^2\n",
    "$$ \n",
    "\n",
    "based on independent replications\n",
    "$\\mathcal{S}_1 \\ldots \\mathcal{S}_R$ of the sample. And, as a\n",
    "back-of-the-envelope thing, you might think of\n",
    "$\\hat{\\text{E}}[\\text{error}(\\mathcal{S})] \\pm 2\\sqrt{\\widehat{\\text{Var}}[\\text{error}(\\mathcal{S})]}$\n",
    "as a 95% confidence interval for $\\text{E}[\\text{error}(\\mathcal{S})]$, i.e. an interval you might\n",
    "expect 95% of replications of $\\text{E}[\\text{error}(\\mathcal{S})]$ to be in.\n",
    "\n",
    "To show this distributional summary, we'll replicate the plots from\n",
    "section on squared errors above, with each point being the mean\n",
    "$\\hat{\\text{E}}[\\text{error}(\\mathcal{S})]$ over $R=100$ replications.\n",
    "Then, to give a sense of spread, we'll display our prediction interval\n",
    "by plotting error bars extending two estimated standard deviations\n",
    "$\\sqrt{\\widehat{\\text{Var}}[\\text{error}(\\mathcal{S})]}$ in each\n",
    "direction.\n",
    "\n",
    "First, we'll make a $R$ tables like we did before, one for each\n",
    "replication $\\mathcal{S}_r$ of our the dataset, and stack them. This\n",
    "takes is a bit slow, but it takes under a minute on my laptop. If it's\n",
    "too slow on yours, you can decrease the number of replications $R$ to\n",
    "reduce the runtime. This code should run for about $R$ times the amount\n",
    "of time it took to make the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "R=100\n",
    "rep.tab = map_dfr(1:R, function(rep) { \n",
    "            map_dfr(mu, .id='mu', function(mu) {\n",
    "              map_dfr(noise(sigma), .id='noise', function(rnoise) {\n",
    "                Y = mu(X) + rnoise(length(X))\n",
    "                tabulate.errors(X, Y, mu, ns)\n",
    "              })\n",
    "            })\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Then we'll do our plots. `ggplot`'s function `stat_summary` does a lot\n",
    "of the work for us, aggregating the data into the means and the upper\n",
    "and lower confidence bounds we need to plot error bars. Here's the code.\n",
    "We'll plot sample and population MSE together as we did before so\n",
    "they're easy to compare, but because the error bars make that plot a\n",
    "little crowded, we'll also plot them separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mse.with.errorbars.plot = function(tab) {\n",
    "  ggplot(tab, aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "    stat_summary(geom='line', fun=mean) +\n",
    "    stat_summary(geom='pointrange', fun=mean,\n",
    "     fun.min=function(x){ mean(x)-2*sd(x) }, \n",
    "\t\t fun.max=function(x){ mean(x)+2*sd(x) },\n",
    "\t\t position=position_dodge(20)) +\n",
    "     facet_grid(cols=vars(mu), rows=vars(noise)) \n",
    "}\n",
    "\n",
    "rep.noise.tab = rep.tab[rep.tab$noise==noise.type,]\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure=='sample',])\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure=='population',])\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure=='point',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Looking at these plots, revisit your answers from the previous part.\n",
    "Describe what you're seeing. \n",
    "\n",
    "- Maybe you got some answers that were true for the particular sample $\\mathcal{S}$ you were looking at, but aren't true on average over samples.\n",
    "- Maybe you got some answers that were true on average, but see a lot of variation from sample to sample.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "I buy most of my claims above and I'm not seeing all that much that's\n",
    "new. There is one I'd take back, but I've already acknowledged its\n",
    "inaccuracy above: the claim that sample MSE tends to be close to (true)\n",
    "but typically a bit lower than (not necessarily true) population MSE.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Rates of Convergence\n",
    "\n",
    "The plots of error vs. sample size in the sections above contain a lot of information. \n",
    "Some things are easy enough to see. It's pretty clear when errors aren't decreasing\n",
    "to zero because we're using a model that can't fit the curve $\\mu$.\n",
    "However, it can be hard to see more quantitative phenomena, like how\n",
    "much faster one model's errors are converging to zero than another's, as\n",
    "this involves behavior at a wide range of $n$. It's a scale problem. If\n",
    "you can see the curve at all for $n=50$, you're zoomed out far enough\n",
    "that you can't see what's going on for $n \\ge 200$. To summarize the\n",
    "curves in a way that can help make some of these relationships clear, we\n",
    "often use *rates of convergence*. We'll say something like $\\hat \\mu$\n",
    "converges to $\\mu$ at the rate $n^{-\\beta}$, by which we mean that if we calculate its expected \n",
    "population mean squared error $\\text{MSE} = E \\lVert \\hat \\mu - \\mu \\rVert_{L_2(P)}^2$ at increasing \n",
    "sample sizes, then $\\text{RMSE}=\\sqrt{MSE}$ satisfies a bound $\\text{RMSE} \\le \\alpha n^{-\\beta}$ for some \n",
    "constant $\\alpha$.[^3] [^4] We can make analogous statements for sample mean squared\n",
    "error and squared error at the point $x=0$. \n",
    "\n",
    "[^3]: This is a bit imprecise. The precise language we use is a\n",
    "    mouthful: $\\hat \\mu$ converges *in mean-square* to $\\mu$ in the\n",
    "    $L_2(P)$ norm.\n",
    "[^4]: We tend to talk about $\\text{RMSE}$ rather than $\\text{MSE}$ because it's a bit more intuitive. You can think of it as a typical distance between $\\hat \\mu - \\mu$, which is something you could see in a plot of the curves $\\hat\\mu$ and $\\mu$, rather than the square of that, which\n",
    "you'd have to calculate.\n",
    "\n",
    "Naturally, in any fixed sample size the rate $n^{-\\beta}$ is meaningless\n",
    "by itself because we could just increase the constant $\\alpha$ to get a\n",
    "bound no matter what $\\text{RMSE}$ is. What makes this concept\n",
    "meaningful is the requirement that the constant be the same as sample\n",
    "size gets bigger. \n",
    "\n",
    "\n",
    "Here's a way to interpret this, although it does require that our rate\n",
    "be an approximation to rather than an upper bound on our\n",
    "root-mean-squared error (RMSE), i.e., that\n",
    "$\\text{RMSE} \\approx \\alpha n^{-\\beta}$. If our rate is\n",
    "$n^{-1/2}$, then if our error at sample size $100$ is roughly $.1$, your\n",
    "error at sample size $10,000=100^2$ would be roughly $.01$, as the ratio\n",
    "of errors should be roughly this. \n",
    "\n",
    "$$ \n",
    "\\frac{\\text{RMSE}_{100^2}}{\\text{RMSE}_{100^1}} \n",
    "  \\approx \\frac{\\alpha (100^2)^{-1/2}}{\\alpha (100^1)^{-1/2}} \n",
    "  = \\left( \\frac{100^2}{100^1} \\right)^{-1/2} \n",
    "  = 100^{-1/2} = .1 \n",
    "$$\n",
    "\n",
    "More generally, if our rate is $n^{-\\beta}$, then if we increase our\n",
    "sample size by a factor of $10^{1/\\beta}$, we get an extra digit of\n",
    "precision, i.e., for our error to go from $.1$ to $.01$ or from $.03$ to\n",
    "$.003$. To get an extra digit ...\n",
    "\n",
    "  - we need $100$ times more data if our rate is $n^{-1/2}$.\n",
    "  - we need $1000$ times more data if our rate is $n^{-1/3}$.\n",
    "  - we need $10,000$ times more data if our rate is $n^{-1/4}$.\n",
    "\n",
    "We tend not to talk less about rates worse than $n^{-1/4}$ because it's really hard to get\n",
    "$10,000$ times more data. And we tend to talk about rates more in the\n",
    "context of theory than in empirical studies like this one. But getting\n",
    "some experience with them now should help build intuition that will help\n",
    "us make sense of theoretically-derived rates of convergence, which we'll\n",
    "talk a lot about later in the semester.\n",
    "\n",
    "To get a sense of each estimator's rates of convergence to the curves\n",
    "$\\mu$, we'll fit a simple two-parameter exponential model\n",
    "$m(n)=\\alpha n^{-\\beta}$ to predict the square root of the\n",
    "expected squared error estimates we calculated in\n",
    "the last section. To do this, we'll use the `R` built-in for nonlinear regression, `nls`. \n",
    "And to make sure this exponential model is meaningful, we'll check its the\n",
    "fit visually by adding the predicted MSE $\\hat m(n)^2=\\hat\\alpha^2 n^{-2\\hat\\beta}$\n",
    "it to the plots of the actual expected error curves.\n",
    "\n",
    "**Sample size**.\n",
    "The range of samples sizes we've been working with so far,\n",
    "$n=\\{25, 50,100,200\\}$, is a little small to estimate a rate accurately.\n",
    "So we'll make a table using a larger range here, doubling from 25 to\n",
    "1600. Other than changing that, this is exactly the code we ran a moment ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "R=100\n",
    "ns.rate = 25*2^(0:6)\n",
    "X = runif(max(ns.rate))\n",
    "\n",
    "rnoise = noise(sigma)[[noise.type]]\n",
    "rate.tab = map_dfr(1:R, function(rep) { \n",
    "            map_dfr(mu, .id='mu', function(mu) {\n",
    "              Y = mu(X) + rnoise(length(X))\n",
    "              tabulate.errors(X, Y, mu, ns.rate)\n",
    "            })\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# adding 1e-2/log(log(n)) to our errors below is a hack that we need to \n",
    "# make nls tolerate working on the errors of our selection estimator, \n",
    "# which are often zero. This replaces those zeros with a very slowly \n",
    "# decreasing curve, so we'll slightly overestimate our rate of convergence\n",
    "hacked.rate.tab = rate.tab\n",
    "hacked.rate.tab[rate.tab$model=='selection','error'] = \n",
    "  rate.tab[rate.tab$model=='selection','error'] + \n",
    "  1e-2/log(log(rate.tab[rate.tab$model=='selection','n']))\n",
    "\n",
    "rates = hacked.rate.tab |> group_by(error.measure, model, mu) %>% \n",
    "\t\tgroup_modify(function(group,...) {\n",
    "\t\t    reg.data = group |> group_by(n) |> summarize(error=mean(error))\n",
    "\t\t    model = nls(formula = sqrt(error) ~ a*n^(-b), data=reg.data, \n",
    "\t\t                start=list(a=1, b=1/2))\n",
    "\t\t    data.frame(a=coef(model)[1], b=coef(model)[2])\n",
    "\t    })\n",
    "rates[rates$error.measure=='population', ]\n",
    "rates[rates$error.measure=='sample', ]\n",
    "rates[rates$error.measure=='point', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rate.predicted = hacked.rate.tab |> group_by(mu,model,error.measure) |>\n",
    "\t\tgroup_modify(function(group,...) {\n",
    "\t\t    reg.data = group |> group_by(n) |> summarize(error=mean(error))\n",
    "\t\t    model = nls(formula = sqrt(error) ~ a*n^(-b), data=reg.data, \n",
    "\t\t                start=list(a=1, b=1/2))\n",
    "\t\t    reg.data$error = predict(model)^2\n",
    "\t\t    reg.data\n",
    "\t    })\n",
    "\n",
    "rate.plots = map(unique(rate.tab$error.measure), function(measure) {\n",
    "  mse.with.errorbars.plot(rate.tab[rate.tab$error.measure == measure,]) + \n",
    "          geom_line(aes(x=n, y=error, color=model), alpha=.4, linewidth=2, \n",
    "\t\t\t\t            data=rate.predicted[rate.predicted$error.measure == measure, ]) +\n",
    "\t\t\t    facet_grid(cols=vars(mu), rows=vars(error.measure))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Styling**.\n",
    "Here's the comparison of the error prediction curves we get using\n",
    "our polynomial model $\\text{RMSE} \\approx \\alpha n^{-\\beta}$\n",
    "and the actual error curves.\n",
    "I've plotted the predictions curves as lines that matching the actual \n",
    "error curve in color, but wider and semi-transparent. That means that\n",
    "if the predictions are really good, what we'll see are thin lines with \n",
    "wide 'tubes' around them. You'll see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "walk(rate.plots, print) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about what this all means.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Let's start with the line, since all of our models can fit it. Have you\n",
    "estimated different rates of convergence $n^{-\\beta}$ in population mean\n",
    "squared error for the lines and monotone regression models?[^4] What do you make\n",
    "of this? Would you say that this summary was helpful or misleading? Why?\n",
    "What about sample mean squared error and squared error at the point\n",
    "$x=0$?\n",
    "\n",
    "**Tip.** It may be helpful to think about the values of $\\alpha$ you've\n",
    "estimated.\n",
    ":::\n",
    "\n",
    "[^4]: I left out the select-one-of-four model because the idea of a rate\n",
    "    for it is a little weird. Error is, after all, zero for most or all\n",
    "    replications for most $n$ we consider. To get at what's happening\n",
    "    with this estimator, we need to be talking about something like the\n",
    "    smallest sample size at which we tend to get zero error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "Yes, I have estimated different rates in population mean squared error\n",
    "(and sample mean squared error, which is very similar): roughly\n",
    "$n^{-1/2}$ for the linear model and $n^{-1/3}$ for the increasing one.\n",
    "We know from our intro classes to expect that $n^{-1/2}$ rate for lines\n",
    "and it makes sense that it takes longer to get a good fit using a\n",
    "more flexible model like the set of increasing curves. The specific rate\n",
    "$n^{-1/3}$ is something that we'll work out theoretically later in the\n",
    "semester.\n",
    "\n",
    "In this case, the summary is helpful. The rate model is fairly faithful\n",
    "to the errors we actually observe and the estimated constants $\\alpha$\n",
    "for these two models are pretty similar, so the rate comparison is\n",
    "pretty meaningful.\n",
    "\n",
    "For squared error at the point $x=0$, the difference we see is sharper.\n",
    "For the linear model, we estimate the rate $\\approx n^{-1/2}$, whereas\n",
    "for the increasing model we estimate the rate $\\approx n^{-1/10}$. And\n",
    "the constants $\\alpha$ in front of these rate estimates are similar.\n",
    "What's happening is that, when we use the linear model, points\n",
    "everywhere in $[0,1]$ are telling us what $\\mu(0)$ ought to be. As a\n",
    "result, we're able to estimate $\\mu(0)$ roughly as well as we can\n",
    "estimate the typical point $\\mu(x)$: population MSE and squared error at\n",
    "$x=0$ are comparable. In reality, this is overconfidence: it only works\n",
    "if $\\mu$ is actually a line. You can see it doesn't work otherwise by\n",
    "looking at what happens for the other curves $\\mu$. On the other hand,\n",
    "as discussed in relation to a previous question, based on the increasing\n",
    "model we can infer little or nothing about what is happening at or past\n",
    "the boundary of the data: $n^{-1/10}$ is probably an overestimate of the\n",
    "rate of convergence in this case. If we are interested in making claims\n",
    "about behavior at the boundary, we will have to base them on assumptions\n",
    "other than monotonicity.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Now let's think about the other three curves $\\mu$ that you've fit: the\n",
    "step, the stepline, and the sine. Are you seeing the same rates of\n",
    "convergence from linear and monotone regression that you got in the\n",
    "previous exercise, when $\\mu$ was the line? Does your answer depend on\n",
    "$\\mu$? If so, try to explain why you're seeing this variation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "\n",
    "::: {.callout-solution}\n",
    "For lines, we're estimating rates of convergence of\n",
    "essentially zero. The reason for this is that, after $n=100$ or so, our\n",
    "error curves aren't making any significant downward progress. After\n",
    "that, any improvements we can get by doing a better job of finding the\n",
    "line closest to $\\mu$, which is what more data can help with, are\n",
    "negligible relative to the difference between that closest line and\n",
    "$\\mu$ itself, which isn't approximated very well by a line.\n",
    "\n",
    "For monotone regression, we're estimating a rate of essentially zero\n",
    "when $\\mu$ is the sine for the same reason: the sine isn't increasing,\n",
    "so once we've got a big enough sample to fit the increasing part\n",
    "reasonably well, any improvements we get by fitting it better will be\n",
    "negligible relative to the error we get because we can't fit the\n",
    "decreasing part with an increasing curve. The other two curves $\\mu$,\n",
    "the step and step-line, are increasing, and we do get the same\n",
    "$n^{-1/3}$ rate we got in the case that $\\mu$ was the line.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universality\n",
    "\n",
    "This is optional. Do it if you've got extra time. I'm not going to\n",
    "provide solutions for these, but we'll dig into these issues a little\n",
    "later on when we've got some theory to guide us.\n",
    "\n",
    "When we looked at the data, we saw that it looked roughly the same no\n",
    "matter whether we used gaussian, uniform, or t-distributed noise. So\n",
    "you'd expect that the MSE and rates of convergence you see wouldn't\n",
    "change much.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Change the distribution of the noise in the code above and rerun it.\n",
    "That's as easy as changing `noise.type='gaussian'` in the plot-pop-mse\n",
    "block to `noise.type='t'` or `noise.type='uniform'` and rerunning\n",
    "everything below that. Discuss whether you see any changes.\n",
    ":::\n",
    "\n",
    "The data certainly would look different if we increased the scale of the\n",
    "noise, say from having standard deviation $0.5$ as we've been using to\n",
    "having standard deviation $5$. However, it might be the case that having\n",
    "a little data with a little noise is similar to having a lot of data\n",
    "with a lot of noise. Look into whether that's the case.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Increase the standard deviation of the noise. All you have to do is\n",
    "change `sigma = .5` in the noise-distributions block above to `sigma=5`. \n",
    "While the quantitative result you see\n",
    "will change, i.e. you'll have more error, you might expect the\n",
    "qualitative, e.g. the relationships between types of MSE and the rates\n",
    "of convergence, to stay the same. Discuss what you find. Then repeat,\n",
    "instead decreasing the standard deviation of the noise, e.g. using\n",
    "`sigma=.05`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Tidyverse Tools\n",
    "\n",
    "While I don't often shill for the tidyverse, I found those tools pretty\n",
    "helpful here. If you're not familiar, the 'tidy' way to do things is to\n",
    "collect a bunch of stuff in one big dataframe. When I tabulated errors\n",
    "earlier, I put them in a data frame with columns `error`, `error.measure`, `n`, `mu`, `model`, and `noise`. \n",
    "This makes everything very easy to plot using ggplot. I'm using these\n",
    "commands to plot mine. I'm plotting the mean squared errors and squared\n",
    "error at the point $x=0$ separately because the scales tend to be\n",
    "different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(tab[tab$error.measure != 'point',], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "  geom_line() + geom_point() + facet_grid(cols=vars(mu), rows=vars(noise))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(tab[tab$error.measure == 'point', ], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "    geom_line() + geom_point() + facet_grid(cols=vars(mu), rows=vars(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My code leans heavily on the function `map_dfr` in the tidyverse package **purrr**.^[It turns out that this function is deprecated, i.e. the maintainers of the package **purrr** are recommending that you stop using it because they're planning to remove it from the package at some point in the future. It's usually considered bad form to use deprecated functions, but it's really hard to avoid when you use the tidyverse, so I'm giving you code that does it. The package maintainers in the tidyverse group are always renaming functions and moving them from package to package with tiny changes, so if you wrote code using tidyverse stuff a few years ago, the odds are pretty good something you've used is deprecated at this point. That's usually considered bad form too, and because of that, many people I know have given up on the idea of using tidyverse stuff in any code that isn't essentially disposable. But there's a lot of useful stuff in there, so I try to take a more conservative approach unless I really need my code not to break, e.g. when I'm writing packages of my own that lots of people will be using. I don't really worry about tidyverse functions getting deprecated because it takes a while for them to actually go away and when they do, you can usually redefine the old functions in terms of the new ones with at most a couple lines of code.] \n",
    "\n",
    "This is really nothing special. It's just like Python's map or R's built-in lapply, except that\n",
    "it expects the function you're mapping to return a dataframe, and\n",
    "instead of collecting these dataframes in a list, it stacks them\n",
    "vertically into a single dataframe. It takes two arguments.\n",
    "\n",
    "1.  a list `list(a=x,b=y,c=z,...)`\n",
    "\n",
    "2.  a function `f(x)` that outputs a data.frame\n",
    "\n",
    "and returns `f(x)`, `f(y)`, `f(z)`, ... vertically stacked into a data\n",
    "frame `df`. And it has one other slightly convenient feature. If you\n",
    "pass the option .id='somestring' for some string id, it'll add a column\n",
    "`df$somestring` that stores the names `a`,`b`,`c`, ... in the rows\n",
    "corresponding to `f(x)`, `f(y)`, and `f(z)` respectively.\n",
    "\n",
    "You can use it to loop over models, error measures, curves, etc. You can\n",
    "nest calls to loop over multiple things; everything winds up getting\n",
    "stacked together.\n",
    "\n",
    "Here's some example code that might help a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mus=list(a=1, b=2, c=3)\n",
    "sigmas=list(small=.1, big=10)\n",
    "mean.squares = map_dfr(mus, .id='mu', function(mu) {\n",
    "                  map_dfr(sigmas, .id='sigma', function(sigma) {\n",
    "                    Y = mu + sigma*rnorm(10)\n",
    "                    data.frame(mean.square=mean(Y^2))\n",
    "                  })\n",
    "               })\n",
    "mean.squares"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
