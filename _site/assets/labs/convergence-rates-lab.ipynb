{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Folklore says that if we use more data, we'll make better estimates. In\n",
    "today's lab, we'll qualify and quantify this statement. We're going to\n",
    "look at how a few measures of the accuracy of our estimated curves vary\n",
    "as a function of sample size. And to summarize what we see, we'll\n",
    "estimate *rates of convergence*.\n",
    "\n",
    "We'll use a few libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "  library(tidyverse)\n",
    "  library(CVXR)\n",
    "})\n",
    "CVXR::add_to_solver_blacklist('OSQP') \n",
    "\n",
    "theme_update(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    panel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                    legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\tpanel.grid.major=element_line(color=rgb(1,0,0,.1,  maxColorValue=1)),\n",
    "\t        panel.grid.minor=element_line(color=rgb(0,0,1,.1,  maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll start by looking at data sampled around a few curves $\\mu$ on\n",
    "$[0,1]$.\n",
    "\n",
    "1.  A step, $\\mu(x) = 1(x \\ge .5)$.\n",
    "2.  A line, $\\mu(x)=x$.\n",
    "3.  A step into a line, $\\mu(x) = x 1(x \\ge .5)$.\n",
    "4.  A sine, $\\mu(x)=\\sin(\\pi x)$.\n",
    "\n",
    "Here's some code for evaluating our curves. We've put them in a list, so\n",
    "`mu$step(x)` is $\\mu(x)$ for the step function and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = list(step     = function(x) { 1*(x >= .5) },\n",
    "\t        line     = function(x) { x },\n",
    "\t        stepline = function(x) { x*(x >= .5) }, \n",
    "\t        sin      = function(x) { sin(pi*x) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll sample $X_i$ from the uniform distribution on $[0,1]$ and then\n",
    "sample $Y_i$ around $\\mu(X_i)$, taking $Y_i=\\mu(X_i) + \\varepsilon_i$\n",
    "where $\\varepsilon_i$ is independent of $X_i$ with mean zero. \n",
    "\n",
    "  - We'll start out by working with Gaussian noise. \n",
    "  - But if we have time, we'll also take a look at what changes when we have ...\n",
    "    - lighter-tailed uniformly-distributed noise \n",
    "    - heavier-tailed $t_3$-distributed noise \n",
    "\n",
    "    Both of these will be scaled to have the same standard deviation $\\sigma=\\sqrt{\\operatorname*{Var}{\\varepsilon_i}}$.\n",
    "\n",
    "Here's some code for sampling noise from these distributions. Again,\n",
    "we've put them in a list, but now that list is the output of a function\n",
    "of the noise standard deviation $\\sigma$, so `noise(.5)$gaussian(n)`\n",
    "gives us $n$ draws from the gaussian distribution with mean zero and\n",
    "standard deviation $.5$. We'll set our default choice for $\\sigma$\n",
    "to $.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#| label: noise-distributions\n",
    "noise = function(sigma) { \n",
    "    list(gaussian = function(n) { sigma*rnorm(n) },\n",
    "\t       uniform  = function(n) { sigma*sqrt(3)*runif(n,-1,1) },\n",
    "\t       t        = function(n) { (sigma/sqrt(3))*rt(n, 3) }) }\n",
    "\n",
    "sigma = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our curves and the data we're sampling around them\n",
    "with these three different noise distributions. There are, of course,\n",
    "differences from one noise distribution to another, but qualitatively\n",
    "things look pretty similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "X = runif(n)\n",
    "x = seq(0,1,by=.001)\n",
    "\n",
    "for(mm in 1:length(mu)) {\n",
    "  observations = purrr::map_dfr(noise(sigma), .id='noise', function(rnoise) { \n",
    "    data.frame(X=X, Y=mu[[mm]](X) + rnoise(n))\n",
    "  })   \n",
    "  curve = purrr::map_dfr(noise(sigma), .id='noise', function(rnoise) {\n",
    "    data.frame(x=x, y=mu[[mm]](x))\n",
    "  }) # we do this so faceting works: \n",
    "     # the values of mu(x) must be repeated for each noise distribution.\n",
    "  plt = ggplot() + \n",
    "          geom_point(aes(x=X, y=Y), alpha=.2, data=observations) + \n",
    "          geom_line(aes(x=x, y=y), data=curve) + \n",
    "          facet_grid(cols=vars(noise))\n",
    "  print(plt  + xlab('') + ylab(''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's hard to have intuition for how accurately we'll be able to\n",
    "estimate a curve in a quantitative sense, we get somewhere by asking a\n",
    "more qualitative question. We can ask whether, based on the observations\n",
    "$X_i,Y_i=\\mu(X_i)+\\varepsilon_i$, we can identify which of our four\n",
    "curves we're looking at. To get a sense of how accuracy in this sense\n",
    "relates to sample size, give this exercise a shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Try varying sample size $n$ in the code above. Based on what you see, at\n",
    "roughly what sample size do you start getting a clear sense of the shape\n",
    "of the curve $\\mu(x)$ from looking at the data? You don't have to be all\n",
    "that precise: try doubling sample sizes $n=50,100,200,400,\\ldots$. \n",
    "\n",
    "Your answer may vary by the curve $\\mu$ and the noise distribution. Briefly\n",
    "describe the trends. No need to make a big table for all curves and\n",
    "noise distributions.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Now let's test your intuition. We'll display, in random order, plots\n",
    "like the ones above for our four different curves $\\mu(x)$. See if you\n",
    "can work out which is which.\n",
    "\n",
    "Run the block below to plot our four curves in random order with meaningless\n",
    "numbers 1-4 as labels. Write down which of our curves you think you're\n",
    "seeing for each number. Then, run the block below that, which will show\n",
    "the same plot with informative labels, to check your answers.\n",
    "\n",
    "Change the first block to try different values of $n$ and report whether\n",
    "your intuition from the last exercise was right. And repeat this for the\n",
    "three noise distributions. To do this, change 'gaussian' in\n",
    "`rnoise=noise(sigma)$gaussian` to 'uniform' or 't'.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "rnoise = noise(sigma)$gaussian\n",
    "X = runif(n)\n",
    " \n",
    "mu.order = sample(1:4)\n",
    "observations = mu.order |> map(function(mm) { \n",
    "  data.frame(X=X, Y=mu[[mm]](X)+rnoise(n), curve=mm)\n",
    "}) |> list_rbind()\n",
    "plt = ggplot() + geom_point(aes(x=X, y=Y,), alpha=.2, data=observations)\n",
    "plt + facet_wrap(~curve, nrow=2) + xlab('') + ylab('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "informative_labeller = function(factors) { list(names(mu))[factors$curve] }\n",
    "plt + facet_wrap(~curve, nrow=2, labeller=informative_labeller) + xlab('') + ylab('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten a sense of what sample sizes are required to\n",
    "visually distinguish our four curves, let's start fitting them using monotone regression. \n",
    "We'll see how our choice of model impacts our ability to estimate them accurately.\n",
    "\n",
    "In addition to linear regression, which chooses the best fitting curve\n",
    "from the set of all lines, and monotone regression, which chooses from\n",
    "the set of all increasing curves, we'll use a model-selection approach\n",
    "that plays the same guessing game you've been playing. Just like you,\n",
    "it'll choose from the set of four curves we're sampling our data around.\n",
    "This is, of course, unusable in practice; it's never going to be\n",
    "plausible that you know that the data's conditional mean\n",
    "$\\mu(x)=E[Y_i \\mid X_i=x]$ is one of a few curves you know a-priori. But\n",
    "comparing this to the other methods will give you a sense of how much\n",
    "harder the task of estimating a curve via something like monotone\n",
    "regression is than the task you were working on visually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Code\n",
    "\n",
    "We'll be comparing linear regression to monotone regression, so you'll\n",
    "want monotone regression code. To minimize the time we wait for code to\n",
    "run, we'll use R's built-in implementation, `isoreg`, which uses a fast\n",
    "specialized algorithm called PAVA to solve the monotonicity-constrained\n",
    "least squares problem. I'll give you a code that wraps it so it behaves\n",
    "exactly like the function `monotonereg` we wrote in lab. I'll\n",
    "also give you an implementation of `selectionreg`, which selects one of\n",
    "our four curves as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg = function(X,Y, decreasing=FALSE) {\n",
    "  isoreg.model = if(decreasing) { isoreg(-X,Y) } else { isoreg(X,Y) }\n",
    "  mu.hat = array(dim=length(X))\n",
    "  mu.hat[isoreg.model$ord] = isoreg.model$yf\n",
    "  \n",
    "  model = list(X=X, mu.hat=mu.hat)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.piecewise.constant = function(model, newdata=data.frame(X=model$X)) {\n",
    "  increasing.order = order(model$X) \n",
    "  X = model$X[increasing.order]\n",
    "  mu.hat = model$mu.hat[increasing.order]\n",
    "  \n",
    "  # for each new data point x[k]\n",
    "  # find the closest observed X[i[k]] left of x[k]\n",
    "  # i.e., i[k] is the largest integer i for which X[i] <= x[k] \n",
    "  # this covers cases 1 and 2\n",
    "  i = findInterval(newdata$X, X) \n",
    "  # if there is no X[i] < x[k], findInterval sets i[k]=0\n",
    "  # to cover case 3, we want X[i] for i=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  mu.hat[i]\n",
    "}\n",
    "predict.monotonereg = predict.piecewise.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "selectionreg = function(X,Y) {\n",
    "  mses = sapply(mu, function(m) { mean((Y-m(X))^2) })\n",
    "  model = list(X=X, mu=names(mu)[which.min(mses)])\n",
    "  attr(model, 'class') = 'selectionreg'\n",
    "  model\n",
    "}\n",
    "predict.selectionreg = function(model, newdata=data.frame(X=model$X)) {\n",
    "  mu[[model$mu]](newdata$X)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, having our code's notation match our mathematical notation makes\n",
    "it easier to read. To help with that, here's a function that takes a\n",
    "model like we'd get by calling `lm`, `monotonereg`, or `selectionreg`\n",
    "and gives us back a function $\\hat\\mu$ that makes predictions\n",
    "$\\hat\\mu(x)$ using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "prediction.function = function(model) { \n",
    "  function(x) { predict(model, newdata=data.frame(X=x)) } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X = runif(n)\n",
    "Y = mu$step(X) + noise(sigma)$gaussian(n)\n",
    "\n",
    "mu.hat = prediction.function(monotonereg(X,Y))\n",
    "mu.hat(c(0,.5,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x=seq(0,1,by=.001)\n",
    "ggplot() + geom_point(aes(x=X, y=Y), alpha=.2) + \n",
    "           geom_line(aes(x=x, y=mu.hat(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squared Errors\n",
    "\n",
    "To get a sense of how sample size impacts the accuracy of our estimator,\n",
    "we'll vary sample size and track a few measures of its error. We'll look\n",
    "at three error measures.\n",
    "\n",
    "1.  Sample mean squared error, the estimator's typical level of error on\n",
    "    the sample used to fit the curve.\n",
    "    $$ \\lVert\\hat\\mu-\\mu\\rVert_{L_2(P_n)}^2\n",
    "                     = \\frac{1}{n}\\sum_{i=1}^n \\{ \\hat\\mu(X_i) - \\mu(X_i)\\}^2. $$\n",
    "\n",
    "2.  Population mean squared error, its typical level error on points\n",
    "    drawn independently from the same distribution as that sample. For\n",
    "    $\\tilde X$ independent of $X_1 \\ldots X_n$ with the same distribution,\n",
    "    $$ \\lVert\\hat\\mu-\\mu\\rVert_{L_2(P)}^2 \n",
    "                     = E\\left[ \\{ \\hat \\mu(\\tilde X) - \\mu(\\tilde X) \\}^2 \\mid X_1 \\ldots X_n\\right]. $$\n",
    "    \n",
    "    - Since we're sampling $X_i$ from the uniform distribution, that's\n",
    "      $\\int_0^1\\{ \\hat \\mu(x) - \\mu(x) \\}^2 dx$. We won't calculate this integral exactly.\n",
    "    - To approximate it, we'll use the simplest numerical integration scheme there is. We'll\n",
    "    just use the average of $\\{\\hat\\mu(x)-\\mu(x)\\}^2$ for an evenly-spaced grid of points $x \\in [0,1]$.\n",
    "\n",
    "3.  Squared error at a single point $x$,\n",
    "    $$\n",
    "    \\{ \\hat \\mu(x) - \\mu(x) \\}^2\n",
    "    $$. \n",
    "    We'll look at the point $x=0$ at the left edge of $X_i$'s distribution. \n",
    "     \n",
    "     - This is the sort of thing you wind up working with in RDD estimates.[^1]\n",
    "\n",
    "[^1]: Think of it like this. You've taken the data to the right of the\n",
    "    enrollment cutoff in our class size example (enrollment $> 40$),\n",
    "    defined $X_i$ by shifting and scaling enrollment so the cutoff\n",
    "    occurs at $x=0$ so the cutoff occurs at $x=0$ and all $X_i$ lie in\n",
    "    into the range $[0,1]$ ($X_i=(\\text{enrollment}_i-40)/40$), fit the\n",
    "    data, and are now evaluating it at $X_i=0$ to get the estimate\n",
    "    $\\hat \\mu_{\\text{right}}(40)$. We'll often want to scale the data\n",
    "    into the unit interval like this for other reasons, so this is a\n",
    "    more natural way of thinking about things than it may sound now.\n",
    "\n",
    "We'll do this for three estimators.\n",
    "\n",
    "1. the least squares line (we use the `R` built-in `lm`) \n",
    "2. the monotone regression estimator (we use the function `monotonereg` defined above)\n",
    "3. the estimator that selects one of our four choices of $\\mu$ (we use `selectionreg` defined\n",
    "above).\n",
    "\n",
    "This kind of code is kind of a pain to write, so I'm going to give you\n",
    "everything you need. But you'll be using this code in a few homeworks including this week's, so you may\n",
    "want to take a look at the last section of this notebook, where I describe how the code works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Run the code below to plot these three error measures at a range of\n",
    "sample sizes. As it is, it plots them at sample sizes\n",
    "$n \\in \\{25,50,100,200\\}$. Feel free to add some more. Briefly describe\n",
    "what you see. \n",
    "\n",
    "Later on, I'll ask more precise questions, so don't worry\n",
    "about being complete.\n",
    ":::\n",
    "\n",
    "\n",
    "Here, to start, is some code for computing our three error measures\n",
    "given a function $\\hat\\mu(x)$. Note that so we can substitute one for\n",
    "another in our code, they all take the sample $X=X_1 \\ldots X_n$ as an\n",
    "argument even though only sample MSE actually needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sample.mse     = function(mu.hat, mu, X) { \n",
    "  mean((mu.hat(X) - mu(X))^2) \n",
    "}\n",
    "population.mse = function(mu.hat, mu, X) { \n",
    "  x=seq(0,1,by=.001)\n",
    "  mean((mu.hat(x) - mu(x))^2) \n",
    "}   \n",
    "point.se = function(mu.hat, mu, X) { \n",
    "  (mu.hat(0) - mu(0))^2 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a code for making a table of these 3 errors, for our 3\n",
    "regression methods, at our 4 different sample sizes. Naturally, it has\n",
    "$3 \\times 3 \\times 4 = 36$ rows, one for each combination of these\n",
    "things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "errors = list(sample=sample.mse, population=population.mse, point=point.se) \n",
    "models = list(lines = function(X,Y) { lm(Y~X, data.frame(X=X,Y=Y)) },\n",
    "              monotone = monotonereg,\n",
    "              selection  = selectionreg)\n",
    "\n",
    "tabulate.errors = function(X,Y, mu, ns) { \n",
    "  map_dfr(ns, function(n) {\n",
    "    map_dfr(models, .id='model', function(model.fit) {\n",
    "\t    model = model.fit(X[1:n], Y[1:n])\n",
    "\t    map_dfr(errors, .id='error.measure', function(error) {\n",
    "        data.frame(n=n, error=error(prediction.function(model), mu, X[1:n]))\n",
    "\t    })\n",
    "    })\n",
    "  })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll run it to produce tables for all combinations of the four curves\n",
    "$\\mu(x)$ and three noise distributions described above, then stack them all together. If you want, you can look at the\n",
    "table, but it's probably easier to plot what's in it and look at the\n",
    "plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ns = c(25,50,100,200)\n",
    "X = runif(max(ns))\n",
    "\n",
    "tab = map_dfr(mu, .id='mu', function(mu) {\n",
    "        map_dfr(noise(sigma), .id='noise', function(rnoise) {\n",
    "          Y = mu(X) + rnoise(length(X))\n",
    "          tabulate.errors(X, Y, mu, ns)\n",
    "        })\n",
    "    })\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's a plot of population and sample MSE. We'll just look at what\n",
    "happens with gaussian noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "noise.type = 'gaussian'\n",
    "noise.tab = tab[tab$noise==noise.type, ]\n",
    "\n",
    "ggplot(noise.tab[noise.tab$error.measure != 'point',], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "  geom_line() + geom_point() + facet_grid(cols=vars(mu), )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And here's a plot of squared error at the point $x=0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(noise.tab[noise.tab$error.measure == 'point', ], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "    geom_line() + geom_point() + facet_grid(cols=vars(mu), rows=vars(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Ignore the squared error at the point $x=0$ for now. Answer these\n",
    "questions for the linear model, the the monotone (increasing) model, and\n",
    "the select one-of-four model. \n",
    "\n",
    "1. Is the qualitative behavior you see roughly the same for all curves? \n",
    "2. If not, for which curve or curves is it different? \n",
    "3. How are they different? Why?\n",
    "\n",
    "**Tip.** If you see different behaviors in the error plots but don't\n",
    "know why, plot the fitted curve $\\hat \\mu$ on top of the data and look\n",
    "at it.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {.callout-exercise}\n",
    "Again, ignore squared error at the point $x=0$. Is either sample or\n",
    "population mean squared error typically larger or is their relationship\n",
    "inconsistent? Why do you think that's the case?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Now let's think about why we've been ignoring squared error at $x=0$.\n",
    "The errors we have plotted are *random*. They depend on the sample\n",
    "$\\mathcal{S}=\\{(X_i,Y_i) : i \\le n\\}$ used to fit $\\hat\\mu$.[^2] Rerun\n",
    "the code in this section to draw a new sample and make your plots. Do it\n",
    "a few times. Is the qualitative behavior you've already described\n",
    "consistent from sample to sample? What about squared error at $x=0$? In\n",
    "which cases is it inconsistent? Why?\n",
    "\n",
    "\n",
    "**Tip.** Each time you resample, plot the fitted curves $\\hat \\mu$ on\n",
    "top of the data and look at them.\n",
    ":::\n",
    "\n",
    "[^2]: And, in the case of sample mean squared error, also more directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Expected Squared Error\n",
    "\n",
    "In this section, we're going to summarize the distribution of the mean\n",
    "squared errors we plotted in the previous one. Because each error is a\n",
    "function of the sample, $\\text{error}(\\mathcal{S})$, it makes sense to\n",
    "talk about its mean $\\text{E}[ \\text{error}(\\mathcal{S})]$ and its\n",
    "variance $\\text{Var}[ \\text{error}(\\mathcal{S})]$. We can, of course,\n",
    "estimate those by the sample mean and sample variance, \n",
    "\n",
    "$$\n",
    "\\hat{\\text{E}}[ \\text{error}(\\mathcal{S})] := \\frac{1}{R}\\sum_{r=1}^R \\text{error}(\\mathcal{S}_r)\n",
    "\\quad \\text{ and } \\quad\n",
    "\\widehat{\\text{Var}}[ \\text{error}(\\mathcal{S})] := \\frac{1}{R}\\sum_{r=1}^R \\left\\{\\text{error}(\\mathcal{S}_r) - \\hat{\\text{E}}[ \\text{error}(\\mathcal{S})] \\right\\}^2\n",
    "$$ \n",
    "\n",
    "based on independent replications\n",
    "$\\mathcal{S}_1 \\ldots \\mathcal{S}_R$ of the sample. And, as a\n",
    "back-of-the-envelope thing, you might think of\n",
    "$\\hat{\\text{E}}[\\text{error}(\\mathcal{S})] \\pm 2\\sqrt{\\widehat{\\text{Var}}[\\text{error}(\\mathcal{S})]}$\n",
    "as a 95% confidence interval for $\\text{E}[\\text{error}(\\mathcal{S})]$, i.e. an interval you might\n",
    "expect 95% of replications of $\\text{E}[\\text{error}(\\mathcal{S})]$ to be in.\n",
    "\n",
    "To show this distributional summary, we'll replicate the plots from\n",
    "section on squared errors above, with each point being the mean\n",
    "$\\hat{\\text{E}}[\\text{error}(\\mathcal{S})]$ over $R=100$ replications.\n",
    "Then, to give a sense of spread, we'll display our prediction interval\n",
    "by plotting error bars extending two estimated standard deviations\n",
    "$\\sqrt{\\widehat{\\text{Var}}[\\text{error}(\\mathcal{S})]}$ in each\n",
    "direction.\n",
    "\n",
    "First, we'll make a $R$ tables like we did before, one for each\n",
    "replication $\\mathcal{S}_r$ of our the dataset, and stack them. This\n",
    "takes is a bit slow, but it takes under a minute on my laptop. If it's\n",
    "too slow on yours, you can decrease the number of replications $R$ to\n",
    "reduce the runtime. This code should run for about $R$ times the amount\n",
    "of time it took to make the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "R=100\n",
    "rep.tab = map_dfr(1:R, function(rep) { \n",
    "            map_dfr(mu, .id='mu', function(mu) {\n",
    "              map_dfr(noise(sigma), .id='noise', function(rnoise) {\n",
    "                Y = mu(X) + rnoise(length(X))\n",
    "                tabulate.errors(X, Y, mu, ns)\n",
    "              })\n",
    "            })\n",
    "          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Then we'll do our plots. `ggplot`'s function `stat_summary` does a lot\n",
    "of the work for us, aggregating the data into the means and the upper\n",
    "and lower confidence bounds we need to plot error bars. Here's the code.\n",
    "We'll plot sample and population MSE together as we did before so\n",
    "they're easy to compare, but because the error bars make that plot a\n",
    "little crowded, we'll also plot them separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mse.with.errorbars.plot = function(tab) {\n",
    "  ggplot(tab, aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "    stat_summary(geom='line', fun=mean) +\n",
    "    stat_summary(geom='pointrange', fun=mean,\n",
    "     fun.min=function(x){ mean(x)-2*sd(x) }, \n",
    "\t\t fun.max=function(x){ mean(x)+2*sd(x) },\n",
    "\t\t position=position_dodge(20)) +\n",
    "     facet_grid(cols=vars(mu), rows=vars(noise)) \n",
    "}\n",
    "\n",
    "rep.noise.tab = rep.tab[rep.tab$noise==noise.type,]\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure!='point',])\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure=='sample',])\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure=='population',])\n",
    "mse.with.errorbars.plot(rep.noise.tab[rep.noise.tab$error.measure=='point',])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Looking at these plots, revisit your answers from the previous part.\n",
    "Describe what you're seeing. \n",
    "\n",
    "- Maybe you got some answers that were true for the particular sample $\\mathcal{S}$ you were looking at, but aren't true on average over samples.\n",
    "- Maybe you got some answers that were true on average, but see a lot of variation from sample to sample.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Rates of Convergence\n",
    "\n",
    "The plots of error vs. sample size in the sections above contain a lot of information. \n",
    "Some things are easy enough to see. It's pretty clear when errors aren't decreasing\n",
    "to zero because we're using a model that can't fit the curve $\\mu$.\n",
    "However, it can be hard to see more quantitative phenomena, like how\n",
    "much faster one model's errors are converging to zero than another's, as\n",
    "this involves behavior at a wide range of $n$. It's a scale problem. If\n",
    "you can see the curve at all for $n=50$, you're zoomed out far enough\n",
    "that you can't see what's going on for $n \\ge 200$. To summarize the\n",
    "curves in a way that can help make some of these relationships clear, we\n",
    "often use *rates of convergence*. We'll say something like $\\hat \\mu$\n",
    "converges to $\\mu$ at the rate $n^{-\\beta}$, by which we mean that if we calculate its expected \n",
    "population mean squared error $\\text{MSE} = E \\lVert \\hat \\mu - \\mu \\rVert_{L_2(P)}^2$ at increasing \n",
    "sample sizes, then $\\text{RMSE}=\\sqrt{MSE}$ satisfies a bound $\\text{RMSE} \\le \\alpha n^{-\\beta}$ for some \n",
    "constant $\\alpha$.[^3] [^4] We can make analogous statements for sample mean squared\n",
    "error and squared error at the point $x=0$. \n",
    "\n",
    "[^3]: This is a bit imprecise. The precise language we use is a\n",
    "    mouthful: $\\hat \\mu$ converges *in mean-square* to $\\mu$ in the\n",
    "    $L_2(P)$ norm.\n",
    "[^4]: We tend to talk about $\\text{RMSE}$ rather than $\\text{MSE}$ because it's a bit more intuitive. You can think of it as a typical distance between $\\hat \\mu - \\mu$, which is something you could see in a plot of the curves $\\hat\\mu$ and $\\mu$, rather than the square of that, which\n",
    "you'd have to calculate.\n",
    "\n",
    "Naturally, in any fixed sample size the rate $n^{-\\beta}$ is meaningless\n",
    "by itself because we could just increase the constant $\\alpha$ to get a\n",
    "bound no matter what $\\text{RMSE}$ is. What makes this concept\n",
    "meaningful is the requirement that the constant be the same as sample\n",
    "size gets bigger. \n",
    "\n",
    "\n",
    "Here's a way to interpret this, although it does require that our rate\n",
    "be an approximation to rather than an upper bound on our\n",
    "root-mean-squared error (RMSE), i.e., that\n",
    "$\\text{RMSE} \\approx \\alpha n^{-\\beta}$. If our rate is\n",
    "$n^{-1/2}$, then if our error at sample size $100$ is roughly $.1$, your\n",
    "error at sample size $10,000=100^2$ would be roughly $.01$, as the ratio\n",
    "of errors should be roughly this. \n",
    "\n",
    "$$ \n",
    "\\frac{\\text{RMSE}_{100^2}}{\\text{RMSE}_{100^1}} \n",
    "  \\approx \\frac{\\alpha (100^2)^{-1/2}}{\\alpha (100^1)^{-1/2}} \n",
    "  = \\left( \\frac{100^2}{100^1} \\right)^{-1/2} \n",
    "  = 100^{-1/2} = .1 \n",
    "$$\n",
    "\n",
    "More generally, if our rate is $n^{-\\beta}$, then if we increase our\n",
    "sample size by a factor of $10^{1/\\beta}$, we get an extra digit of\n",
    "precision, i.e., for our error to go from $.1$ to $.01$ or from $.03$ to\n",
    "$.003$. To get an extra digit ...\n",
    "\n",
    "  - we need $100$ times more data if our rate is $n^{-1/2}$.\n",
    "  - we need $1000$ times more data if our rate is $n^{-1/3}$.\n",
    "  - we need $10,000$ times more data if our rate is $n^{-1/4}$.\n",
    "\n",
    "We tend not to talk less about rates worse than $n^{-1/4}$ because it's really hard to get\n",
    "$10,000$ times more data. And we tend to talk about rates more in the\n",
    "context of theory than in empirical studies like this one. But getting\n",
    "some experience with them now should help build intuition that will help\n",
    "us make sense of theoretically-derived rates of convergence, which we'll\n",
    "talk a lot about later in the semester.\n",
    "\n",
    "To get a sense of each estimator's rates of convergence to the curves\n",
    "$\\mu$, we'll fit a simple two-parameter exponential model\n",
    "$m(n)=\\alpha n^{-\\beta}$ to predict the square root of the\n",
    "expected squared error estimates we calculated in\n",
    "the last section. To do this, we'll use the `R` built-in for nonlinear regression, `nls`. \n",
    "And to make sure this exponential model is meaningful, we'll check its the\n",
    "fit visually by adding the predicted MSE $\\hat m(n)^2=\\hat\\alpha^2 n^{-2\\hat\\beta}$\n",
    "it to the plots of the actual expected error curves.\n",
    "\n",
    "**Sample size**.\n",
    "The range of samples sizes we've been working with so far,\n",
    "$n=\\{25, 50,100,200\\}$, is a little small to estimate a rate accurately.\n",
    "So we'll make a table using a larger range here, doubling from 25 to\n",
    "1600. Other than changing that, this is exactly the code we ran a moment ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "R=100\n",
    "ns.rate = 25*2^(0:6)\n",
    "X = runif(max(ns.rate))\n",
    "\n",
    "rnoise = noise(sigma)[[noise.type]]\n",
    "rate.tab = map_dfr(1:R, function(rep) { \n",
    "            map_dfr(mu, .id='mu', function(mu) {\n",
    "              Y = mu(X) + rnoise(length(X))\n",
    "              tabulate.errors(X, Y, mu, ns.rate)\n",
    "            })\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# adding 1e-2/log(log(n)) to our errors below is a hack that we need to \n",
    "# make nls tolerate working on the errors of our selection estimator, \n",
    "# which are often zero. This replaces those zeros with a very slowly \n",
    "# decreasing curve, so we'll slightly overestimate our rate of convergence\n",
    "hacked.rate.tab = rate.tab\n",
    "hacked.rate.tab[rate.tab$model=='selection','error'] = \n",
    "  rate.tab[rate.tab$model=='selection','error'] + \n",
    "  1e-2/log(log(rate.tab[rate.tab$model=='selection','n']))\n",
    "\n",
    "rates = hacked.rate.tab |> group_by(error.measure, model, mu) %>% \n",
    "\t\tgroup_modify(function(group,...) {\n",
    "\t\t    reg.data = group |> group_by(n) |> summarize(error=mean(error))\n",
    "\t\t    model = nls(formula = sqrt(error) ~ a*n^(-b), data=reg.data, \n",
    "\t\t                start=list(a=1, b=1/2))\n",
    "\t\t    data.frame(a=coef(model)[1], b=coef(model)[2])\n",
    "\t    })\n",
    "rates[rates$error.measure=='population', ]\n",
    "rates[rates$error.measure=='sample', ]\n",
    "rates[rates$error.measure=='point', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rate.predicted = hacked.rate.tab |> group_by(mu,model,error.measure) |>\n",
    "\t\tgroup_modify(function(group,...) {\n",
    "\t\t    reg.data = group |> group_by(n) |> summarize(error=mean(error))\n",
    "\t\t    model = nls(formula = sqrt(error) ~ a*n^(-b), data=reg.data, \n",
    "\t\t                start=list(a=1, b=1/2))\n",
    "\t\t    reg.data$error = predict(model)^2\n",
    "\t\t    reg.data\n",
    "\t    })\n",
    "\n",
    "rate.plots = map(unique(rate.tab$error.measure), function(measure) {\n",
    "  mse.with.errorbars.plot(rate.tab[rate.tab$error.measure == measure,]) + \n",
    "          geom_line(aes(x=n, y=error, color=model), alpha=.4, linewidth=2, \n",
    "\t\t\t\t            data=rate.predicted[rate.predicted$error.measure == measure, ]) +\n",
    "\t\t\t    facet_grid(cols=vars(mu), rows=vars(error.measure))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Styling**.\n",
    "Here's the comparison of the error prediction curves we get using\n",
    "our polynomial model $\\text{RMSE} \\approx \\alpha n^{-\\beta}$\n",
    "and the actual error curves.\n",
    "I've plotted the predictions curves as lines that matching the actual \n",
    "error curve in color, but wider and semi-transparent. That means that\n",
    "if the predictions are really good, what we'll see are thin lines with \n",
    "wide 'tubes' around them. You'll see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "walk(rate.plots, print) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about what this all means.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Let's start with the line, since all of our models can fit it. Have you\n",
    "estimated different rates of convergence $n^{-\\beta}$ in population mean\n",
    "squared error for the lines and monotone regression models?[^4] What do you make\n",
    "of this? Would you say that this summary was helpful or misleading? Why?\n",
    "What about sample mean squared error and squared error at the point\n",
    "$x=0$?\n",
    "\n",
    "**Tip.** It may be helpful to think about the values of $\\alpha$ you've\n",
    "estimated.\n",
    ":::\n",
    "\n",
    "[^4]: I left out the select-one-of-four model because the idea of a rate\n",
    "    for it is a little weird. Error is, after all, zero for most or all\n",
    "    replications for most $n$ we consider. To get at what's happening\n",
    "    with this estimator, we need to be talking about something like the\n",
    "    smallest sample size at which we tend to get zero error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Now let's think about the other three curves $\\mu$ that you've fit: the\n",
    "step, the stepline, and the sine. Are you seeing the same rates of\n",
    "convergence from linear and monotone regression that you got in the\n",
    "previous exercise, when $\\mu$ was the line? Does your answer depend on\n",
    "$\\mu$? If so, try to explain why you're seeing this variation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universality\n",
    "\n",
    "This is optional. Do it if you've got extra time. I'm not going to\n",
    "provide solutions for these, but we'll dig into these issues a little\n",
    "later on when we've got some theory to guide us.\n",
    "\n",
    "When we looked at the data, we saw that it looked roughly the same no\n",
    "matter whether we used gaussian, uniform, or t-distributed noise. So\n",
    "you'd expect that the MSE and rates of convergence you see wouldn't\n",
    "change much.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Change the distribution of the noise in the code above and rerun it.\n",
    "That's as easy as changing `noise.type='gaussian'` in the plot-pop-mse\n",
    "block to `noise.type='t'` or `noise.type='uniform'` and rerunning\n",
    "everything below that. Discuss whether you see any changes.\n",
    ":::\n",
    "\n",
    "The data certainly would look different if we increased the scale of the\n",
    "noise, say from having standard deviation $0.5$ as we've been using to\n",
    "having standard deviation $5$. However, it might be the case that having\n",
    "a little data with a little noise is similar to having a lot of data\n",
    "with a lot of noise. Look into whether that's the case.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Increase the standard deviation of the noise. All you have to do is\n",
    "change `sigma = .5` in the noise-distributions block above to `sigma=5`. \n",
    "While the quantitative result you see\n",
    "will change, i.e. you'll have more error, you might expect the\n",
    "qualitative, e.g. the relationships between types of MSE and the rates\n",
    "of convergence, to stay the same. Discuss what you find. Then repeat,\n",
    "instead decreasing the standard deviation of the noise, e.g. using\n",
    "`sigma=.05`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Using Tidyverse Tools\n",
    "\n",
    "While I don't often shill for the tidyverse, I found those tools pretty\n",
    "helpful here. If you're not familiar, the 'tidy' way to do things is to\n",
    "collect a bunch of stuff in one big dataframe. When I tabulated errors\n",
    "earlier, I put them in a data frame with columns `error`, `error.measure`, `n`, `mu`, `model`, and `noise`. \n",
    "This makes everything very easy to plot using ggplot. I'm using these\n",
    "commands to plot mine. I'm plotting the mean squared errors and squared\n",
    "error at the point $x=0$ separately because the scales tend to be\n",
    "different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(tab[tab$error.measure != 'point',], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "  geom_line() + geom_point() + facet_grid(cols=vars(mu), rows=vars(noise))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(tab[tab$error.measure == 'point', ], \n",
    "       aes(x=n, y=error, color=model, linetype=error.measure, shape=error.measure)) + \n",
    "    geom_line() + geom_point() + facet_grid(cols=vars(mu), rows=vars(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My code leans heavily on the function `map_dfr` in the tidyverse package **purrr**.^[It turns out that this function is deprecated, i.e. the maintainers of the package **purrr** are recommending that you stop using it because they're planning to remove it from the package at some point in the future. It's usually considered bad form to use deprecated functions, but it's really hard to avoid when you use the tidyverse, so I'm giving you code that does it. The package maintainers in the tidyverse group are always renaming functions and moving them from package to package with tiny changes, so if you wrote code using tidyverse stuff a few years ago, the odds are pretty good something you've used is deprecated at this point. That's usually considered bad form too, and because of that, many people I know have given up on the idea of using tidyverse stuff in any code that isn't essentially disposable. But there's a lot of useful stuff in there, so I try to take a more conservative approach unless I really need my code not to break, e.g. when I'm writing packages of my own that lots of people will be using. I don't really worry about tidyverse functions getting deprecated because it takes a while for them to actually go away and when they do, you can usually redefine the old functions in terms of the new ones with at most a couple lines of code.] \n",
    "\n",
    "This is really nothing special. It's just like Python's map or R's built-in lapply, except that\n",
    "it expects the function you're mapping to return a dataframe, and\n",
    "instead of collecting these dataframes in a list, it stacks them\n",
    "vertically into a single dataframe. It takes two arguments.\n",
    "\n",
    "1.  a list `list(a=x,b=y,c=z,...)`\n",
    "\n",
    "2.  a function `f(x)` that outputs a data.frame\n",
    "\n",
    "and returns `f(x)`, `f(y)`, `f(z)`, ... vertically stacked into a data\n",
    "frame `df`. And it has one other slightly convenient feature. If you\n",
    "pass the option .id='somestring' for some string id, it'll add a column\n",
    "`df$somestring` that stores the names `a`,`b`,`c`, ... in the rows\n",
    "corresponding to `f(x)`, `f(y)`, and `f(z)` respectively.\n",
    "\n",
    "You can use it to loop over models, error measures, curves, etc. You can\n",
    "nest calls to loop over multiple things; everything winds up getting\n",
    "stacked together.\n",
    "\n",
    "Here's some example code that might help a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mus=list(a=1, b=2, c=3)\n",
    "sigmas=list(small=.1, big=10)\n",
    "mean.squares = map_dfr(mus, .id='mu', function(mu) {\n",
    "                  map_dfr(sigmas, .id='sigma', function(sigma) {\n",
    "                    Y = mu + sigma*rnorm(10)\n",
    "                    data.frame(mean.square=mean(Y^2))\n",
    "                  })\n",
    "               })\n",
    "mean.squares"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
