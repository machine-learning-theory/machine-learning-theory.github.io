{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Summary \n",
    "\n",
    "So far, we've talked about least squares regression using four models. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat\\mu &= \\argmin_{m \\in \\mathcal{M}} \\frac{1}{n} \\sum_{i=1}^n \\{ Y_i - m(X_i)\\}^2 \\quad \\text{for} \n",
    "        && \\mathcal{M} = \\{ \\text{increasing functions} \\ m  \\} \\\\\n",
    "        &&& \\mathcal{M} = \\{ \\text{convex functions} \\ m \\} \\\\\n",
    "        &&& \\mathcal{M} = \\{ m \\ : \\ \\rho_{\\text{TV}}(m) \\le B \\} \\\\\n",
    "        &&& \\mathcal{M} = \\{ m \\ : \\ \\rho_{\\text{Lip}}(m) \\le B \\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For every one, our implementation boiled down to two steps. Given a sample $\\mathcal{X} = \\{(X_1,Y_1),\\dots,(X_n,Y_n)\\}$, we do this. \n",
    "1. Think about what these constraints tell us about restriction $m_{\\mid \\mathcal{X}}$ of the functions in our model $\\mathcal{M}$ to sample and choose the best of these. \n",
    "2. Extend this choice $\\hat \\mu_{\\mid \\mathcal{X}}$ to the real line in some way that keeps us in the model. \n",
    "  -  e.g. without increasing our seminorm $\\rho$\n",
    "  -  e.g.  violating our monotonicity or convexity constraints. \n",
    "\n",
    "This involves a bit of code but, with the help of CVXR, it's not too hard. Implementations are below. \n",
    "There's a lot of code there, but there's a lot of redundancy in it. If we wanted to boil this down to 50 or so lines, we could do that. All we'd have to do, really, is write more abstract fitting code and pass in the right constraints for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "  library(tidyverse)\n",
    "  library(CVXR)\n",
    "  CVXR::add_to_solver_blacklist('OSQP') \n",
    "})\n",
    "\n",
    "## A function we'll want. unique with an inverse mapping.\n",
    "\n",
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "a = c(1,2,3,3,4,5,5,5,6)\n",
    "unique.a=invert.unique(a)\n",
    "stopifnot(a[unique.a$elements][unique.a$inverse] == a)\n",
    "\n",
    "## Fitting Functions\n",
    "\n",
    "monotonereg = function(X, Y, decreasing = FALSE) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list(if (decreasing) { diff(m) <= 0 } else { diff(m) >= 0 })\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "convexreg = function(X, Y, concave = FALSE, monotone = NULL) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  # Interpretation (rearrange): secant slopes are increasing\n",
    "  uX = X[unique.X$elements]\n",
    "  ii = 1:(n-2)\n",
    "  constraints = \n",
    "    list(((m[ii+1]-m[ii])  * (uX[ii+2]-uX[ii]) - \n",
    "          (m[ii+2]-m[ii]) *  (uX[ii+1]-uX[ii])) * (-1)^concave <= 0)\n",
    "  if(!is.null(monotone)) { \n",
    "       decreasing = monotone == 'decreasing'\n",
    "       constraints = c(constraints, diff(m) * (-1)^decreasing >= 0)\n",
    "  }\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.convexreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"convexreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "bvreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list( sum(abs(diff(m))) <= B )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.bvreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"bvreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "lipreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  uX = X[unique.X$elements]\n",
    "  constraints = list( abs(diff(m)) <= B * diff(uX) )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.lipreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"lipreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "## Prediction Functions\n",
    "\n",
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.piecewise.constant = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  Y = model$mu.hat; X=model$X; x=newdata$X\n",
    "  # for each new data point x in newdata$X, \n",
    "  # find the closest observed X[k] left of x\n",
    "  # i.e., the largest k for which X[k] <= x \n",
    "  # this covers cases 1 and 2\n",
    "  # i will be a vector of these numbers k, with one for each x in newdata$X\n",
    "  i = findInterval(x, X) \n",
    "  # if there is no X[k] < x, findInterval tells us k=0\n",
    "  # to cover case 3, we want X[k] for k=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  Y[i]\n",
    "}\n",
    "\n",
    "predict.piecewise.linear = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  Y = model$mu.hat; X=model$X; x=newdata$X; n = length(X) \n",
    "  # for each new data point x[k]\n",
    "  # find the closest observed X[i[k]] left of x[k]\n",
    "  # i.e., i[k] is the largest integer i for which X[i] <= x[k] \n",
    "  i = findInterval(newdata$X, X) \n",
    "  # If there is no X[i] < x[k], findInterval sets i[k]=0\n",
    "  #  and we'll want to act as if we'd gotten 1 so we use the\n",
    "  #  line through (X[1], Y[1])  and (X[2], Y[2])\n",
    "  # If that k is n, we'll want to act as if we'd gotten n-1 so we use \n",
    "  #  the line through (X[n-1], Y[n-1])  and (X[n], Y[n])\n",
    "  i[i==0] = 1; i[i==n] = n-1\n",
    "  # make a prediction using the formula y - y0 = (x-x0) * slope \n",
    "  Y[i] + (x-X[i]) * (Y[i+1]-Y[i])/(X[i+1]-X[i])\n",
    "}\n",
    "\n",
    "predict.monotonereg = predict.piecewise.constant\n",
    "predict.convexreg = predict.piecewise.linear\n",
    "predict.bvreg = predict.piecewise.constant\n",
    "predict.lipreg = predict.piecewise.linear\n",
    "\n",
    "## Conveniences\n",
    "\n",
    "prediction.function = function(model) { \n",
    "  if(class(model)=='function') { model }\n",
    "  else { function(x) { predict(model, newdata=data.frame(X=x)) } }\n",
    "}\n",
    "\n",
    "## Styles\n",
    "\n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t\t\t\t\tpanel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\tpanel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t      panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example where they all work. If you uncomment line 4, most of them don't. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "n = 50\n",
    "mu = function(x) { sin(2*pi*x) } \n",
    "#mu = function(x) { -x^2 }\n",
    "X = runif(n)\n",
    "Y = mu(X) + .25*rnorm(n)\n",
    "\n",
    "models = list(monotone = monotonereg(X, Y),\n",
    "              convex = convexreg(X, Y),\n",
    "              bv = bvreg(X, Y),\n",
    "              lip = lipreg(X, Y, B=2*pi)) |> map(prediction.function)\n",
    "\n",
    "x = seq(0, 1, by=.001) \n",
    "ggplot() + \n",
    "  geom_point(data=data.frame(X, Y), aes(X, Y), color='black', alpha=.3) + \n",
    "  geom_line(data=data.frame(x=x, y=mu(x)), aes(x, y), color='black', linewidth=2, alpha=.3) + \n",
    "  geom_line(data=data.frame(x=x, y=models$monotone(x)), aes(x, y), color='red', alpha=.3) + \n",
    "  geom_line(data=data.frame(x=x, y=models$convex(x)), aes(x, y), color='blue', alpha=.3) + \n",
    "  geom_line(data=data.frame(x=x, y=models$bv(x)), aes(x, y), color='green', alpha=.3) + \n",
    "  geom_line(data=data.frame(x=x, y=models$lip(x)), aes(x, y), color='purple', alpha=.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are these models? \n",
    "\n",
    "In differentiable terms ...\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{M} &= \\{ m \\ : \\ m'(x) \\ge 0 \\}   && \\text{monotone} \\\\\n",
    "\\mathcal{M} &= \\{ m \\ : \\ m''(x) \\ge 0 \\}  && \\text{convex}  \\\\\n",
    "\\mathcal{M} &= \\{ m \\ : \\lVert m'(x) \\rVert_{L_1} \\le B \\}  && \\text{bounded variation} \\\\\n",
    "\\mathcal{M} &= \\{ m \\ : \\lVert m'(x) \\rVert_{L_\\infty} \\le B \\}  && \\text{Lipschitz} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "But if we tried to optimize over sets of differentiable functions like this, we'd converge to something nondifferentiable. \n",
    "So we use definitions that don't require differentiability instead.  \n",
    "\n",
    "- The **monotone** case is easy enough to generalize. Probably easier without the derivative.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{M} &= \\{ m \\ :  \\ m(a) \\le m(b) \\ \\forall a \\le b \\}  \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- The **bounded variation** case we covered in class. It took a little thought, but the result was simple. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{M} &= \\{ m \\ :  \\rho_{TB}(m) \\le B  \\} \\quad \n",
    "\\text{for} \\quad \\rho_{TV}(m)=\\sup_{\\substack{\\text{increasing sequences } \\\\\n",
    "0=x_1 < x_2 < \\dots < x_n=1 }} \\sum_{i=1}^{n-1} |m(x_{i+1}) - m(x_i)|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- The other two are about the slopes of *secants*. Let's think this through.\n",
    "- The differentiable curve below, $m(x)=x^3/3$, is *convex* with $\\lVert m'(x) \\rVert_{L_\\infty} = 1$.  \n",
    "  - What do our differentiable-case definitions tell us about the slopes of *tangents* to this curve? \n",
    "  - What does this tell us about the slopes of *secants*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = function(x) { x^3/3 } \n",
    "ggplot() + geom_line(aes(x=x, y=mu(x)), linewidth=2, alpha=.5) + ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tip*. First, think about the secant between $x$ and $x+h$ for small $h$. Then, think about the mean value theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The usual definition of convexity\n",
    "\n",
    "The usual definition of convexity isn't about secant slopes. It's about secants lying above the curve. $m$ is convex if and only if, for all $a \\le b$,\n",
    "\n",
    "$$\n",
    "(1-\\lambda)m(a) + \\lambda m(b) \\ge m\\{(1-\\lambda)a + \\lambda b\\} \\quad \\text{for all } \\lambda \\in [0,1]\n",
    "$$\n",
    "\n",
    "What does that mean in plot terms? Draw it! \n",
    "\n",
    "How is this related to secant slopes? Well, let's transform it into a statement about slopes. \n",
    "Let's think about doing that to the right-hand side of the inequality above. But obviously,\n",
    "to get an equivalent inequality, we need to apply the same transformation to the left-hand side. \n",
    "\n",
    "To get a secant slope, we need to ...\n",
    "1. Get a 'rise' by subtracting $m(x)$ at some $x$ . I like $m(a)$. \n",
    "2. Divide the corresponding 'run' \n",
    "\n",
    "What does the resulting inequality mean? Draw it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Next Class\n",
    "\n",
    "We'll talk about Sobolev models.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{M} &= \\{ m \\ : \\ \\lVert m' \\rVert_{L_2} \\le B \\}  && \\text{Sobolev} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is a lot like the Lipchitz and BV models, but we'll use pretty different tools to analyze them.\n",
    "- We'll think about $\\frac{d^2}{dx^2}$ as a self-adjoint operator on periodic functions. \n",
    "- And use the eigenvalues and eigenfunctions of this operator to understand it.\n",
    "- Should we review that stuff from the inner product spaces homework today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Let's look at some examples from the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Restriction to the Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Slow Way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension to the Real Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something that doesn't work. \n",
    "Why doesn't piecewise-constant extension work for the Lipschitz and convex models? \n",
    "### Something that does. \n",
    "**Common key idea**. If we have a piecewise-linear curve, each secant slope is a weighted average of segment slopes.\n",
    "\n",
    "1. Why does this imply the piecewise-linear extension of a $B$-Lipschitz function on $\\mathcal{X}$ is $B$-Lipschitz?\n",
    "2. Why does this imply the piecewise-linear extension of a convex function is convex? \n",
    "\n",
    "How does this tell us extending our restricted solution to the real line gives us a solution to the unrestricted problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Implementation\n",
    "\n",
    "In a naive implementation, we get *a lot* of constraints. Roughly how many?\n",
    "  - Lipschitz?\n",
    "  - Convex?\n",
    "  \n",
    "How can we reduce the number of constraints? Think about what *local* properties tell us about the *global* behavior.  \n",
    "\n",
    "- Lipschitz case. \n",
    "- Convex case. Why is it a little less simple? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rates of Convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ns = c(25,50,100,200,400,800,1600)\n",
    "x = seq(0,1,by=.001)\n",
    "\n",
    "mu = function(x) { x }\n",
    "sigma = .5\n",
    "\n",
    "models  = list(lines = \\(X,Y)lm(Y~X),\n",
    "               monotone = monotonereg,\n",
    "\t             bv         = bvreg,\n",
    "               lip        = lipreg,\n",
    "               convex     = convexreg,\n",
    "               convex.monotone = \\(X,Y) convexreg(X,Y,monotone='increasing'))\n",
    "               \n",
    "errors = list(sample     = function(mu.hat, mu, X) { mean( (mu.hat(X)-mu(X))^2 ) },\n",
    "\t            population = function(mu.hat, mu, X) { mean( (mu.hat(x)-mu(x))^2 ) })\n",
    "\n",
    "tabulate.errors.for.sample = function(X,Y,mu,ns) { \n",
    "  purrr::map_dfr(ns, function(n) {\n",
    "    purrr::map_dfr(models, .id='model', function(model.fit) {\n",
    "      tryCatch({\n",
    "        model = model.fit(X[1:n], Y[1:n])\n",
    "        purrr::map_dfr(errors, .id='error.measure', function(error) {\n",
    "          data.frame(n=n, error=error(prediction.function(model), mu, X[1:n]))\n",
    "        })\n",
    "      }, error = function(e) {})\n",
    "    })\n",
    "  })\n",
    "}\n",
    "\n",
    "tabulate.errors = function(models, replications=10) { \n",
    "  purrr::map_dfr(1:replications, .id='rep', function(rep) {\n",
    "    X           = runif(max(ns))\n",
    "    epsilon     = sigma*rnorm(length(X))\n",
    "    Y = mu(X) + epsilon\n",
    "    tabulate.errors.for.sample(X, Y, mu, ns)\n",
    "  })\n",
    "}\n",
    "\n",
    "set.seed(4)\n",
    "tab = tabulate.errors(models, replications=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rates = tab %>% \n",
    "    group_by(error.measure, model) %>% \n",
    "\tgroup_modify(function(group,...) {\n",
    "\t    reg.data = group %>% group_by(n) %>% summarize(error=mean(error))\n",
    "\t    model = nls(formula = sqrt(error) ~ a*n^(-b), data=reg.data, \n",
    "\t                start=list(a=1, b=1/2))\n",
    "\t     data.frame(a=coef(model)[1], b=coef(model)[2])\n",
    "\t})\n",
    "\n",
    "rate.predicted = tab %>% \n",
    "    group_by(model,error.measure) %>% \n",
    "\tgroup_modify(function(group,...) {\n",
    "\t    reg.data = group %>% group_by(n) %>% summarize(error=mean(error))\n",
    "\t    model = nls(formula = sqrt(error) ~ a*n^(-b), data=reg.data, \n",
    "\t    \t        start=list(a=1, b=1/2))\n",
    "\t    reg.data$error = predict(model)^2\n",
    "\t    reg.data\n",
    "\t})\n",
    "\n",
    "\n",
    "plot.error.curves = function(error.measure) { \n",
    "    ggplot(tab[tab$error.measure==error.measure,], \n",
    "       aes(x=n, y=error, color=model)) + \n",
    "    stat_summary(geom='line', fun=mean) +  \n",
    "    stat_summary(geom='pointrange', fun.data=mean_se,\n",
    "    \t\t position=position_dodge(5)) \n",
    "}\n",
    "plot.error.curves.with.rate = function(error.measure) { \n",
    "    plot.error.curves(error.measure) +\n",
    "    geom_line(aes(x=n, y=error, color=model), alpha=.4, linewidth=2, \n",
    "\tdata=rate.predicted[rate.predicted$error.measure == error.measure, ])\n",
    "}\n",
    "\n",
    "plot.error.curves.with.rate('population') + coord_cartesian(ylim=c(0,.025))\n",
    "rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note differences in rate between \n",
    "  - Lines \n",
    "  - Convex \n",
    "  - Monotone, Bounded Variation, and Lipschitz\n",
    "\n",
    "- What's that about? Intuition? \n",
    "  - Note: monotone convex and convex are get pretty similar rates. \n",
    "  - So you can think of the convex model as 'smaller' than the monotone model.  \n",
    "  - But the Lipschitz model is 'smaller' than the 'Bounded Variation' model. Why no difference in rate there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maybe a constraint on second derivative is stronger than a constraint on the first \n",
    "   - e.g. Think about what taylor approximation tells you when you have 1 vs. 2 bounded derivatives.\n",
    "   - e.g. Think about a way of counting 'meaningfully different' increasing curves. What proportion of them are convex?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
