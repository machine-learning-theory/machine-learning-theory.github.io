{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To prepare for our first lab, let's develop some familiarity with\n",
    "`CVXR`. It's the `R` package we'll be using to formulate and solve\n",
    "optimization problems that come up throughout the semester. In this\n",
    "homework, we'll use it for linear regression. That is, we'll solve this\n",
    "optimization problem.\n",
    "\n",
    "$$\n",
    "\\hat\\mu(x) = \\hat\\beta_0 + \\hat\\beta_1 x \\quad \\text{ where } \\quad \\hat \\beta = \\operatorname*{argmin}_{b \\in \\mathbb{R}^2} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - (b_0 + b_1 X_i) \\}^2.\n",
    "$$\n",
    "\n",
    "We could, of course, just use `lm` for this, but we're going to be\n",
    "solving some other problems `lm` can't handle and we can use the\n",
    "practice. `CVXR` lets us write down convex optimization problems like\n",
    "the one above in a special subset of ordinary `R` code. It chooses an\n",
    "appropriate numerical solver for us, translates our input into what that\n",
    "solver expects, runs it, and gives us the result. So in essence, if you\n",
    "write down an expression to optimize and a list of constraints in\n",
    "language that looks more or less like math, it gives you the solution.\n",
    "\n",
    "We'll use a few libraries: `CVXR` for optimization, `ggplot2` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(ggplot2)\n",
    "    library(CVXR)\n",
    "})\n",
    "\n",
    "# OSQP claims some feasible problems aren't, so we'll tell CVXR not to use it\n",
    "CVXR::add_to_solver_blacklist('OSQP')  \n",
    "\n",
    "# And we'll style our plots  \n",
    "theme_update(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    panel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                    legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\tpanel.grid.major=element_line(color=rgb(1,0,0,.1,  maxColorValue=1)),\n",
    "\t        panel.grid.minor=element_line(color=rgb(0,0,1,.1,  maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting our curve\n",
    "\n",
    "We'll be using `CVXR` a lot, so you'll get used to it. To get a sense \n",
    "of how it works, let's take a look at the `CVXR` code for fitting a line\n",
    "to a set of points $(X_1, Y_1) \\ldots (X_n, Y_n)$. \n",
    "\n",
    "\n",
    "There are basically four things we need to do in this code.\n",
    "\n",
    "1. \n",
    "  - a. We specify the variable(s) we're optimizing over. This is a vector $b \\in \\mathbb{R}^2$,\n",
    "  so we'll tell `CVXR` we're working with a vector of two variables called `b`.  \n",
    "  - b. What we're interested in are the set of predictions \n",
    "    $m(x) = b_0 + b_1 x$ corresponding to those variables. So we'll \n",
    "    write, in `R` terms, an expression for the vector of predictions \n",
    "    $m(X_1) \\ldots m(X_n)$ in terms of those two variables.\n",
    "\n",
    "2.  We specify, in terms of those predictions, what we want to minimize.\n",
    "    That's mean squared error,\n",
    "    $\\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i) \\}^2$.\n",
    "\n",
    "3.  We ask CVXR to actually minimize it and to tell us at which values\n",
    "    of $b$ this minimum occurs.\n",
    "\n",
    "4.  We organize the results into a structure that'll be convenient\n",
    "    later: a list with our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "linereg = function(X,Y) {  \n",
    "  # Here's Step 1a.\n",
    "  # We tell CVXR we're thinking about a vector b in R^2\n",
    "  b = Variable(2)\n",
    "  # Here's Step 1b.\n",
    "  # We tell CVXR how to compute the predictions m(X) in terms of b.\n",
    "  m = b[1] + b[2]*X\n",
    "\n",
    "  # Here's Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - m)^2) / n\n",
    "\n",
    "  # Here's Step 3.\n",
    "  # We ask CVXR to minimize mean squared error and ask for vector b that does it.\n",
    "  solved = solve(Problem(Minimize(mse))) \n",
    "  beta.hat = solved$getValue(b)\n",
    "  mu.hat = solved$getValue(m)\n",
    "  \n",
    "  # Here's Step 4: a little boilerplate to make it idiomatic R. \n",
    "  #  1. we record beta.hat and corresponding predictions mu.hat in a list. We also record the input data.\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.linereg\n",
    "  #  3. we return the list\n",
    "  model = list(beta.hat=beta.hat, mu.hat=mu.hat, input=list(X=X,Y=Y))\n",
    "  attr(model, \"class\") = \"linereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note about Object Oriented Programming in R\n",
    "\n",
    "Object Oriented Programming in R is, at a basic level, very easy. If we\n",
    "have a list `that.list`, and we've set its 'class' attribute to\n",
    "`linearreg`, then when we call `predict(that.list, ...)`, `R` will\n",
    "actually call `predict.linearreg(that.list, ...)`. This is useful\n",
    "because it means we don't need to change code that makes and uses\n",
    "predictions when we change our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Trying it out\n",
    "\n",
    "Now let's fit some data and plot a curve through it. The data we'll be\n",
    "looking at is fake and stylized. We'll look at points sampled around a\n",
    "'stepline' function that jumps from 0 to the line $y=x$ at $x=0.5$. \n",
    "\n",
    "$$\n",
    "\\mu(x) = \\begin{cases} 0 & \\quad \\text{if} \\quad x < .5 \\\\ \n",
    "                       x & \\quad \\text{if} \\quad x \\ge .5 \\end{cases}\n",
    "$$ \n",
    "\n",
    "Let's start by plotting the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = function(x) { x*(x >= .5) }\n",
    "sigma = .1\n",
    "\n",
    "n = 100\n",
    "X = runif(n)\n",
    "Y = mu(X) + sigma*rnorm(n)\n",
    "\n",
    "observations = ggplot() + geom_point(aes(x=X, y=Y), alpha=.2)\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `linereg` to fit a line $\\hat\\mu(x)=\\hat\\beta_0 + \\hat\\beta_1 x$ and take a look at the predictions $\\hat\\mu(X_i)$ of our least squares line at the observed points $X_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model = linereg(X,Y)\n",
    "mu.hat = model$mu.hat\n",
    "observations + geom_point(aes(x=X, y=mu.hat), color='blue', alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions between the observations\n",
    "\n",
    "Let's implement `predict.linearreg`. `R` convention expects a predict\n",
    "function like this to take two arguments.\n",
    "\n",
    "1. This list we've been\n",
    "talking about, which describes the curve we've fit.\n",
    "2. A data frame describing the points $X_i$ that we want to make predictions at. \n",
    "\n",
    "Conventionally that the second argument is optional, and if it's not passed,\n",
    "the function should make predictions at the the points $X_i$ you used to fit the model. \n",
    "\n",
    "Take a look at this implementation. This is the function that gets called when we say\n",
    "`predict(model)` or `predict(model, newdata=data.frame(x=...))` for a model with class `'linereg'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predict.linereg = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "    X=newdata$X \n",
    "    beta.hat = model$beta.hat\n",
    "    beta.hat[1] + X*beta.hat[2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy, right? We're making predictions\n",
    "$\\hat\\mu(x) = \\hat\\beta_0 + \\hat\\beta_1 X$ for the points $X$ in the\n",
    "data frame `newdata`. And if you don't pass `newdata` we just use the\n",
    "points $X$ from our training data, which we saved when we wrote\n",
    "`linearreg` just for this purpose.\n",
    "\n",
    "Now let's fit the data and plot a curve through it. To plot this curve,\n",
    "we'll make predictions at a points on a dense sequence of Xs and ask `R`\n",
    "to draw a line through those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model = linereg(X,Y)\n",
    "\n",
    "x = seq(0,1,by=.001)\n",
    "muhat.x = predict(model, newdata=data.frame(X=x))\n",
    "observations + geom_line(aes(x=x, y=muhat.x), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that we've done things right by using the built-in\n",
    "function `lm` to do the same. This is a one line change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model = lm(Y~X)\n",
    "\n",
    "x = seq(0,1,by=.001)\n",
    "muhat.x = predict(model, newdata=data.frame(X=x))\n",
    "observations + geom_line(aes(x=x, y=muhat.x), color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Linear Regression\n",
    "\n",
    "Sometimes we have domain knowledge we want to use when we fit our line. \n",
    "For example, we might think that our slope $b_1$ should be non-negative. \n",
    "Like if we're predicting height in childhood as a function of age: kids grow, but they\n",
    "don't shrink. While it might not be necessary in a simple 2-coefficient\n",
    "model, in more complex models imposing constraints based on domain\n",
    "knowledge tends to improve performance. \n",
    "\n",
    "To encode this knowledge, we add a step to what we've done above.\n",
    "\n",
    " - Step 2a. We specify constraints on the coefficients beta that our\n",
    "solution must satisfy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "increasinglinereg = function(X,Y) {  \n",
    "  # Here's Step 1. Same as before.\n",
    "  # 1a. We tell CVXR we're thinking about a vector b in R^2\n",
    "  b = Variable(2)\n",
    "  # 1b. We tell CVXR how to compute the predictions m(X) in terms of b.\n",
    "  m = b[1] + b[2]*X\n",
    "  \n",
    "  # Here's Step 2. Same as before.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - m)^2) / n\n",
    "\n",
    "  # Here's Step 2a. This is new.\n",
    "  # We specify our constraints.\n",
    "  nonneg.constraint = b[2] >= 0\n",
    "  \n",
    "  # Here's Step 3. This changes to incorporate the constraint. \n",
    "  # We ask CVXR to minimize mean squared error, subject to our constraints,\n",
    "  # and ask for vector b that does it. Note that because CVXR allows us to\n",
    "  # use multiple constraints, it expects us to pass a list of constraints;\n",
    "  # because we have only one here, we pass it a one-element list.\n",
    "  solved = solve(Problem(Minimize(mse), list(nonneg.constraint)))\n",
    "  beta.hat = solved$getValue(b)\n",
    "  mu.hat = solved$getValue(m)\n",
    "  \n",
    "  # Here's Step 4: a little boilerplate to make it idiomatic R. Same as before.\n",
    "  #  1. we record the input X and the solution beta.hat and corresponding predictions mu.hat in a list\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.increasinglinereg\n",
    "  #  3. we return the list\n",
    "  model = list(beta.hat=beta.hat, mu.hat=mu.hat, input=list(X=X,Y=Y))\n",
    "  attr(model, \"class\") = \"increasinglinereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while we've changed the way we solve for our coefficients, we're making predictions just as before:\n",
    "$\\hat \\mu(x) = \\hat\\beta_0 + \\hat\\beta_1 x$. That means we can use the prediction function we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predict.increasinglinereg = predict.linereg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can make and plot our predictions just as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model = increasinglinereg(X,Y)\n",
    "\n",
    "x = seq(0,1,by=.001)\n",
    "muhat.x = predict(model, newdata=data.frame(X=x))\n",
    "observations + geom_line(aes(x=x, y=muhat.x), color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has changed because our data follows an increasing trend: we get\n",
    "$\\hat\\beta_1 \\ge 0$ even without the constraint. But let's check that\n",
    "something will change if our data isn't increasing. We'll look at points\n",
    "sampled around a negated version of our 'stepline'. \n",
    "\n",
    "$$\n",
    "\\mu(x) = \\begin{cases} \\hphantom{-}0 & \\quad \\text{if} \\quad x < .5 \\\\ \n",
    "                       -x & \\quad \\text{if} \\quad x \\ge .5 \\end{cases}\n",
    "$$ \n",
    "\n",
    "Here's the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = function(x) { -x*(x >= .5) }\n",
    "sigma = .1\n",
    "\n",
    "n = 100\n",
    "X = runif(n)\n",
    "Y = mu(X) + sigma*rnorm(n)\n",
    "\n",
    "observations = ggplot() + geom_point(aes(x=X, y=Y), alpha=.2)\n",
    "observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are fits using `linereg` (the unconstrained regression) in\n",
    "blue and `increasinglinereg` (the increasingness-constrained one) in red. The\n",
    "increasingness-constrained regression can't fit the data, so it does the\n",
    "best it can. It gives us a horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model.uc = linereg(X,Y)\n",
    "model.nn = increasinglinereg(X,Y)\n",
    "\n",
    "x = seq(0,1,by=.001)\n",
    "muhat.x.uc = predict(model.uc, newdata=data.frame(X=x))\n",
    "muhat.x.nn = predict(model.nn, newdata=data.frame(X=x))\n",
    "\n",
    "observations + geom_line(aes(x=x, y=muhat.x.uc), color='blue') +\n",
    "               geom_line(aes(x=x, y=muhat.x.nn), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Doing it yourself\n",
    "\n",
    "Try implementing a version of linear regression in which the coefficient\n",
    "$\\beta_1$ is constrained to be *small* in magnitude. Given some bound\n",
    "$B$ on how large it can be, we'll solve this problem.\n",
    "\n",
    "$$\n",
    "\\hat\\mu(x) = \\hat\\beta_0 + \\hat\\beta_1 x \\quad \\text{ where } \\quad\n",
    "\\hat \\beta = \\operatorname*{argmin}_{\\substack{b \\in \\mathbb{R}^2 \\\\ \\lvert b_1\\rvert \\le B}} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - (b_0 + b_1 X_i) \\}^2.\n",
    "$$\n",
    "\n",
    "Give it a shot. Here's a template to work with. \n",
    "\n",
    "**Tip**. `CVXR` knows what functions like `abs` mean, so you can use them in your constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "boundedlinereg = function(X,Y,B) {  \n",
    "  # Step 1. \n",
    "  # We tell CVXR we're thinking about a vector b in R^2\n",
    "  b = Variable(2) \n",
    "\n",
    "  # Step 2. \n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - (b[1] + b[2]*X))^2) / n\n",
    "\n",
    "  # Step 2a. \n",
    "  # We specify our constraints.\n",
    "  boundedness.constraint = abs(b[2]) <= B\n",
    "\n",
    "  # Step 3. \n",
    "  # We ask CVXR to minimize mean squared error, subject to our constraints,\n",
    "  # and ask for vector b that does it. \n",
    "  solved = solve(Problem(Minimize(mse), list(boundedness.constraint))) \n",
    "  beta.hat = solved$getValue(b)\n",
    "\n",
    "  # Here's Step 4: a little boilerplate to make it idiomatic R. Same as before.\n",
    "  #  1. we record the input X and the solution beta.hat in a list\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.boundedlinereg\n",
    "  #  3. we return the list\n",
    "  model = list(beta.hat=beta.hat, input=list(X=X,Y=Y,B=B))\n",
    "  attr(model, \"class\") = \"boundedlinereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "predict.boundedlinereg = predict.linereg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use your code to make predictions for two values of the bound\n",
    "$B$: a big one ($B=1.0$ in blue) and a small one ($B=0.1$ in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model.big   = boundedlinereg(X,Y, B=1.0)\n",
    "model.small = boundedlinereg(X,Y, B=0.1)\n",
    "\n",
    "x = seq(0,1,by=.001)\n",
    "muhat.x.big   = predict(model.big,   newdata=data.frame(X=x))\n",
    "muhat.x.small = predict(model.small, newdata=data.frame(X=x))\n",
    "\n",
    "observations + geom_line(aes(x=x, y=muhat.x.big),   color='blue') +\n",
    "               geom_line(aes(x=x, y=muhat.x.small), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while we do a better job with the larger value of the bound,\n",
    "we're still not doing a great job. After all, the curve $\\mu(x)$ our\n",
    "data is sampled around isn't a line. So let's conclude for now. \n",
    "\n",
    "In lab, we'll start working on fitting *monotone* curves, i.e. curves that are increasing or decreasing, and we'll do better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
