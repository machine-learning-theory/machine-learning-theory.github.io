{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Summary\n",
    "\n",
    "Today, we're going to be picking apart *Least Squares Model Selection*.\n",
    "We'll be refining our theoretical argument from last lecture to guide us\n",
    "here, investigating how well rankings of curves based on squared error\n",
    "loss lines up with a rankings based on sample RMSE. \n",
    "\n",
    "We'll be using code from previous labs so we have some curves to rank\n",
    "and choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "    CVXR::add_to_solver_blacklist('OSQP')  \n",
    "})\n",
    "\n",
    "## Styles\n",
    "\n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t  \t\t\tpanel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\t  panel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t              panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t          axis.ticks.x = element_blank(),\n",
    "\t\t          axis.ticks.y = element_blank(),\n",
    "\t\t          axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## A function we'll want. unique with an inverse mapping.\n",
    "\n",
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "a = c(1,2,3,3,4,5,5,5,6)\n",
    "unique.a=invert.unique(a)\n",
    "stopifnot(a[unique.a$elements][unique.a$inverse] == a)\n",
    "\n",
    "## Fitting Functions\n",
    "\n",
    "monotonereg = function(X, Y, decreasing = FALSE) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list(if (decreasing) { diff(m) <= 0 } else { diff(m) >= 0 })\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "convexreg = function(X, Y, concave = FALSE, monotone = NULL) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  # Interpretation (rearrange): secant slopes are increasing\n",
    "  uX = X[unique.X$elements]\n",
    "  ii = 1:(n-2)\n",
    "  constraints = \n",
    "    list(((m[ii+1]-m[ii])  * (uX[ii+2]-uX[ii]) - \n",
    "          (m[ii+2]-m[ii]) *  (uX[ii+1]-uX[ii])) * (-1)^concave <= 0)\n",
    "  if(!is.null(monotone)) { \n",
    "       decreasing = monotone == 'decreasing'\n",
    "       constraints = c(constraints, diff(m) * (-1)^decreasing >= 0)\n",
    "  }\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.convexreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"convexreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "bvreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list( sum(abs(diff(m))) <= B )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.bvreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"bvreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "lipreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  uX = X[unique.X$elements]\n",
    "  constraints = list( abs(diff(m)) <= B * diff(uX) )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.lipreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"lipreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "## Prediction Functions\n",
    "\n",
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.piecewise.constant = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  Y = model$mu.hat; X=model$X; x=newdata$X\n",
    "  # for each new data point x in newdata$X, \n",
    "  # find the closest observed X[k] left of x\n",
    "  # i.e., the largest k for which X[k] <= x \n",
    "  # this covers cases 1 and 2\n",
    "  # i will be a vector of these numbers k, with one for each x in newdata$X\n",
    "  i = findInterval(x, X) \n",
    "  # if there is no X[k] < x, findInterval tells us k=0\n",
    "  # to cover case 3, we want X[k] for k=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  Y[i]\n",
    "}\n",
    "\n",
    "predict.piecewise.linear = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  Y = model$mu.hat; X=model$X; x=newdata$X; n = length(X) \n",
    "  # for each new data point x[k]\n",
    "  # find the closest observed X[i[k]] left of x[k]\n",
    "  # i.e., i[k] is the largest integer i for which X[i] <= x[k] \n",
    "  i = findInterval(newdata$X, X) \n",
    "  # If there is no X[i] < x[k], findInterval sets i[k]=0\n",
    "  #  and we'll want to act as if we'd gotten 1 so we use the\n",
    "  #  line through (X[1], Y[1])  and (X[2], Y[2])\n",
    "  # If that k is n, we'll want to act as if we'd gotten n-1 so we use \n",
    "  #  the line through (X[n-1], Y[n-1])  and (X[n], Y[n])\n",
    "  i[i==0] = 1; i[i==n] = n-1\n",
    "  # make a prediction using the formula y - y0 = (x-x0) * slope \n",
    "  Y[i] + (x-X[i]) * (Y[i+1]-Y[i])/(X[i+1]-X[i])\n",
    "}\n",
    "\n",
    "predict.monotonereg = predict.piecewise.constant\n",
    "predict.convexreg = predict.piecewise.linear\n",
    "predict.bvreg = predict.piecewise.constant\n",
    "predict.lipreg = predict.piecewise.linear\n",
    "\n",
    "## Conveniences\n",
    "\n",
    "prediction.function = function(model) { \n",
    "  if(class(model)=='function') { model }\n",
    "  else { function(x) { predict(model, newdata=data.frame(X=x)) } }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares in Finite Models\n",
    "\n",
    "Let's suppose we have independent and identically distributed\n",
    "observations $(X_1,Y_1) \\ldots (X_n,Y_n)$ where\n",
    "$Y_i=\\mu(X_i)+\\varepsilon_i$ and $\\varepsilon_i$ is normal with\n",
    "mean-zero and standard deviation $\\sigma$. We're going to be talking\n",
    "about a least squares estimator.\n",
    "\n",
    "$$ \\hat\\mu = \\operatorname*{argmin}_{m \\in \\mathcal{M}} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i) \\}^2 $$\n",
    "\n",
    "Here's the error bound we proved in lecture. For a model $\\mathcal{M}$\n",
    "containing no more than $K$ curves,\n",
    "$$ \\lVert\\hat\\mu - \\mu\\rVert_{L_2(Pn)} \\le 4\\sigma\\sqrt{\\frac{\\log(K)}{n}}  \\ \\text{ with probability } \\ 1-1/K \\ \\textbf{ if } \\ \\mu \\in \\mathcal{M}. $$\n",
    "This is not all that useful. It's a bound that's valid when we have a finite model (a set of finitely many curves) that contains $\\mu$.\n",
    "That's not going to happen.\n",
    "\n",
    "The good news is that it's not a dead end. We can adapt the argument we\n",
    "used to get a more general bound that has meaningful implications. \n",
    "$$ \n",
    "\\lVert\\hat\\mu - \\mu\\rVert_{L_2(Pn)} \\le 4\\sigma\\sqrt{\\frac{\\log(K)}{n}} + 3\\lVert \\mu^\\star -\\mu \\rVert_{L_2(P_n)} \\ \\text{ with probability } \\ 1-1/K \\ \\textbf{ for any } \\ \\mu^\\star \\in \\mathcal{M}. \n",
    "$$\n",
    "This bound applies whether or not $\\mu$ is in our model $\\mathcal{M}$. \n",
    "And what is says, if we take $\\mu^star$ to be our model's best approximation\n",
    "to $\\mu$, that the curve we do select isn't much further than the best option\n",
    "in our model. In the case that $\\mu$ *is* in our model, that means taking $\\mu^\\star=\\mu$, \n",
    "so this implies the bound above. There's a proof of this more general bound\n",
    "at the end of this document. \n",
    "\n",
    "## Model selection\n",
    "\n",
    "We're going to be talking about this in the context of a model selection problem.\n",
    "The reason is that it's the only context anyone really\n",
    "uses finite models. Consider these examples.\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\mathcal{M}  &= \\{m(x)=x^T b : b \\in \\mathbb{R}^k\\} && \\text{ Linear model } \\\\\n",
    "\\mathcal{M} &= \\{ \\text{ increasing } \\ m(x) \\} && \\text{ Monotone model } \\\\\n",
    "\\mathcal{M}  &= \\{ m(x) : \\int_0^1 \\lvert m'(x) \\rvert dx \\le B_{TV} \\} && \\text{ Bounded variation model } \\\\\n",
    "\\mathcal{M}  &= \\{ m(x) : \\max_{x \\in [0,1]} \\lvert m'(x) \\rvert \\le B_{Lip} \\} && \\text{ Lipschitz model }\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "All of them contain infinitely many curves. But sometimes, we fit a few\n",
    "models like this. After all, we don't know which is best. Maybe we're\n",
    "choosing within a sort of *model family*, like these examples.\n",
    "\n",
    "1.  Polynomial models of different orders.\n",
    "2.  Models defined by different shape constraints, e.g. increasing vs.\n",
    "    convex models.\n",
    "3.  Balls of different radius in the same seminorm. For example,\n",
    "    -   Bounded variation models for different bounds $B_{TV}$ on total\n",
    "        variation.\n",
    "    -   Lipschitz models for different bounds $B_{Lip}$ on the Lipschitz\n",
    "        constant.\n",
    "\n",
    "Or maybe we're choosing between as well as within families like this. In\n",
    "any case, if we fit $K$ models $\\mathcal{M}^1 \\ldots \\mathcal{M}^K$, we\n",
    "get $K$ least squares estimators $\\hat \\mu^1 \\ldots \\hat \\mu^K$. And we\n",
    "can define a new, finite, model that contains these least squares\n",
    "estimators. $$ \\mathcal{M} = \\{ \\hat \\mu^1 \\ldots \\hat \\mu^K \\}. $$\n",
    "Choosing one of these curves, which if we prefer we can think of as\n",
    "choosing a model from the set $\\mathcal{M}^1 \\ldots \\mathcal{M}^K$, is the\n",
    "**model selection problem**.\n",
    "\n",
    "\n",
    "Here's an example where we're choosing between lipschitz models with different lipschitz constants $B$. And, for the sake of theory, also $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Bs = 2^(-5:5)\n",
    "models = Bs |> map(function(B) {\n",
    "  function(X,Y) { prediction.function(lipreg(X,Y,B=B)) }\n",
    "})\n",
    "names(models) = sprintf('lip %1.2f', Bs)\n",
    "models = c(models, list(mu=function(X,Y) mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "sigma = .2\n",
    "mu = function(x) { cos(1-2*x) }\n",
    "#mu = function(x) { sin(pi/x) }\n",
    "#mu = function(x) { x*sin(pi/x)}\n",
    "\n",
    "set.seed(1)\n",
    "X = runif(2*n)\n",
    "epsilon = sigma*rnorm(length(X))\n",
    "Y = mu(X) + epsilon\n",
    "\n",
    "predictions = models |> map(function(fit.model) { \n",
    "  muhat = fit.model(X, Y)\n",
    "  data.frame(X=X, muhat=muhat(X)) \n",
    "}) |> list_rbind(names_to='model')\n",
    "\n",
    "x = seq(0,1,length.out=1000)\n",
    "ggplot() +\n",
    "  geom_line(aes(x=x, y=mu(x)), linewidth=1.5, alpha=.5) + \n",
    "  geom_point(aes(x=X, y=Y), alpha=.1) + \n",
    "  geom_line(aes(x=X, y=muhat, color=model), data=predictions, alpha=.5, linewidth=.2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before we go further, let's ask whether the results we've proven apply here.\n",
    "$$ \\text{Is} \\quad \\mathcal{M} = \\{ \\hat \\mu^1 \\ldots \\hat \\mu^K \\} \\quad \\text{ a model? }$$\n",
    "\n",
    "\n",
    "\n",
    "The answer is no, not in the sense that we've been discussing. It's a\n",
    "set of *random curves* that depend on the noise $\\varepsilon_i$.\n",
    "As a result, the projection\n",
    "$$\n",
    "\\left\\langle \\varepsilon, \\frac{m - \\mu^\\star}{\\lVert m - \\mu^\\star \\rVert} \\right\\rangle\n",
    "$$\n",
    "isn't a mean-zero normal random variable. That breaks our argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.replications = 1:1000 |> map(function(rep) {\n",
    "  X = runif(2*n)\n",
    "  epsilon = sigma*rnorm(length(X))\n",
    "  Y = mu(X) + epsilon\n",
    "  muhat = lipreg(X,Y) |> prediction.function()\n",
    "  data.frame(rep=rep, projection = mean(epsilon * (muhat(X) - mu(X)) / sqrt(mean((muhat(X) - mu(X))^2))))\n",
    "}) |> list_rbind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot() + \n",
    "  geom_histogram(aes(x=projection, y=after_stat(density)), \n",
    "                 bins=30, data=error.replications, fill='red', color='gray', alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This breaks our argument; you may recall from our last lecture\n",
    "that random curves aren't and can't be analyzed like (nonrandom) curves.\n",
    "But there is an easy fix. Sample splitting!\n",
    "\n",
    "## Sample Splitting\n",
    "\n",
    "By fitting our $K$ curves $\\hat\\mu^1 \\ldots \\hat \\mu^K$ on a sample\n",
    "independent\n",
    "$(\\tilde X_1, \\tilde Y_1) \\ldots (\\tilde X_{\\tilde n}, \\tilde Y_{\\tilde n})$\n",
    "different from the one $(X_1,Y_1) \\ldots (X_n,Y_n)$ we use for\n",
    "selection, we repair our argument. Sure, the curves we'll get are random\n",
    "in the sense that they'll change if we resampled all of our data, but\n",
    "they won't depend on $Y_1 \\ldots Y_n$. That's all we need.\n",
    "\n",
    "Here's a plot of the projection's distribution with sample splitting (in blue) and without it (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "split.replications = 1:1000 |> map(function(rep) {\n",
    "  X = runif(2*n)\n",
    "  epsilon = sigma*rnorm(length(X))\n",
    "  Y = mu(X) + epsilon\n",
    "  train = (1:(2*n)) > n \n",
    "  muhat = lipreg(X[train],Y[train]) |> prediction.function()\n",
    "  data.frame(rep=rep, projection = mean(epsilon[!train] * (muhat(X[!train]) - mu(X[!train])) / sqrt(mean((muhat(X[!train]) - mu(X[!train]))^2))))\n",
    "}) |> list_rbind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ggplot() + \n",
    "  geom_histogram(aes(x=projection, y=after_stat(density)), \n",
    "                 bins=30, data=error.replications, fill='red', color='gray', alpha=.2) +\n",
    "  geom_histogram(aes(x=projection, y=after_stat(density)), \n",
    "                 bins=30, data=split.replications, fill='blue', color='gray', alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "\n",
    "Formally we'd say we *condition on* the sample used to fit\n",
    "$\\hat\\mu^1 \\ldots \\hat\\mu^K$. One way to think about that is that, if\n",
    "we're using randomly-generated fake data to check that our error bound\n",
    "is in fact valid with the probability we claim, we should estimate that\n",
    "probability by checking how often it's valid when we regenerate the\n",
    "selection sample $(X_1,Y_1) \\ldots (X_n,Y_n)$ but not the training\n",
    "sample\n",
    "$(\\tilde X_1, \\tilde Y_1) \\ldots (\\tilde X_{\\tilde n}, \\tilde Y_{\\tilde n})$.\n",
    "That said, if a bound is valid with some probability for any given\n",
    "training sample, it's going to be valid with (at least) that probability\n",
    "on average over training samples. That is, if\n",
    "$\\lVert \\hat\\mu - \\mu \\rVert_{L_2(P_n)} \\le s$ with probability\n",
    "$1-\\delta$ *conditional on*\n",
    "$(\\tilde X_1, \\tilde Y_1) \\ldots (\\tilde X_{\\tilde n}, \\tilde Y_{\\tilde n})$,\n",
    "it's also true that $\\lVert \\hat\\mu - \\mu \\rVert_{L_2(P_n)} \\le s$ with\n",
    "(unconditional) probability $1-\\delta$.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "1.  Split the data in half.\n",
    "    $$ \\underset{\\text{first half}}{(X_1, Y_1) \\ldots (X_n, Y_n)} \\qquad\n",
    "    \\underset{\\text{second half}}{(X_{n+1}, Y_{n+1}) \\ldots (X_{2n}, Y_{2n})}. $$\n",
    "\n",
    "2.  Use the second half to fit our $K$ models and get our $K$ least\n",
    "    squares estimators.\n",
    "    $$ \\hat\\mu^k = \\operatorname*{argmin}_{m \\in \\mathcal{M}^k} \\frac{1}{n}\\sum_{i=n+1}^{2n} \\{ Y_i - m(X_i)\\}^2 \\quad \\text{ for } \\quad k = 1 \\ldots K. $$\n",
    "    These $K$ curves go in our finite model $\\mathcal{M}$. As far as the\n",
    "    first half of the data knows, the curves aren't random. There's no\n",
    "    dependence on the outcomes $Y_i$ in the first half.\n",
    "\n",
    "3.  Using the first half, get least squares estimator $\\hat \\mu$ based\n",
    "    on this finite model.\n",
    "    $$ \\hat \\mu = \\operatorname*{argmin}_{m \\in \\mathcal{M}} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i)\\}^2 \\quad \\text{ for } \\quad \\mathcal{M} = \\{ \\hat\\mu^1 \\ldots \\hat\\mu^K \\}. $$\n",
    "    This is close to the target $\\mu$ like we've proven, satisfying\n",
    "    $$ \\lVert \\hat \\mu - \\mu\\rVert_{L_2(P_n)} \\le 4\\sigma\\sqrt{\\log(K)/n} \\quad \\text{ with probability } \\quad 1-1/K, $$\n",
    "    **if** the target $\\mu$ is in this finite model $\\mathcal{M}$. But\n",
    "    it won't be. So what we can say is this.\n",
    "    $$ \\lVert\\hat\\mu - \\mu\\rVert_{L_2(Pn)} \\le 4\\sigma\\sqrt{\\frac{\\log(K)}{n}} + 3\\lVert \\mu^\\star -\\mu \\rVert_{L_2(P_n)} \\ \\text{ with probability } \\ 1-1/K \\ \\textbf{ for all } \\ \\mu^\\star \\in \\mathcal{M}. $$\n",
    "    In this context, what we're saying is that the selected curve\n",
    "    $\\hat\\mu$ can't be much further than $\\mu$ than the closest of the\n",
    "    curves $\\hat\\mu^1 \\ldots \\hat \\mu^K$ that we're selecting from.\n",
    "\n",
    "### An Exercise\n",
    "\n",
    "::: exercise\n",
    "It seems wasteful not to use all the data to estimate your curves\n",
    "$\\hat\\mu_1 \\ldots \\hat\\mu_K$. Even if our theory doesn't apply, does it\n",
    "make sense to do model selection without sample splitting, i.e. fitting\n",
    "the curves $\\hat\\mu_1 \\ldots \\hat\\mu_K$ on the same set we're using to select from them? \n",
    "How bad could it really be?\n",
    "\n",
    "**Hint.** What happens when you fit the bounded variation model\n",
    "$\\{m : \\rho_{TV}(m) \\le B\\}$ for larger and larger variation budgets\n",
    "$B$?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Model Selection\n",
    "\n",
    "Let's get to the heart of what it means to select a model. There are\n",
    "really two interpretations out there.\n",
    "\n",
    "1.  The first focuses on whether we're selecting *the right model*, for\n",
    "    some sense of what that is. If you ever see the phrase *model\n",
    "    selection consistency*, that's what they're talking about.\n",
    "2.  The second focuses on how much we're paying for selecting a model\n",
    "    automatically rather than knowing which will be best ahead of time.\n",
    "    If you see the phrase *regret*, that's often what they're talking\n",
    "    about. It's an evocative name. What we want is to ensure that, even\n",
    "    if we haven't made 'the right choice' or it doesn't even make sense\n",
    "    to think of there being one, we're not all that unhappy with our\n",
    "    selection.\n",
    "\n",
    "We'll do some exercises to help us to think about what's promised by the\n",
    "results we've proven.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: exercise\n",
    "Do our results tell us that $\\hat\\mu$ is always the *closest* curve to\n",
    "$\\mu$ in our finite model $\\mathcal{M}$? Why or why not?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's the next exercise. Or really, a pair of them. Choose based on\n",
    "your answer to the last one.\n",
    "\n",
    "::: exercise\n",
    "**If you answered yes.**\n",
    "\n",
    "Let's think about whether there's something special about having the\n",
    "smallest squared-error loss. If you took $\\hat\\mu_2$ to be the curve in\n",
    "$\\mathcal{M}$ with the second-smallest squared error loss, would it be\n",
    "the *second-closest* curve to $\\mu$ in our model?\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: solution\n",
    "You should've answered no. This question was meant to encourage you to\n",
    "rethink your last answer if it were a yes. The correct answer to the\n",
    "last question was, after all, that there's nothing special about the\n",
    "curve with the smallest squared-error loss either.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: exercise\n",
    "**If you answered no.**\n",
    "\n",
    "Are there non-trivial models $\\mathcal{M}$ for which the least squares\n",
    "estimator $\\hat\\mu$ *is* always the closest curve to $\\mu$ ? If so, what\n",
    "is it about them that makes that happen? And in a model selection\n",
    "context, would you expect to see them when selecting from the\n",
    "least-squares curves $\\hat\\mu^K \\ldots \\hat\\mu^K$ within some sets of\n",
    "models $\\mathcal{M}^1 \\ldots \\mathcal{M}^K$ and not other sets?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: solution\n",
    "Basically no, unless our curves are well-separated.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Model Ranking\n",
    "\n",
    "Now let's get into what is, in a sense, an expanded version of our last\n",
    "exercise. When we choose $\\hat\\mu$ to be the curve in this model with\n",
    "the smallest squared error loss, we're in effect *ranking* our curves in\n",
    "order of increasing loss and then taking the first. We'll look into that\n",
    "ranking.\n",
    "\n",
    "It'll be important to keep in mind what that loss is. Here's what we\n",
    "know.\n",
    "$$ \n",
    "\\ell(m)-\\ell(\\mu^\\star) = \\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}^2  - 2\\left\\langle\\varepsilon, m - \\mu^\\star\\right\\rangle_{L_2(P_n)} - \\left\\langle \\mu^\\star-\\mu, m - \\mu^\\star \\right\\rangle_{L_2(P_n)}  \n",
    "$$\n",
    "To keep things simple, we'll suppose that by some miracle, $\\mu$ is in\n",
    "our model $\\mathcal{M}$. So we can think of our loss like this.\n",
    "$$   \n",
    "\\ell(m)-\\ell(\\mu) = \\lVert m-\\mu\\rVert_{L_2(P_n)}^2  - 2\\left\\langle\\varepsilon, m - \\mu \\right\\rangle_{L_2(P_n)} \n",
    "$$\n",
    "We'll break this down, making a table with three columns: one for the\n",
    "whole thing and one each for the two terms in this decomposition. In\n",
    "real data, we don't know $\\mu$ and therefore can't calculate\n",
    "$\\ell(m)-\\ell(\\mu)$, but that doesn't matter: $\\ell(\\mu)$ is a constant,\n",
    "so we get the same ranking whether we look at this incalculable\n",
    "difference or on the calculable quantity $\\ell(m)$.\n",
    "\n",
    "Let's start by calculating a model\n",
    "$\\mathcal{M}=\\{\\hat\\mu^1 \\ldots \\hat\\mu^K\\}$. We'll use bounded\n",
    "variation regression for a range of $B$. And we'll make sure to include\n",
    "$\\mu$. Then we'll rank our curves and thing about the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: exercise\n",
    "Look at the table generated by the code below. I've printed versions\n",
    "sorted both by loss-difference and by distance to $\\mu$. To what extent\n",
    "does the ranking in terms of loss difference correspond to the ranking\n",
    "in terms of distance to $\\mu$? Why or why not?\n",
    "\n",
    "After you've answered that, break down your answer a little bit. Think\n",
    "about a few things. \n",
    "\n",
    "1. Are the curves with the *lowest* loss differences\n",
    "all among the closest to $\\mu$? \n",
    "2. Do the curves that are *furthest*\n",
    "from $\\mu$ ever have the lowest loss differences? \n",
    "3. Which of these matters in the argument we use to prove our bound?\n",
    ":::\n",
    "\n",
    "::: exercise\n",
    "**Optional.** Is there another way of ranking curves that might get at\n",
    "what's going on in our proof a little better? If you can think of one,\n",
    "add it to your table and sort on it. See what you can see.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "Bs = 2^(-5:5)\n",
    "models = Bs |> map(function(B) {\n",
    "  function(X,Y) { prediction.function(lipreg(X,Y,B=B)) }\n",
    "})\n",
    "names(models) = sprintf('lip %1.2f', Bs)\n",
    "models = c(models, list(mu=function(X,Y) mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "sigma = .5\n",
    "mu = function(x) { cos(1-2*x) }\n",
    "#mu = function(x) { sin(pi/x) }\n",
    "#mu = function(x) { x*sin(pi/x)}\n",
    "\n",
    "set.seed(1)\n",
    "X = runif(2*n)\n",
    "epsilon = sigma*rnorm(length(X))\n",
    "Y = mu(X) + epsilon\n",
    "\n",
    "select = 1:n\n",
    "fit  = (n+1) : (2*n)\n",
    "fitted.models = models |> map(function(fit.model) { \n",
    "  muhat = fit.model(X[fit], Y[fit])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "loss = function(m) { mean( (Y[1:n] - m(X[1:n]))^2 ) }\n",
    "dist.sq = function(m) { mean( (m(X[1:n]) - mu(X[1:n]))^2 ) }\n",
    "mean.zero = function(m) { -2*mean( epsilon[1:n] * (m(X[1:n]) - mu(X[1:n])) ) }\n",
    "\n",
    "error.table = map(fitted.models, function(muhat) {\n",
    "  data.frame(loss.dif =  1000*(loss(muhat) - loss(mu)),\n",
    "             dist.sq  =  1000*dist.sq(muhat),\n",
    "             mean.zero = 1000*mean.zero(muhat),\n",
    "             scaled.loss.dif=1000*(loss(muhat)-loss(mu))/sqrt(dist.sq(muhat)),\n",
    "             scaled.mean.zero = 1000*mean.zero(muhat))\n",
    "}) |> list_rbind(names_to='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the table sorted by loss difference, $\\ell(m)-\\ell(\\mu)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.table[order(error.table$loss.dif),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a corresponding plot. It shows the Lipschitz regression estimators $\\hat\\mu$, with varying budgets $B$, we looked at above.\n",
    "But now I've plotted the curves with *postive loss difference*, i.e. the ones with bigger squared error loss than $\\mu$'s, as faint dotted lines.\n",
    "We're not selecting one of these. The remaining curves, i.e. the ones with squared error loss no larger than $\\mu$'s, are plotted as \n",
    "solid lines. Our error bound actually applies to every one of these. The thickest of these lines is the one we actually select, i.e. \n",
    "the one with the smallest squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predictions = fitted.models |> map(function(muhat) { \n",
    "  data.frame(X=X, muhat=muhat(X)) \n",
    "}) |> list_rbind(names_to='model')\n",
    "\n",
    "good.models = error.table$model[error.table$loss.dif <= 0]\n",
    "best.model = error.table$model[error.table$loss.dif == min(error.table$loss.dif)]\n",
    "x = seq(0,1,length.out=1000)\n",
    "ggplot() +\n",
    "  geom_line(aes(x=x, y=mu(x)), linewidth=1.5, alpha=.5) + \n",
    "  geom_point(aes(x=X, y=Y), alpha=.1) + \n",
    "  geom_line(aes(x=X, y=muhat, color=model), data=predictions[predictions$model == best.model,], alpha=.5, linewidth=1)  +\n",
    "  geom_line(aes(x=X, y=muhat, color=model), data=predictions[predictions$model %in% good.models,], alpha=.5, linewidth=.5)  +\n",
    "  geom_line(aes(x=X, y=muhat, color=model), data=predictions, alpha=.5, linewidth=.25, linetype='dotted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a few more ways of sorting the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.table[order(error.table$dist.sq), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.table[order(error.table$scaled.loss.dif), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving our more general bound\n",
    "\n",
    "To prove an error bound that doesn't require $\\mu$ to be in our model,\n",
    "we don't have to do much more than substitute $\\mu^\\star$ for $\\mu$\n",
    "itself in the argument we used to prove the bound that did. \n",
    "\n",
    "We can think of that argument as having three parts.\n",
    "\n",
    "1.  *The high-level part.* We work out an condition involving this loss\n",
    "    difference that implies $\\hat \\mu$ satisfies our bound.\n",
    "2.  *The arithmetic part.* We characterize the squared-error loss\n",
    "    difference $\\ell(m)-\\ell(\\mu^\\star)$ between an arbitrary curve $m$\n",
    "    and our approximation $\\mu^\\star$ to $\\mu$.\n",
    "3.  *The synthesis part.* We show, using our characterization of this\n",
    "    difference, that this condition holds with high probability.\n",
    "\n",
    "You can do the first two parts in either order, but you've got to do\n",
    "both of them before you do the third. Let's work through them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The high-level part.\n",
    "\n",
    "We're going to be talking about a *neighborhood* of $\\mu^\\star$ our\n",
    "model.\n",
    "$$ \n",
    "\\mathcal{M}_s = \\left\\{ m \\in \\mathcal{M} : \\lVert m - \\mu^\\star \\rVert_{L_2(P_n)} \\le s \\right\\} \n",
    "$$\n",
    "What we're doing here is finding a condition---the one in bold\n",
    "below---that implies $\\hat \\mu$ is in this neighborhood. This\n",
    "neighborhood, of course, is a set of curves that's close to $\\mu^\\star$.\n",
    "To get an implied statement about distance to $\\mu$, we use the triangle\n",
    "inequality. \n",
    "$$ \n",
    "\\lVert \\hat \\mu - \\mu \\rVert_{L_2(P_n)} \n",
    "\\le \\lVert \\hat \\mu - \\mu^\\star \\rVert_{L_2(P_n)} + \\lVert \\mu^\\star - \\mu \\rVert_{L_2(P_n)} \n",
    "\\le s + \\lVert \\mu^\\star - \\mu \\rVert_{L_2(P_n)} \\quad \\text{ if } \\quad \\hat\\mu \\in \\mathcal{M}_s \n",
    "$$ \n",
    "For now, we'll leave the *radius* $s$ of this neighborhood\n",
    "unspecified. In the third part, we'll make the best choice we can.\n",
    "Here's our argument.\n",
    "\n",
    "What we know is that $\\hat \\mu$ matches (i.e. beats or ties) every other\n",
    "curve in the model in terms of squared error loss. That's what a\n",
    "minimizer (argmin) does.\n",
    "$$ \n",
    "\\hat \\mu = \\operatorname*{argmin}_{m \\in \\mathcal{M}} \\ell(m) \\quad \\iff \\quad \\ell(\\hat\\mu) \\le \\ell(m) \\ \\text{ for all } \\ m \\in \\mathcal{M} \n",
    "$$\n",
    "For any approximation $\\mu^\\star$ to $\\mu$ that's in our model, that\n",
    "means it matches $\\mu^\\star$.\n",
    "$$ \n",
    "\\ell(\\hat\\mu) \\le \\ell(m) \\ \\text{ for all } \\ m \\in \\mathcal{M} \\implies \\ell(\\hat\\mu) \\le \\ell(\\mu^\\star). \n",
    "$$\n",
    "And **if no curve in our neighborhood's complement matches**\n",
    "$\\mu^\\star$, this means $\\hat\\mu$ isn't in that complement.\n",
    "$$ \n",
    "\\ell(\\hat\\mu) \\le \\ell(\\mu^\\star) \\ \\textbf{ and } \\ \\ell(m) > \\ell(\\mu^\\star) \\ \\text{ for all } \\ m \\in \\mathcal{M} \\setminus \\mathcal{M}_s \\implies \\hat\\mu \\not \\in \\mathcal{M}\\setminus \\mathcal{M}_s \n",
    "$$\n",
    "And because $\\hat\\mu$ *is* in the model, that means $\\hat\\mu$ is in\n",
    "the neighborhood\n",
    "$$  \n",
    "\\hat\\mu \\not \\in \\mathcal{M}\\setminus \\mathcal{M}_s \\ \\textbf{ and } \\  \\hat\\mu \\in \\mathcal{M} \\quad \\iff \\quad \\hat \\mu \\in \\mathcal{M}_s \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The arithmetic part.\n",
    "\n",
    "Here's where we characterize the loss differences we've been talking\n",
    "about. Here's what we're after.\n",
    "$$ \\ell(m)-\\ell(\\mu^\\star) = \\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}^2  - 2\\left\\langle\\varepsilon, m - \\mu^\\star\\right\\rangle_{L_2(P_n)} - \\left\\langle \\mu^\\star-\\mu, m - \\mu^\\star \\right\\rangle_{L_2(P_n)} $$\n",
    "We've got three terms. From left to right, they're the squared sample\n",
    "two-norm of the difference $m-\\mu^\\star$, a mean-zero normal term with\n",
    "standard deviation proportional to this two-norm (not-squared), and a\n",
    "last term that scales with both the difference $m-\\mu^\\star$ and the\n",
    "approximation error $\\mu^\\star - \\mu$. Here's a derivation. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell(m) - \\ell(\\mu^\\star) \n",
    "&= \\frac{1}{n}\\sum_{i=1}^n  \\{Y_i - m(X_i)\\}^2 - \\frac{1}{n}\\sum_{i=1}^n\\{ Y_i - \\mu^\\star(X_i) \\}^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n  [ \\{Y_i - \\mu^\\star(X_i)\\} + \\{\\mu^\\star(X_i) - m(X_i)\\} ]^2 - \\{ Y_i - \\mu(X_i) \\}^2 \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n  \\{\\mu^\\star(X_i) - m(X_i)\\}^2 - \\frac{2}{n}\\sum_{i=1}^n \\{ Y_i - \\mu^\\star(X_i) \\}\\{m(X_i) - \\mu^\\star(X_i))\\} \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n  \\{\\mu^\\star(X_i) - m(X_i)\\}^2 - \\frac{2}{n}\\sum_{i=1}^n \\{Y_i - \\mu(X_i)\\} \\{ m(X_i) - \\mu^\\star(X_i) \\} \\\\\n",
    "&- \\frac{2}{n}\\sum_{i=1}^n \\{\\mu(X_i) - \\mu^\\star(X_i) \\}\\{m(X_i) - \\mu^\\star(X_i) \\}.\n",
    "\\end{aligned}\n",
    "$$ \n",
    "Our characterization above is just this in vector notation.\n",
    "\n",
    "Here's what we need to know about our terms. \n",
    "$$ \n",
    "\\begin{aligned}\n",
    "2\\left\\langle \\varepsilon, m - \\mu^\\star\\right\\rangle_{L_2(P_n)} &\\overset{(a)}{=} \\frac{2 \\sigma \\lVert m-\\mu^\\star \\rVert_{L_2(P_n)}}{\\sqrt{n}}Z_m \\quad \\text{ where } \\quad Z_m \\sim N(0,1) \\\\\n",
    "2\\left\\langle \\mu^\\star - \\mu, m - \\mu^\\star\\right\\rangle_{L_2(P_n)} &\\overset{(b)}{\\le} 2\\lVert \\mu - \\mu^\\star \\rVert_{L_2(P_n)} \\lVert m-\\mu^\\star \\rVert_{L_2(P_n)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "a.  This is true because a weighted average of the mean-zero normal\n",
    "    random variables ($\\varepsilon_i$) is mean-zero normal. And we can\n",
    "    write any mean-zero normal as its standard deviation times a\n",
    "    standard normal. That's what we've done. We calculated the standard\n",
    "    deviation to be what's in front of $Z_m$ last lecture.\n",
    "\n",
    "b.  This is an application of the Cauchy-Schwarz inequality. We saw that\n",
    "    on the Inner Product Spaces Homework.\n",
    "\n",
    "## The synthesis part.\n",
    "\n",
    "The high-level part promises us a bound if no curve in our\n",
    "neighborhood's complement matches $\\mu^\\star$ in terms of squared error\n",
    "loss.\n",
    "$$ \\lVert \\hat \\mu - \\mu \\rVert_{L_2(P_n)} \\le s + \\lVert \\mu^\\star - \\mu \\rVert_{L_2(P_n)} \\quad \\text{ if } \\quad \\ell(m) > \\ell(\\mu^\\star) \\ \\text{ for all } \\ m \\in \\mathcal{M} \\setminus \\mathcal{M}_s $$\n",
    "The bound we've claimed is this one with\n",
    "$s=4\\sigma\\sqrt{\\frac{\\log(K)}{n}} + 2\\lVert \\mu^\\star -\\mu \\rVert_{L_2(P_n)}$.\n",
    "So what we need to show to prove it is that our premise, i.e. what's to\n",
    "the right of the **if** above, is true with the claimed probability\n",
    "$1-1/K$. Let's do it.\n",
    "\n",
    "We need to show that with the claimed probability,\n",
    "$$ \n",
    "\\ell(m) - \\ell(\\mu^\\star) > 0  \\quad \\text{ for all } \\quad m \\in \\mathcal{M} \\setminus \\mathcal{M}_s. \n",
    "$$\n",
    "Plugging in our expansion of this loss difference, here's what this\n",
    "means.\n",
    "$$ \n",
    "\\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}^2 - \\frac{2\\sigma \\lVert m-\\mu^\\star \\rVert_{L_2(P_n)}}{\\sqrt{n}} Z_m \n",
    "   - 2\\langle \\mu - \\mu^\\star, m-\\mu^\\star \\rangle_{L_2(P_n)} > 0 \\ \\text{ for all } \\ m \\in \\mathcal{M} \\setminus \\mathcal{M}_s \n",
    "$$\n",
    "And because this last term is at least\n",
    "$-2\\lVert \\mu-\\mu^\\star\\rVert_{L_2(P_n)} \\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}$,\n",
    "this is true if\n",
    "$$ \n",
    "\\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}^2- \\frac{2\\sigma \\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}}{\\sqrt{n}}Z_m - 2\\lVert \\mu - \\mu^\\star\\rVert_{L_2(P_n)}\\lVert m-\\mu^\\star\\rVert_{L_2(P_n)} > 0 \\ \\text{ for all } \\ m \\in \\mathcal{M} \\setminus \\mathcal{M}_s \n",
    "$$\n",
    "or equivalently, crossing out common factors of\n",
    "$\\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}$ and rearranging, if\n",
    "$$ \n",
    "\\lVert m-\\mu^\\star\\rVert_{L_2(P_n)} > \\frac{2\\sigma \\lVert m-\\mu^\\star\\rVert_{L_2(P_n)}}{\\sqrt{n}}Z_m + 2\\lVert \\mu - \\mu^\\star\\rVert_{L_2(P_n)} \\ \\text{ for all } \\ m \\in \\mathcal{M} \\setminus \\mathcal{M}_s \n",
    "$$\n",
    "This is true if the *minimum* of the left side (over $m$ in the\n",
    "complement) is larger than the *maximum* of the right. \n",
    "$$ \n",
    "s = \\min_{m \\in \\mathcal{M} \\setminus \\mathcal{M}_s} \\lVert m-\\mu^\\star \\rVert_{L_2(P_n)}  \n",
    "> \\frac{2\\sigma}{\\sqrt{n}}\\max_{m \\in \\mathcal{M} \\setminus \\mathcal{M}_s} Z_m \n",
    "+ 2\\lVert \\mu - \\mu^\\star\\rVert_{L_2(P_n)} \\ \\text{ for all } \\ m \\in \\mathcal{M} \\setminus \\mathcal{M}_s. \n",
    "$$\n",
    "\n",
    "To get our bound $s$, we plug in the standard upper bound\n",
    "$2\\sqrt{\\log(K)}$ on the maximum of $K$ (or fewer) standard normals,\n",
    "which holds with probability $1-1/K$. We're done.[^1]\n",
    "\n",
    "[^1]: We proved that standard bound in the last lecture, for what it's\n",
    "    worth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
