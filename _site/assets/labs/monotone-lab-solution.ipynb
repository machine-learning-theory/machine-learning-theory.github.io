{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Today, we're going to be implementing least squares regression subject\n",
    "to an *increasingness* constraint. That is, we're going to find a curve\n",
    "$\\hat \\mu$ that solves this optimization problem.\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i) \\}^2.\n",
    "$$\n",
    "\n",
    "Then we'll see what it tells us about the effect of reducing class\n",
    "sizes for 5th graders using some admittedly fake data about test outcomes.\n",
    "\n",
    "\n",
    "We'll use a few libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "})\n",
    "\n",
    "# OSQP claims some feasible problems aren't, so we'll tell CVXR not to use it\n",
    "CVXR::add_to_solver_blacklist('OSQP')  \n",
    "\n",
    "# And we'll style our plots  \n",
    "theme_update(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    panel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                    legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\tpanel.grid.major=element_line(color=rgb(1,0,0,.1,  maxColorValue=1)),\n",
    "\t        panel.grid.minor=element_line(color=rgb(0,0,1,.1,  maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by focusing on the increasing case. We'll use `CVXR` to help us solve the following optimization problem.\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m} \\frac{1}{n}\\sum_{i=1}^n \\left\\{ Y_i - m(X_i) \\right\\}^2.\n",
    "$$\n",
    "\n",
    "This might feel like a tall order because there are *a lot* of increasing functions. The set is infinite-dimensional, which essentially means you can't play 20 questions. \n",
    "\n",
    "- If you know I'm thinking of an increasing function, there's no number of questions you can ask me that'll let you pin down which one. \n",
    "- If I wanted to cheat at the game, every time you said 'is it this one' I could come up with an increassing function that's different from the one you guessed *and* consistent with all the answers I've given you so far.\n",
    "\n",
    "But it turns out that finding a solution to optimization problem is pretty easy if you break it down into steps in the right way. We're going to use an approach I'll call **restriction and extension**, which often works when you're trying to solve infinite dimensional optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Pedantry \n",
    "\n",
    "## Functions, Restriction, and Extension\n",
    "\n",
    "When we say *an increasing function*, we tend to think of an unbroken curve that goes up as you move from left to right. Like the curve $f(x)=e^x$ that we've coded up and plotted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x = seq(-1,1,by=.01)\n",
    "f = function(x) { exp(x) }\n",
    "ggplot() + geom_line(aes(x=x, y=f(x)), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To be precise, $f$ is a *function on the real line* because it tells us how to take any *input* $x$ on the real line to an *output* $f(x)$. \n",
    "- To be even more precise, $f$ is a function *from* the real line *to* the real line, because those outputs are real numbers too.\n",
    "\n",
    "Here's another increasing function $g$. It's a function on a set of five points $\\mathcal{X} = \\{-1, -\\frac12, 0, +\\frac12, +1\\}$. Here's how we might code and plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X  = c(-1,-1/2,0,1/2,1) \n",
    "e = exp(1)\n",
    "g = function(x) { \n",
    "    case_when(x == -1    ~  1/e,\n",
    "              x == -1/2  ~  sqrt(1/e),\n",
    "              x == 0     ~  1,\n",
    "              x == +1/2  ~  sqrt(e),\n",
    "              x == +1    ~  e)\n",
    "}\n",
    "\n",
    "ggplot() + geom_point(aes(x=X, y=g(X)), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each valid input to $g$ is a valid input to $f$ as well, i.e. because the set of five points $\\mathcal{X}$ is a *contained in* the set of all real numbers, we can ask if the outputs $f(x)$ and $g(x)$ coincide on $\\mathcal{X}$. And they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot() + geom_line(aes(x=x, y=f(x)), color='blue') + \n",
    "           geom_point(aes(x=X, y=g(X)), color='red') \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But they aren't the same thing. The function $f$ is defined for all real numbers, while the function $g$ is only defined for the five points in $\\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "f(3/4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "g(3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the this happens---when $f(x)$ and $g(x)$ coincide on a set $\\mathcal{X}$ but $g$ is not defined elsewhere---we say that ...\n",
    "- $g$ is **the** *restriction* of $f$ to $\\mathcal{X}$\n",
    "- $f$ is **an** *extension* of $g$ to the real line.\n",
    "Those are the formal terms. \n",
    "\n",
    "There are other extensions of $g$. For example ...\n",
    "- a **piecewise linear** extension. It's what we get by connecting the dots in the plot of $g$ above with straight lines segments. That's what ggplot's geom_line does.\n",
    "- a **piecewise constant** extension. We get one by moving horizontally rightward from each dot until we hit the next one. That's what ggplot's geom_step does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot() + geom_point(aes(x=X, y=g(X)), color='red') + \n",
    "           geom_line(aes(x=X, y=g(X)),  color='red', linetype='dashed') +\n",
    "           geom_step(aes(x=X, y=g(X)),  color='red', linetype='dotted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not quite getting extensions of $g$ to the real line when we use geom_line and geom_step. We're getting extensions to the unit interval, the range between the largest and smallest point in $\\mathcal{X}$. \n",
    "\n",
    "**R** has a built-in function `approxfun` that will extend functions from a set of points to the real line. \n",
    "\n",
    "  - We can ask for piecewise-constant and piecewise-linear extensions. \n",
    "  - But the piecewise-linear extension isn't the one we usually want. \n",
    "\n",
    "We'll write our own extension code later on in this lab so we can get it to do what we want. What's wrong with `approxfun`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "g.linear   = approxfun(X,g(X), rule=2, method='linear')\n",
    "g.constant = approxfun(X,g(X), rule=2, method='constant')\n",
    "\n",
    "x = seq(-2,2,by=.01)\n",
    "ggplot() + geom_point(aes(x=X, y=g(X)),  color='red') + \n",
    "  geom_line(aes(x=x,  y=g.linear(x)),    color='red', linetype='dashed') +\n",
    "  geom_line(aes(x=x,  y=g.constant(x)),  color='red', linetype='dotted') +\n",
    "  geom_line(aes(x=x,  y=f(x)),           color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Terminology and Notation\n",
    "\n",
    "- We say $f$ is *a function from $\\mathcal{X}$ to $\\mathcal{Y}$* if, for every point $x$ in 'set of possible inputs' $\\mathcal{X}$, we know the corresponding value $f(x)$ and it is in the 'set of possible outputs' $\\mathcal{Y}$. \n",
    "  - To make this a little more compact, often people write $f: \\mathcal{X} \\to \\mathcal{Y}$ with this meaning. \n",
    "  - Typically this is prounounced exactly the same way, i.e. as  '$f$ is a function from $\\mathcal{X}$ to $\\mathcal{Y}$'.\n",
    "  - Or more efficiently as '$f$ *maps* $\\mathcal{X}$ to $\\mathcal{Y}$'.\n",
    "- If we expect the set of possible outputs to be inferred from context, we might say '$f$ is a function on $\\mathcal{X}$', which is a little shorter. \n",
    "    - That's a pretty safe bet in this class because our outputs are almost always real numbers.\n",
    "    - As far as I know, there isn't really an accepted notation for this. People still write $f: \\mathcal{X} \\to \\mathcal{Y}$ even when the output set is clear from context.\n",
    "    - The options $f: \\mathcal{X} \\to$ or $f: \\mathcal{X} \\to [\\text{you figure it out}]$ are awkward and aggressive respectively, and while $f: \\mathcal{X}$ is a little better, I don't see it much.\n",
    "- When we want to write about a relationship like the one between $f$ and $g$, we often write $f|_{\\mathcal{X}}$, which is pronounced 'the restriction of $f$ to $\\mathcal{X}$'. The statement $g=f|_{\\mathcal{X}}$ is read as '$g$ is the restriction of $f$ to $\\mathcal{X}$'.\n",
    "- I often find myself saying $f$ and $g$ *agree on* $\\mathcal{X}$, by which I mean that $f(x)=g(x)$ for all $x$ in $\\mathcal{X}$. \n",
    "    - This means restrictions of $f$ and $g$ to $\\mathcal{X}$ are the same, i.e. that $f|_{\\mathcal{X}} = g|_{\\mathcal{X}}$, but doesn't tell us that one is a restriction of the other.\n",
    "    - If we think about our code for $f$ and $g$ above, the distinction is that that $g$ might not 'return NA' for inputs $x$ that aren't in $\\mathcal{X}$.  It might just return a value that's not $f(x)$.\n",
    "    - For example, $f$, $g$, and the piecewise-constant and piecewise-linear extensions of $g$ all agree on $\\mathcal{X}$. Their restrictions to $\\mathcal{X}$ are all the same function: $g$.\n",
    "- Sometimes people use different words for extension inside and outside the range of $\\mathcal{X}$.\n",
    "  - *Interpolation* is the process of extending a function to points inside the range of $\\mathcal{X}$. That's what `geom_line` and `geom_step` did for us above.\n",
    "  - *Extrapolation* is the process of extending a function to points outside the range of $\\mathcal{X}$. That's what we had to use `approxfun` for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Write a function $h$ that agrees with $f(x)=e^x$ on the unit interval $[-1,+1]$ but is constant elsewhere. Plot $h$ and $f$ on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "h = function(x) { \n",
    "  case_when( -1 <= x & x <= 1 ~ exp(x),\n",
    "             x  < -1          ~ exp(-1),\n",
    "             x  >  1          ~ exp(1))\n",
    "}\n",
    "\n",
    "ggplot() + geom_line(aes(x=x, y=f(x)), color='blue') + \n",
    "           geom_line(aes(x=x, y=h(x)), color='red', linetype='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasingness \n",
    "\n",
    "If we have a function $f$ on a set of points $\\mathcal{X}$ on the real line, we can ask if it's increasing. \n",
    "\n",
    "- Intuitively, that means that, as we move from left to right, the values of $f$ get bigger. Or actually, because this turns out to be more convenient, that they don't get smaller. \n",
    "- Formally, we say the $f: \\mathcal{X} \\to \\mathbb{R}$ is increasing if $f(x) \\le f(x')$ for all pairs of points $x,x'$ in $\\mathcal{X}$ with $x \\le x'$.\n",
    "\n",
    "All of the functions we've seen so far are increasing. \n",
    "\n",
    "- The ones with flat segments, like $h$ and the piecewise-constant extension of $g$, are sometimes called *non-decreasing* instead of increasing.\n",
    "    - This aligns a bit better with plain english, but it tends to make your sentences awkward and harder to understand.\n",
    "    - Math folks tend to say 'strictly increasing' if they want to rule out flat segments, but it's pretty rare that they want to. \n",
    "    - Informally, I'll say 'gets bigger' meaning 'doesn't get smaller' analogously.\n",
    "    \n",
    "An *increasing function on the real line* ($\\mathbb{R}$) is a function on the real line that gets bigger whenever $x \\in \\mathbb{R}$ does. That is, its values satisfy \n",
    "\n",
    "$$ \n",
    "f(x) \\le f(x') \\text{ for all pairs of points \\textbf{on the real line} satisfying } x \\le x' \n",
    "$$\n",
    "\n",
    "An *increasing function on the set* $\\mathcal{X}$ is one where this value gets bigger whenever $x \\in \\mathcal{X}$ does. That is, its values satisfy \n",
    "\n",
    "$$ \n",
    "f(x) \\le f(x') \\text{ for all pairs of points \\textbf{in $\\mathcal{X}$} satisfying $x \\le x'$} \n",
    "$$\n",
    "\n",
    "There are two things you should know about increasingness and restriction/extension. Suppose $\\mathcal{X}$ is a set of real numbers, i.e., a subset of the real line.\n",
    "\n",
    "1. If $f$ is an increasing function on the real line: $f_{\\mid \\mathcal{X}}$, *the restriction* of $f$ to $\\mathcal{X}$, is increasing.\n",
    "2. If $g$ is an increasing function on $\\mathcal{X}$: *it has* an increasing extension to the real line. That is, there is an increasing function $f$ on the real line that agrees with $g$ on $\\mathcal{X}$. \n",
    "\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If $f(x) \\le f(x')$ for all pairs of real numbers $x,x'$ with $x \\le x'$, then $f(x) \\le f(x')$ for all pairs of real numbers with $x \\le x'$ that are both in the set $\\mathcal{X}$.\n",
    "2. The piecewise-constant extension of $g$ is increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Is it true that *every* extension of an increasing function $g$ on $\\mathcal{X}$ is increasing? \n",
    "\n",
    "  - If so, explain why. \n",
    "  - If not, draw a counterexample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enough Already (Almost)\n",
    "\n",
    "If you think all this sounds a like a waste of time, I'm sympathetic. I don't usually talk this formally and I certainly don't usually write 7 lines of code when `g=exp` will do. But sometimes being formal about stuff like this can help focus our thinking.\n",
    "\n",
    "**Question**. If I'm thinking of an increasing function on $\\mathcal{X}=\\{-1,-\\frac12,0,+\\frac12,+1\\}$, or any function on $\\mathcal{X}$, how many questions would you need to ask to pin down which one I'm thinking of? \n",
    "\n",
    "**Answer**. 5. You can just ask me for the value of the function at each point in $\\mathcal{X}$.\n",
    "\n",
    "What this tells us is that, even if it's impossible to find out exactly what increasing function $f$ on the real line I'm thinking of, it can be pretty easy to find out its restriction $f_{\\mathcal{X}}$ to a finite set of points. With this in mind, let's take a look at our optimization problem again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Least Squares Problem \n",
    "## Step 1. Restrict and Solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the least squares problem we've been writing. It's a little vague.\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m} \\ \\ell(m) \\quad \\text{ for } \\quad \\ell(m) = \\frac{1}{n}\\sum_{i=1}^n \\left\\{ Y_i - m(X_i) \\right\\}^2.\n",
    "$$\n",
    "\n",
    "What's vague about it? It doesn't specify *what kind of function* the solution $\\hat\\mu$ is supposed to be, i.e., the sets of possible inputs and outputs. \n",
    "\n",
    "That vagueness speaks to a tension between what we want and what we can know.\n",
    "\n",
    "- The data we have is only informative about the values of the function on the sample---the set $\\mathcal{X}=\\{X_1,\\ldots,X_n\\}$.\n",
    "    - If two functions on the real line agree on the sample, then either they're both solutions to this optimization problem or neither is. \n",
    "    - This means that, at best, we can hope to identify a function on the sample.\n",
    "    - In a way, this is a good thing. It means we don't have to solve an infinite-dimensional optimization problem. Computers like that.\n",
    "- Often, we will want to make predictions for points that aren't in the sample, so we'll want to find a function on the real line. \n",
    "    - This means that, whatever solution we do get, we'll have to extend it to the real line. \n",
    "    - And the data doesn't prefer any one extension to another. That's on us to make up.\n",
    "    - This isn't great, but it's not damning either. In large samples, the extension we use doesn't matter much. Why?\n",
    "    \n",
    "Here's the recipe for solving this optimization problem.\n",
    "1. Think of it as a problem of choosing from the set of increasing functions $m_{\\mid \\mathcal{X}}$ on the sample.\n",
    "    - That's a finite-dimensional model: we need one parameter for each distinct value of $X_1 \\ldots X_n$.\n",
    "    - We can solve it using CVXR, just like we did for the linear regression problem.\n",
    "2. Once we have a solution, we can extend it to an increasing function on the real line in any way we like. Our loss function $\\ell$ doesn't care.\n",
    "    - We can, for example, do piecewise constant extension or piecewise linear extension.\n",
    "    - If we have an increasing function on the sample, these extensions will be increasing functions on the real line.\n",
    "\n",
    "Let's prove that this restrict-then-extend approach works. Suppose we've found a minimizer $\\hat\\mu_{\\mid\\mathcal{X}}$\n",
    "over the set of all increasing functions from $\\mathcal{X} \\to \\mathbb{R}$. We know it *has* an increasing extension $\\hat\\mu: \\mathbb{R} \\to \\mathbb{R}$. We'll show that this extension minimizes $\\ell$ over over the set of all increasing functions from $\\mathcal{X} \\to \\mathbb{R}$.\n",
    "\n",
    "*A proof by contradiction*. If $\\hat \\mu$ isn't a minimizer of $\\ell$, then there is some increasing function $\\hat m: \\mathbb{R} \\to \\mathbb{R}$ with $\\ell(\\hat m) < \\ell(\\hat\\mu)$. Because $\\ell(m_{\\mid \\mathcal{X}})=\\ell(m)$ for all functions $m: \\mathbb{R} \\to \\mathbb{R}$, i.e. our loss function only cares what happens on the sample, it follows that $\\ell(\\hat m_{\\mid \\mathcal{X}}) < \\ell(\\hat \\mu_{\\mid \\mathcal{X}})$, so $\\hat \\mu_{\\mid \\mathcal{X}}$ couldn't have been a minimizer.\n",
    "\n",
    "Having convinced ourselves we're on the right track, let's calculate $\\hat\\mu_{\\mid\\mathcal{X}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Implementation in Mathematical Notation\n",
    "\n",
    "To keep things simple we'll start by assuming that the observed values of $x$, $X_1 \\ldots  X_n$, are distinct. \n",
    "\n",
    "`CVXR` likes to work with vectors, not functions on finite sets, so we'll have to do a little translation.\n",
    "\n",
    " - We'll tell `CVXR` we're interested in optimizing over vectors $\\vec m$ in some set $\\vec M \\subseteq \\mathbb{R}^n$. \n",
    " - We'll interpret these vectors as functions $m:\\mathcal{X} \\to \\mathbb{R}$  using the correspondence $m(X_i) = \\vec m_i$. \n",
    "\n",
    "Plugging this correspondence into our optimization problem, we get ...\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\mu(X_i) &= \\vec\\mu_i  \n",
    "&&  \\text{ where } \\\\\n",
    "&\\vec \\mu = \\operatorname*{argmin}_{\\vec m \\in \\vec M}  \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - \\vec m_i \\}^2  \n",
    "&& \\text{ for } \\\\ \n",
    "& \\textcolor{red}{\\vec M = \\text{some set of vectors in} \\  \\mathbb{R}^n}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we want $\\hat\\mu$ to solve the optimization problem we started with, i.e. $\\hat\\mu = \\operatorname*{argmin}\\limits_{\\text{increasing} \\ m: \\mathcal{X} \\to \\mathcal{R}} \\frac{1}{n}\\sum_{i=1}^n \\left\\{ Y_i - m(X_i) \\right\\}^2$, what should $\\textcolor{red}{\\vec M}$ be?\n",
    "\n",
    "**Tips**.\n",
    "\n",
    "1. Look back at the definition of increasingness and think about what it means for a function on $\\mathcal{X}=\\{X_1 \\ldots X_n\\}$. Finish this sentence. \n",
    " > $m:\\mathcal{X} \\to \\mathbb{R}$ is increasing *if and only if* the $n$ values $m(X_1) \\ldots m(X_n)$ satisfy ...\n",
    "2. If you've expressed what you want in terms of the $n$ values $m(X_1) \\ldots m(X_n)$, you can translate it into a statement about $\\vec m$ mechanically. Just replace $m(X_i)$ with $\\vec m_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Implementation in `CVXR`\n",
    "\n",
    "Write some monotone regression code. Then use it to plot $\\hat\\mu(X_i)$ on top of the data below. \n",
    "\n",
    "I've written a template for you to fill in. \n",
    "  - If you run it without changes, it'll give you *an* increasing function, by which I mean a vector that satisfies the constraints \n",
    "in the optimization problem written out above. \n",
    "  - But not necessarily one that's a good fit to the data. \n",
    "  - Go ahead and make changes where indicated to get the right fit.\n",
    "\n",
    "There are a few tips below the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg = function(X,Y) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(!anyDuplicated(X))\n",
    "  stopifnot(length(X)==length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "\n",
    "  # Step 1. \n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^n.\n",
    "  m = Variable(n)\n",
    "\n",
    "  # Step 2. \n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y-m)^2)/n                                       ## CHANGED \n",
    "\n",
    "  # Step 3. \n",
    "  # We specify our constraints.\n",
    "  all.pairs = expand.grid(i=1:n, j=1:n)\n",
    "  le = all.pairs$i <= all.pairs$j \n",
    "  le.pairs = all.pairs[le,]\n",
    "  constraints = list(m[le.pairs$i] <= m[le.pairs$j])         ## CHANGED\n",
    "\n",
    "  # Step 4. \n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R. \n",
    "  #  1. we record the vector of inputs, X, and the vector of corresponding outputs, mu.hat, in a list. We also record the input data.\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  #  3. we return the list\n",
    "  model = list(X=X, mu.hat=mu.hat, input=input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Tips.**\n",
    "\n",
    "1.  `CVXR` takes a list of constraints expressed in terms of a vector of unknowns `m` that it calls a Variable.\n",
    "    - These are expressions that would evaluate to TRUE or FALSE if `m` were a vector of numbers, e.g. `m[i] <= m[j]` for some pair of indices `i` and `j`.  \n",
    "    - It's also happy if these constraints in this list are *vector-valued expressions*, i.e. expressions that would evaluate to a vector with elements that are TRUE or FALSE. \n",
    "      - In this case, it enforces the constraint that all elements of the vector are TRUE when it solves for `m`. \n",
    "      - This means that if you've got *vectors* of indices `i` and `j`, saying `m[i] <= m[j]` will enforce that `m[i[1]] <= m[j[1]]`, `m[i[2]] <= m[j[2]]`, etc.\n",
    "\n",
    "2.  If you want to make a list of all pairs of indices $i$ and $j$ with $X_i \\le X_j$, you can do it like this.\n",
    "    - Make a table of all pairs of indices. \n",
    "    - Find the rows where $X_i \\le X_j$. \n",
    "\n",
    "    Some code that does this is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X = c(-1,-1/2,0,1/2,1)\n",
    "n = length(X)\n",
    "\n",
    "all.pairs=expand.grid(i=1:n,j=1:n)\n",
    "le = X[all.pairs$i] <= X[all.pairs$j]\n",
    "le.pairs = all.pairs[le,]\n",
    "\n",
    "i = le.pairs$i\n",
    "j = le.pairs$j\n",
    "\n",
    "i\n",
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Solution\n",
    "\n",
    "Here's some code to help you see what yours is doing. It uses the code you've written to fit a function to some data\n",
    "and then plots the function and data together. We'll use data sampled around our old friend from the warm-up, the 'stepline'. \n",
    "It'll plot observations $(X_i, Y_i)$ in as gray dots and the predictions $(X_i, \\hat\\mu(X_i))$ as blue ones. And because \n",
    "this is fake data, we can plot the function we've sampled our points around, too. That'll be a black line.\n",
    "\n",
    "You don't need to change this code at all, just run it after you've written what you need to above. If you run this code\n",
    "without changing *that*, $\\hat\\mu$ will be some increasing function but not necessarily a good one. That said, it might\n",
    "be good in some places and not others---a stopped clock is right twice a day.\n",
    "\n",
    "If you've written the code above correctly, you should get a pretty good estimate of the black line. \n",
    "It is, after all, an increasing function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "mu = function(x) { x*(x >= .5) }\n",
    "sigma = .1\n",
    "\n",
    "n = 100\n",
    "X = seq(0,1,length.out=n)\n",
    "Y = mu(X) + sigma*rnorm(n)\n",
    "\n",
    "# our fit curve\n",
    "model   = monotonereg(X,Y)\n",
    "\n",
    "# a grid for plotting and a plot\n",
    "x = seq(0,1,by=.001) \n",
    "fit.on.sample = ggplot() + \n",
    "\tgeom_point(aes(x=X,y=Y),  alpha=.2) + \n",
    "\tgeom_line(aes(x=x,y=mu(x)), alpha=1) +\n",
    "\tgeom_point(aes(x=model$X,y=model$mu.hat), color='blue', alpha=.2)\n",
    "fit.on.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Exploration\n",
    "\n",
    "1. Try changing the function `mu` that we're sampling our data around. Find a 'hard function' one that your estimate $\\hat\\mu$ doesn't fit well.\n",
    "2. Try changing the sampling size `n`. If you make it bigger, does your fit to the 'stepline' improve significantly? What about your 'hard function'?\n",
    "3. Try fitting a version of the data without noise, i.e., `Y.without=mu(X)`. \n",
    "    - Vary `n` and compare to the estimate you get when you fit the noisy data. \n",
    "    - Do this for the 'stepline' and your 'hard function'. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise: Optimization\n",
    " \n",
    "Now let's simplify our set of constraints to make things easier for\n",
    "`CVXR`. When we have a constraint for all pairs of indices $i,j$ with\n",
    "$X_i \\le X_j$, many of them are redundant. \n",
    "\n",
    "- This is a consequence of *transitivity*: the constraints $m(X_i) \\le m(X_j)$ and\n",
    "$m(X_j) \\le m(X_k)$ imply the additional constraint $m(X_i) \\le m(X_k)$.\n",
    "- It means that if our points $X_i$ are sorted in increasing order, i.e. if $X_1 \\le X_2 \\le \\ldots \\le X_{n-1} \\le X_n$, the\n",
    "constraints $m(X_{i}) \\le m(X_{i+1})$ for $i \\in 1 \\ldots n-1$ imply the\n",
    "whole set.\n",
    "\n",
    "Here's a template to modify.  I've done the sorting for you so\n",
    "you can focus on formulating your optimization in terms of the sorted\n",
    "data. What you need to do is fix the mse and monotonicity constraint in lines 19 and 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg.fast = function(X,Y) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(!anyDuplicated(X))\n",
    "  stopifnot(length(X)==length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and reorder pairs (Xi,Yi) so Xs are sorted: X[i] <= X[i+1] <= ...\n",
    "  increasing.order=order(X)\n",
    "  Y = Y[increasing.order]\n",
    "  X = X[increasing.order]\n",
    "  \n",
    "  # Step 1. \n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^n.\n",
    "  m = Variable(n)\n",
    "\n",
    "  # Step 2. \n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y-m)^2)/n                  \n",
    "\n",
    "  # Step 3. \n",
    "  # We specify our constraints.\n",
    "  constraints = list(diff(m) >= 0) \n",
    "\n",
    "  # Step 4. \n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  \n",
    "  # Step 5: a little boilerplate to make it idiomatic R. \n",
    "  #  1. we record X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data.\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  #  3. we return the list\n",
    "  model = list(X=X, mu.hat=mu.hat, input=input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that it does what we expect when $X_1 \\ldots X_n$ *are* distinct by applying it to the same data as before and comparing to what our old code gave us.\n",
    "\n",
    "We'll plot predictions from our old code as red boxes and our new code as blue xs: we want to see red boxes with blue xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model.fast = monotonereg.fast(X,Y)\n",
    "model.slow = monotonereg(X,Y) \n",
    "\n",
    "ggplot() + geom_point(aes(x=model.fast$X, y=model.fast$mu.hat), color='blue', shape=4) + \n",
    "           geom_point(aes(x=model.slow$X, y=model.slow$mu.hat), color='red', shape=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Exercise: Generalization\n",
    "\n",
    "The code you're written above has a few limitations relative to what you might want in practice.\n",
    "\n",
    "1. There's no option to fit a decreasing function instead of an increasing one.\n",
    "2. It requires the observations $X_1 \\ldots X_n$ to be distinct.\n",
    "\n",
    "Let's address these so we get code that's a little more broadly applicable. \n",
    "\n",
    "The decreasing bit is easy. You'll need to tweak to the constraints you're passing to `CVXR`, but it's probably not too hard to figure out what the change should be.\n",
    "\n",
    "Dealing with non-distinct $X_i$ is a little trickier, so let's take a minute to prepare. The issue is that, if $X_1 \\ldots X_n$ contains duplicates,\n",
    "we can't think of $m(X_1) \\ldots m(X_n)$ as $n$ unknowns: if $X_i=X_j$, then $m(X_i)$ and $m(X_j)$ have to be the same. Handling this is just a matter\n",
    "of bookkeeping. If $X_1 \\ldots X_n$ has $p$ distinct values, then that's how many unknowns we need to solve for. And what we need is to know, for each $X_i$,\n",
    "which of these unknowns it corresponds to. To help out, I'll give you a function `invert.unique` that takes a vector $X$ and returns a list of two things.\n",
    "\n",
    "1. A vector `elements` cointaining the positions of the unique elements of $X$, so that `X[elements]` is vector of unique elements of $X$ sorted in increasing order. \n",
    "    - This vector has length $p$ where $p$ is the number of unique elements in $X$.\n",
    "2. A vector `inverse` that tells you, for each $i$, the position of $X[i]$ in $X[elements]$. \n",
    "    - This vector has length $n$ where $n$ is the length of $X$.\n",
    "    - `X[elements][inverse]` is the same as `X`.\n",
    "\n",
    "A lot of programming languages have a function like this built in---*Matlab* does and *Python* (NumPy) does---but `R` doesn't. It's a bit fussy to write, so I've saved you the trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "a = c(1,2,3,3,4,5,5,5,6)\n",
    "unique.a=invert.unique(a)\n",
    "stopifnot(a[unique.a$elements][unique.a$inverse] == a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and write a function `monotonereg` that works with non-distinct $X_i$. This isn't a big change to the code you just wrote above, but you may need to think a little bit about how to use `invert.unique` to make it work.\n",
    "When you think you've got it, test it out and make sure it works as you expect. You can use the block above to check that it does the right thing *without* duplicates, but you're on your own for checking that it works *with* duplicates.\n",
    "\n",
    "When you've got that working, test it out and make sure it works as you expect. You can use the block above to check that it does the right thing *without* duplicates, but you're on your own for checking that it works *with* duplicates.\n",
    "Then, tweak it so it fits a decreasing function instead of an increasing one if you pass `decreasing=TRUE`. Go ahead and test that out, too.\n",
    "\n",
    "I'm not going to give you a detailed template for this one because I can't think of a way to do it that doesn't totally give it away, but I'll talk you through it . You'll want to tell `CVXR` that you're thinking about a vector $m$ of $p$ unknowns, where $p$ is the number of unique elements in $X$. Go ahead an think of $m[1]$ as the function's value at the smallest level of $X_i$, $m[2]$ as the function's value at the second smallest level of $X_i$, etc. It should be pretty easy to work out what the right constraints are in terms of the vector $m$. From there, all you need to know is which position in $m$ corresponds to each *observation* $(X_i,Y_i)$, so you can use the right value of $m$ in the loss function. The `inverse` you get from `invert.unique(X)` should help you with that. Below, I've given you a not-very-helpful template to give you a cell to work in.\n",
    "\n",
    "All that said, while it's useful to be able to do little programming tasks like this, that's not really what this class is supposed to be about. If you're having trouble with this or you don't feel motivated, go ahead and skip it. It's not going to be something you'll need later on in class and I'll give you code that does all this in the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg.general = function(X, Y, decreasing = FALSE) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list(if (decreasing) { diff(m) <= 0 } else { diff(m) >= 0 })\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "We can check that it does what we expect when $X_1 \\ldots X_n$ *are* distinct by applying it to the same data as before and comparing to what our old code gave us.\n",
    "\n",
    "Again, we'll plot predictions from our old code as red boxes and our new code as blue xs: we want to see red boxes with blue xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model.general = monotonereg.general(X,Y)\n",
    "model.slow = monotonereg(X,Y) \n",
    "\n",
    "ggplot() + geom_point(aes(x=model.general$X, y=model.general$mu.hat), color='blue', shape=4) + \n",
    "           geom_point(aes(x=model.slow$X, y=model.slow$mu.hat), color='red', shape=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "All this having worked out, we may as well just call the good version `monotonereg` so we're not writing `monotonereg.general` everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg=monotonereg.general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Extending Your Solution\n",
    "\n",
    "## Sketching\n",
    "\n",
    "Now we're going we're going to extend our solution $\\hat\\mu_{\\mid \\mathcal{X}}$ from\n",
    "the previous part to a function $\\hat \\mu : \\mathbb{R} \\to \\mathbb{R}$ that we can\n",
    "evaluate at any $x \\in \\mathbb{R}$. `monotonereg` gives us pairs $(X_i, \\hat\\mu(X_i))$ for $i \\le n$.\n",
    "\n",
    "We need is code that takes an increasing point-set, i.e.\n",
    "a set $\\{(X_i, Y_i) : i\\le n\\}$ for which $Y_i \\le Y_j$ whenever\n",
    "$X_i \\le X_j$, and gives us an increasing function from $\\mathbb{R} \\to \\mathbb{R}$\n",
    "that goes through those points. We've already talked about two ways to do this.\n",
    "\n",
    "1. Piecewise-constant extension. \n",
    "2. Piecewise-linear extension. \n",
    "\n",
    "We're going to do piecewise-constant extension here because it's a bit easier to implement. \n",
    "The piecewise-linear one will come later, when we talk about Lipschiz and Convex regression.\n",
    "\n",
    "Writing code than runs on computers is hard, so to ease into this. We'll do it in three steps.\n",
    "\n",
    "1. Draw an increasing point set, then sketch the piecewise-constant extension of it. Think carefully about what you're doing.\n",
    "2. Write down an algorithm for drawing an increasing curve that goes\n",
    "through an increasing point-set $\\{(X_i, Y_i) : i \\le n\\}$. You don't\n",
    "have to write pseudocode. Think of it as giving instructions two\n",
    "different people could follow and draw exactly the same curve on top of a plot of the\n",
    "points $\\{(X_i, Y_i)\\}$. Make sure that your curve extends past the\n",
    "edges of your data. We want our increasing curve defined for all $x$ on\n",
    "the real line.\n",
    "3. As a test, use your algorithm to draw a curve through the following two\n",
    "plots of an increasing point-set $\\{(X_i, Y_i)\\}$. Then give your\n",
    "algorithm to a classmate and ask them to use it to do the same. If there\n",
    "are any differences between what you draw and they do, one of two things\n",
    "went wrong. Either your algorithm was too vague to have a well-defined\n",
    "outcome or somebody made an error following it. Figure out which and\n",
    "eliminate any vagueness. Soon you'll be implementing this on a computer,\n",
    "which can't handle vague code.\n",
    "\n",
    "Here are a few increasing point-sets you can try your algorithm on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x=seq(.05,.95,by=.1)\n",
    "\n",
    "y=1-cos(pi*x/2)\n",
    "ggplot(data.frame(x=x,y=y), aes(x=x, y=y)) + geom_point() + ylim(0,1)\n",
    "\n",
    "y=((1+x)/2)*(.3 + .5*(x >= .5) + .2*(x >= .8))\n",
    "ggplot(data.frame(x=x,y=y), aes(x=x, y=y)) + geom_point() + ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation\n",
    "\n",
    "Once you've got that working, implement a `predict` function (like\n",
    "`predict.linereg` from the warm-up) to fill in the gaps. Then add the curve to your plot\n",
    "from the last exercise.\n",
    "\n",
    "**Tips.**\n",
    "\n",
    "1.  It's likely that your algorithm for drawing an increasing curve\n",
    "    through a point-set $\\{(X_i, Y_i) : i \\le n\\}$ involves knowing the\n",
    "    closest point $X_i$ to the left of each $x \\in \\mathbb{R}$, or to\n",
    "    the right of it, or both. To make this easier, I recommend you sort\n",
    "    everything so $X_1 \\le X_2 \\le \\ldots \\le X_n$ throughout.\n",
    "\n",
    "2.  When making your predictions, you can use the built-in `R` function\n",
    "    `findInterval` to find the index $i$ of the closest point to the\n",
    "    left of $x$. In particular, if $X$ is the sorted vector\n",
    "    $[X_1 X_2 \\ldots X_n]$ and $x$ is a vector of query points,\n",
    "    `findInterval(x,X)` will return a vector $i$ such that $X_{i_k}$ is\n",
    "    the closest point to the left of $x_k$. Naturally $X_{i_k+1}$ will\n",
    "    be the closest point to the right of $x_k$. You may have to handle\n",
    "    some edge cases. If there is no observation $X_i$ left of $x_k$,\n",
    "    then the entry $i_k$ will be zero; if there is no observation\n",
    "    $X_{i}$ to the right of it, $i_{k}+1$ will be $n+1$.\n",
    "\n",
    "Here's a template. To help you out, I'll give you code to put your\n",
    "observations $X_i$ and corresponding predictions $\\hat\\mu(X_i)$ in\n",
    "sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.monotonereg = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  y = model$mu.hat; X=model$X; x=newdata$X\n",
    "  # for each new data point x in newdata$X, \n",
    "  # find the closest observed X[k] left of x\n",
    "  # i.e., the largest k for which X[k] <= x \n",
    "  # this covers cases 1 and 2\n",
    "  # bin will be a vector of these numbers k, with one for each x in newdata$X\n",
    "  i = findInterval(x, X) \n",
    "  # if there is no X[k] < x, findInterval tells us k=0\n",
    "  # to cover case 3, we want X[k] for k=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  y[i]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented this predict method, let's use it to plot a curve.\n",
    "We'll add this curve to the plot of our observations and our predictions\n",
    "$\\hat\\mu(X_i)$ that we made in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x = seq(0,1,by=.001)\n",
    "muhat.x = predict.monotonereg(model, newdata=data.frame(X=x))\n",
    "\n",
    "fit.on.sample + geom_line(aes(x=x, y=muhat.x), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Application\n",
    "\n",
    "Now let's see what this tells us in an application. We'll look at the application discussed in the\n",
    "'Actually using this stuff ...' section of the Intro Lecture's slides:  estimating the effect of class size \n",
    "on 5th graders' test scores in Israel using a *Regression Discontinuity*. Almost all of the code in this section\n",
    "is all written for you. Your job is interpreting the results.\n",
    "\n",
    "We'll start by grabbing some data from my website and plotting it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "school=read.csv('https://davidahirshberg.bitbucket.io/data/israel-schools-enrollment-model-1200-points.csv')\n",
    "ggplot() + geom_point(aes(x=enrollment, y=score), alpha=.2, data=school)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from our first lecture that class sizes are capped at 40, so\n",
    "there are $x$ students per class in schools with enrollment $x \\le 40$,\n",
    "an average of $x/2$ in schools with enrollment $x \\in [41,80]$, an\n",
    "average of $x/3$ in schools with enrollment $x \\in [81,120]$, and so on.\n",
    "To keep things simple, we'll just be comparing schools with enrollment\n",
    "$x \\le 80$. We'd expect test scores to *decrease* as a function of class\n",
    "size and therefore, if we stick to one of the ranges $[1,40]$ and\n",
    "$[41,80]$, with enrollment. \n",
    "\n",
    "For this, we'll want to have done the optional 'Generalizing' exercise in Part 1,\n",
    "as we have repeated values of $X_i$ in our data and we want to fit a decreasing function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our estimate, $\\hat \\mu_{right}(40) - \\hat \\mu_{left}(40)$, of\n",
    "the effect if decreasing classes from size $40$ to size $40/2=20$. Since\n",
    "you're not starting out with working monotone regression code, I'll use a line\n",
    "instead. You'll change that by redefining `reg` to be a monotone regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "reg = function(X,Y) { monotonereg(X,Y,decreasing = TRUE) }\n",
    "\n",
    "estimate.effect = function(fit.model) { \n",
    "  data.left  = school[school$enrollment <= 40, ]\n",
    "  data.right = school[40 < school$enrollment & school$enrollment <= 80, ]\n",
    "\n",
    "  model.left  = fit.model(data.left$enrollment,   data.left$score)\n",
    "  model.right = fit.model(data.right$enrollment,  data.right$score)\n",
    "\n",
    "  effect.estimate = predict(model.right, newdata = data.frame(X=40)) - \n",
    "                    predict(model.left, newdata  = data.frame(X=40))\n",
    "  \n",
    "  list(model.left  = model.left, \n",
    "       model.right = model.right, \n",
    "       data.left   = data.left,\n",
    "       data.right  = data.right,\n",
    "       estimate=effect.estimate)\n",
    "}\n",
    "\n",
    "estimate = estimate.effect(reg)\n",
    "estimate$estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And let's plot our fits $\\hat\\mu_{left}$ and $\\hat\\mu_{right}$ over the\n",
    "data to get a sense of what our estimate is based on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot.effect.estimate = function(estimate) { \n",
    "  x.left  = seq(0,40, by=.1)\n",
    "  x.right = seq(40,80,by=.1)\n",
    "  plot.left  = data.frame(enrollment=x.left,  \n",
    "                          score=predict(estimate$model.left,  newdata=data.frame(X=x.left)))\n",
    "  plot.right = data.frame(enrollment=x.right, \n",
    "                          score=predict(estimate$model.right, newdata=data.frame(X=x.right)))\n",
    "  ggplot() + geom_point(aes(x=enrollment, y=score), alpha=.2, \n",
    "                        data=rbind(estimate$data.left, estimate$data.right)) +\n",
    "\t           geom_line(aes(x=enrollment,  y=score), color='blue',  data=plot.left) +\n",
    "\t           geom_line(aes(x=enrollment,  y=score), color='blue',  data=plot.right) +\n",
    "\t           geom_vline(aes(xintercept=40)) \n",
    "}\n",
    "plot.effect.estimate(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you've got your monotone regression code code written and modified the blocks above to use it\n",
    "(just change `reg` in the first of the two), rerunning these blocks will give you a new treatment effect estimate and contextualize it\n",
    "with an illustration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compare the effect estimates we get using lines and monotone regression. Which estimate are you inclined to trust? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "That speaks to a more general approach we can use for any model.\n",
    "\n",
    "1. Work out the *set of functions on the sample*, $\\mathcal{M}|_{\\mathcal{X}}$, that agree with functions in the model.\n",
    "   $$ \\mathcal{M}|_{\\mathcal{X}} = \\{ m|_{\\mathcal{X}} : m \\in \\mathcal{M} \\} $$\n",
    "   Because there are only finitely many observations, this is a finite-dimensional set. Often, we can optimize over it using CVXR.\n",
    "2. Extend your solution to a function that's actually in the model however you like.\n",
    "    - This can be pretty mechanical, but it is possible to screw it up. Let's look at how.\n",
    "agree with functions in the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# A Variation\n",
    "\n",
    "If you have extra time, check out [nearly-monotone\n",
    "regression](https://www.stat.cmu.edu/~ryantibs/papers/neariso.pdf).[^1]\n",
    "The nearly-monotone model adds a bit of slack so you can fit curves that\n",
    "aren't monotone, but are close. This is a version for nearly-increasing\n",
    "curves. The parameter $B$, which is up to you, controls how much slack\n",
    "is allowed. \n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\substack{m \\\\ \\sum_{i=1}^{n-1} \\{m(X_{i})-m(X_{i+1})\\}_+ \\le B}} \\frac{1}{n}\\sum_{i=1}^n \\left\\{Y_i - m(X_i)\\right\\}^2 \n",
    "\\quad \\text{ where } \\quad \\{x\\}_+ = \\begin{cases} x &\\text{if} \\  x \\ge 0 \\\\ 0 &\\text{ if } x < 0. \\end{cases} \\quad \\text{ having sorted the data so } X_1 \\le X_2 \\le \\ldots \\le X_n. \n",
    "$$ \n",
    "\n",
    "You should be able to implement this with a very small change to your `CVXR` code if you've done the optional exercise in Part 1 where you worked with sorted $X_1 \\ldots X_n$.\n",
    "\n",
    "[^1]: It's called nearly-isotonic regression there. Monotone and isotonic are synonyms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
