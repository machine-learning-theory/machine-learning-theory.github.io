{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Today, we're going to be implementing least squares regression subject\n",
    "to an *increasingness* constraint. That is, we're going to find a curve\n",
    "$\\hat \\mu$ that solves this optimization problem.\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i) \\}^2.\n",
    "$$\n",
    "\n",
    "Then we'll see what it tells us about the effect of reducing class\n",
    "sizes for 5th graders using some admittedly fake data about test outcomes.\n",
    "\n",
    "\n",
    "We'll use a few libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "})\n",
    "\n",
    "# OSQP claims some feasible problems aren't, so we'll tell CVXR not to use it\n",
    "CVXR::add_to_solver_blacklist('OSQP')  \n",
    "\n",
    "# And we'll style our plots  \n",
    "theme_update(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    panel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                    legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                    legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\tpanel.grid.major=element_line(color=rgb(1,0,0,.1,  maxColorValue=1)),\n",
    "\t        panel.grid.minor=element_line(color=rgb(0,0,1,.1,  maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by focusing on the increasing case. We'll use `CVXR` to help us solve the following optimization problem.\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m} \\frac{1}{n}\\sum_{i=1}^n \\left\\{ Y_i - m(X_i) \\right\\}^2.\n",
    "$$\n",
    "\n",
    "This might feel like a tall order because there are *a lot* of increasing functions. The set is infinite-dimensional, which essentially means you can't play 20 questions. \n",
    "\n",
    "- If you know I'm thinking of an increasing function, there's no number of questions you can ask me that'll let you pin down which one. \n",
    "- If I wanted to cheat at the game, every time you said 'is it this one' I could come up with an increassing function that's different from the one you guessed *and* consistent with all the answers I've given you so far.\n",
    "\n",
    "But it turns out that finding a solution to optimization problem is pretty easy if you break it down into steps in the right way. We're going to use an approach I'll call **restriction and extension**, which often works when you're trying to solve infinite dimensional optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Pedantry \n",
    "\n",
    "## Functions, Restriction, and Extension\n",
    "\n",
    "When we say *an increasing function*, we tend to think of an unbroken curve that goes up as you move from left to right. Like the curve $f(x)=e^x$ that we've coded up and plotted below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x = seq(-1,1,by=.01)\n",
    "f = function(x) { exp(x) }\n",
    "ggplot() + geom_line(aes(x=x, y=f(x)), color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To be precise, $f$ is a *function on the real line* because it tells us how to take any *input* $x$ on the real line to an *output* $f(x)$. \n",
    "- To be even more precise, $f$ is a function *from* the real line *to* the real line, because those outputs are real numbers too.\n",
    "\n",
    "Here's another increasing function $g$. It's a function on a set of five points $\\mathcal{X} = \\{-1, -\\frac12, 0, +\\frac12, +1\\}$. Here's how we might code and plot this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X  = c(-1,-1/2,0,1/2,1) \n",
    "e = exp(1)\n",
    "g = function(x) { \n",
    "    case_when(x == -1    ~  1/e,\n",
    "              x == -1/2  ~  sqrt(1/e),\n",
    "              x == 0     ~  1,\n",
    "              x == +1/2  ~  sqrt(e),\n",
    "              x == +1    ~  e)\n",
    "}\n",
    "\n",
    "ggplot() + geom_point(aes(x=X, y=g(X)), color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each valid input to $g$ is a valid input to $f$ as well, i.e. because the set of five points $\\mathcal{X}$ is a *contained in* the set of all real numbers, we can ask if the outputs $f(x)$ and $g(x)$ coincide on $\\mathcal{X}$. And they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot() + geom_line(aes(x=x, y=f(x)), color='blue') + \n",
    "           geom_point(aes(x=X, y=g(X)), color='red') \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But they aren't the same thing. The function $f$ is defined for all real numbers, while the function $g$ is only defined for the five points in $\\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "f(3/4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "g(3/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the this happens---when $f(x)$ and $g(x)$ coincide on a set $\\mathcal{X}$ but $g$ is not defined elsewhere---we say that ...\n",
    "- $g$ is **the** *restriction* of $f$ to $\\mathcal{X}$\n",
    "- $f$ is **an** *extension* of $g$ to the real line.\n",
    "Those are the formal terms. \n",
    "\n",
    "There are other extensions of $g$. For example ...\n",
    "- a **piecewise linear** extension. It's what we get by connecting the dots in the plot of $g$ above with straight lines segments. That's what ggplot's geom_line does.\n",
    "- a **piecewise constant** extension. We get one by moving horizontally rightward from each dot until we hit the next one. That's what ggplot's geom_step does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ggplot() + geom_point(aes(x=X, y=g(X)), color='red') + \n",
    "           geom_line(aes(x=X, y=g(X)),  color='red', linetype='dashed') +\n",
    "           geom_step(aes(x=X, y=g(X)),  color='red', linetype='dotted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not quite getting extensions of $g$ to the real line when we use geom_line and geom_step. We're getting extensions to the unit interval, the range between the largest and smallest point in $\\mathcal{X}$. \n",
    "\n",
    "**R** has a built-in function `approxfun` that will extend functions from a set of points to the real line. \n",
    "\n",
    "  - We can ask for piecewise-constant and piecewise-linear extensions. \n",
    "  - But the piecewise-linear extension isn't the one we usually want. \n",
    "\n",
    "We'll write our own extension code later on in this lab so we can get it to do what we want. What's wrong with `approxfun`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "g.linear   = approxfun(X,g(X), rule=2, method='linear')\n",
    "g.constant = approxfun(X,g(X), rule=2, method='constant')\n",
    "\n",
    "x = seq(-2,2,by=.01)\n",
    "ggplot() + geom_point(aes(x=X, y=g(X)),  color='red') + \n",
    "  geom_line(aes(x=x,  y=g.linear(x)),    color='red', linetype='dashed') +\n",
    "  geom_line(aes(x=x,  y=g.constant(x)),  color='red', linetype='dotted') +\n",
    "  geom_line(aes(x=x,  y=f(x)),           color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Terminology and Notation\n",
    "\n",
    "- We say $f$ is *a function from $\\mathcal{X}$ to $\\mathcal{Y}$* if, for every point $x$ in 'set of possible inputs' $\\mathcal{X}$, we know the corresponding value $f(x)$ and it is in the 'set of possible outputs' $\\mathcal{Y}$. \n",
    "  - To make this a little more compact, often people write $f: \\mathcal{X} \\to \\mathcal{Y}$ with this meaning. \n",
    "  - Typically this is prounounced exactly the same way, i.e. as  '$f$ is a function from $\\mathcal{X}$ to $\\mathcal{Y}$'.\n",
    "  - Or more efficiently as '$f$ *maps* $\\mathcal{X}$ to $\\mathcal{Y}$'.\n",
    "- If we expect the set of possible outputs to be inferred from context, we might say '$f$ is a function on $\\mathcal{X}$', which is a little shorter. \n",
    "    - That's a pretty safe bet in this class because our outputs are almost always real numbers.\n",
    "    - As far as I know, there isn't really an accepted notation for this. People still write $f: \\mathcal{X} \\to \\mathcal{Y}$ even when the output set is clear from context.\n",
    "    - The options $f: \\mathcal{X} \\to$ or $f: \\mathcal{X} \\to [\\text{you figure it out}]$ are awkward and aggressive respectively, and while $f: \\mathcal{X}$ is a little better, I don't see it much.\n",
    "- When we want to write about a relationship like the one between $f$ and $g$, we often write $f|_{\\mathcal{X}}$, which is pronounced 'the restriction of $f$ to $\\mathcal{X}$'. The statement $g=f|_{\\mathcal{X}}$ is read as '$g$ is the restriction of $f$ to $\\mathcal{X}$'.\n",
    "- I often find myself saying $f$ and $g$ *agree on* $\\mathcal{X}$, by which I mean that $f(x)=g(x)$ for all $x$ in $\\mathcal{X}$. \n",
    "    - This means restrictions of $f$ and $g$ to $\\mathcal{X}$ are the same, i.e. that $f|_{\\mathcal{X}} = g|_{\\mathcal{X}}$, but doesn't tell us that one is a restriction of the other.\n",
    "    - If we think about our code for $f$ and $g$ above, the distinction is that that $g$ might not 'return NA' for inputs $x$ that aren't in $\\mathcal{X}$.  It might just return a value that's not $f(x)$.\n",
    "    - For example, $f$, $g$, and the piecewise-constant and piecewise-linear extensions of $g$ all agree on $\\mathcal{X}$. Their restrictions to $\\mathcal{X}$ are all the same function: $g$.\n",
    "- Sometimes people use different words for extension inside and outside the range of $\\mathcal{X}$.\n",
    "  - *Interpolation* is the process of extending a function to points inside the range of $\\mathcal{X}$. That's what `geom_line` and `geom_step` did for us above.\n",
    "  - *Extrapolation* is the process of extending a function to points outside the range of $\\mathcal{X}$. That's what we had to use `approxfun` for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Write a function $h$ that agrees with $f(x)=e^x$ on the unit interval $[-1,+1]$ but is constant elsewhere. Plot $h$ and $f$ on the same graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasingness \n",
    "\n",
    "If we have a function $f$ on a set of points $\\mathcal{X}$ on the real line, we can ask if it's increasing. \n",
    "\n",
    "- Intuitively, that means that, as we move from left to right, the values of $f$ get bigger. Or actually, because this turns out to be more convenient, that they don't get smaller. \n",
    "- Formally, we say the $f: \\mathcal{X} \\to \\mathbb{R}$ is increasing if $f(x) \\le f(x')$ for all pairs of points $x,x'$ in $\\mathcal{X}$ with $x \\le x'$.\n",
    "\n",
    "All of the functions we've seen so far are increasing. \n",
    "\n",
    "- The ones with flat segments, like $h$ and the piecewise-constant extension of $g$, are sometimes called *non-decreasing* instead of increasing.\n",
    "    - This aligns a bit better with plain english, but it tends to make your sentences awkward and harder to understand.\n",
    "    - Math folks tend to say 'strictly increasing' if they want to rule out flat segments, but it's pretty rare that they want to. \n",
    "    - Informally, I'll say 'gets bigger' meaning 'doesn't get smaller' analogously.\n",
    "    \n",
    "An *increasing function on the real line* ($\\mathbb{R}$) is a function on the real line that gets bigger whenever $x \\in \\mathbb{R}$ does. That is, its values satisfy \n",
    "\n",
    "$$ \n",
    "f(x) \\le f(x') \\text{ for all pairs of points \\textbf{on the real line} satisfying } x \\le x' \n",
    "$$\n",
    "\n",
    "An *increasing function on the set* $\\mathcal{X}$ is one where this value gets bigger whenever $x \\in \\mathcal{X}$ does. That is, its values satisfy \n",
    "\n",
    "$$ \n",
    "f(x) \\le (x') \\text{ for all pairs of points \\textbf{in $\\mathcal{X}$} satisfying $x \\le x'$} \n",
    "$$\n",
    "\n",
    "There are two things you should know about increasingness and restriction/extension. Suppose $\\mathcal{X}$ is a set of real numbers, i.e., a subset of the real line.\n",
    "\n",
    "1. If $f$ is an increasing function on the real line: $f_{\\mathcal{X}}$, *the restriction* of $f$ to $\\mathcal{X}$, is increasing.\n",
    "2. If $g$ is an increasing function on $\\mathcal{X}$: *it has* an increasing extension to the real line. That is, there is an increasing function $f$ on the real line that agrees with $g$ on $\\mathcal{X}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If $f(x) \\le f(x')$ for all pairs of real numbers $x,x'$ with $x \\le x'$, then $f(x) \\le f(x')$ for all pairs of real numbers with $x \\le x'$ **hat are both in the set $\\mathcal{X}$*.\n",
    "2. The piecewise-constant extension of $g$ is increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Is it true that *every* extension of an increasing function $g$ on $\\mathcal{X}$ is increasing? \n",
    "\n",
    "  - If so, explain why. \n",
    "  - If not, draw a counterexample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enough Already (Almost)\n",
    "\n",
    "If you think all this sounds a like a waste of time, I'm sympathetic. I don't usually talk this formally and I certainly don't usually write 7 lines of code when `g=exp` will do. But sometimes being formal about stuff like this can help focus our thinking.\n",
    "\n",
    "**Question**. If I'm thinking of an increasing function on $\\mathcal{X}=\\{-1,-\\frac12,0,+\\frac12,+1\\}$, or any function on $\\mathcal{X}$, how many questions would you need to ask to pin down which one I'm thinking of? \n",
    "\n",
    "**Answer**. 5. You can just ask me for the value of the function at each point in $\\mathcal{X}$.\n",
    "\n",
    "What this tells us is that, even if it's impossible to find out exactly what increasing function $f$ on the real line I'm thinking of, it can be pretty easy to find out its restriction $f_{\\mathcal{X}}$ to a finite set of points. With this in mind, let's take a look at our optimization problem again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Least Squares Problem \n",
    "## Step 1. Restrict and Solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the least squares problem we've been writing. It's a little vague.\n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\text{increasing} \\ m} \\frac{1}{n}\\sum_{i=1}^n \\left\\{ Y_i - m(X_i) \\right\\}^2.\n",
    "$$\n",
    "\n",
    "What's vague about it? It doesn't specify *what kind of function* the solution $\\hat\\mu$ is supposed to be, i.e., the sets of possible inputs and outputs. \n",
    "\n",
    "That vagueness speaks to a tension between what we want and what we can know.\n",
    "\n",
    "- The data we have is only informative about the values of the function on the sample---the set $\\mathcal{X}=\\{X_1,\\ldots,X_n\\}$.\n",
    "    - If two functions on the real line agree on the sample, then either they're both solutions to this optimization problem or neither is. \n",
    "    - This means that, at best, we can hope to identify a function on the sample.\n",
    "    - In a way, this is a good thing. It means we don't have to solve an infinite-dimensional optimization problem. Computers like that.\n",
    "- Often, we will want to make predictions for points that aren't in the sample, so we'll want to find a function on the real line. \n",
    "    - This means that, whatever solution we do get, we'll have to extend it to the real line. \n",
    "    - And the data doesn't prefer any one extension to another. That's on us to make up.\n",
    "    - This isn't great, but it's not damning either. In large samples, the extension we use doesn't matter much. Why?\n",
    "    \n",
    "Here's the recipe for solving this optimization problem.\n",
    "1. Think of it as a problem of choosing from the set of increasing functions on the sample.\n",
    "    - That's a finite-dimensional model: we need one parameter for each distinct value of $X_1 \\ldots X_n$.\n",
    "    - We can solve it using CVXR, just like we did for the linear regression problem.\n",
    "2. Once we have a solution, we can extend it to an increasing function on the real line in any way we like. The data doesn't care.\n",
    "    - We can, for example, do piecewise constant extension or piecewise linear extension.\n",
    "    - If we have an increasing function on the sample, these extensions will be increasing functions on the real line.\n",
    "    \n",
    "Let's solve for a function on the sample first.  We'll worry about the extension later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Implementation in Mathematical Notation\n",
    "\n",
    "To keep things simple we'll start by assuming that the observed values of $x$, $X_1 \\ldots  X_n$, are distinct. \n",
    "\n",
    "`CVXR` likes to work with vectors, not functions on finite sets, so we'll have to do a little translation.\n",
    "\n",
    " - We'll tell `CVXR` we're interested in optimizing over vectors $\\vec m$ in some set $\\vec M \\subseteq \\mathbb{R}^n$. \n",
    " - We'll interpret these vectors as functions $m:\\mathcal{X} \\to \\mathbb{R}$  using the correspondence $m(X_i) = \\vec m_i$. \n",
    "\n",
    "Plugging this correspondence into our optimization problem, we get ...\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\mu(X_i) &= \\vec\\mu_i  \n",
    "&&  \\text{ where } \\\\\n",
    "&\\vec \\mu = \\operatorname*{argmin}_{\\vec m \\in \\vec M}  \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - \\vec m_i \\}^2  \n",
    "&& \\text{ for } \\\\ \n",
    "& \\textcolor{red}{\\vec M = \\text{some set of vectors in} \\  \\mathbb{R}^n}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we want $\\hat\\mu$ to solve the optimization problem we started with, i.e. $\\hat\\mu = \\operatorname*{argmin}\\limits_{\\text{increasing} \\ m: \\mathcal{X} \\to \\mathcal{R}} \\frac{1}{n}\\sum_{i=1}^n \\left\\{ Y_i - m(X_i) \\right\\}^2$, what should $\\textcolor{red}{\\vec M}$ be?\n",
    "\n",
    "**Tips**.\n",
    "\n",
    "1. Look back at the definition of increasingness and think about what it means for a function on $\\mathcal{X}=\\{X_1 \\ldots X_n\\}$. Finish this sentence. \n",
    " > $m:\\mathcal{X} \\to \\mathbb{R}$ is increasing *if and only if* the $n$ values $m(X_1) \\ldots m(X_n)$ satisfy ...\n",
    "2. If you've expressed what you want in terms of the $n$ values $m(X_1) \\ldots m(X_n)$, you can translate it into a statement about $\\vec m$ mechanically. Just replace $m(X_i)$ with $\\vec m_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Implementation in `CVXR`\n",
    "\n",
    "Write some monotone regression code. Then use it to plot $\\hat\\mu(X_i)$ on top of the data below. \n",
    "\n",
    "I've written a template for you to fill in. \n",
    "  - If you run it without changes, it'll give you *an* increasing function, by which I mean a vector that satisfies the constraints \n",
    "in the optimization problem written out above. \n",
    "  - But not necessarily one that's a good fit to the data. \n",
    "  - Go ahead and make changes where indicated to get the right fit.\n",
    "\n",
    "There are a few tips below the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg = function(X,Y) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(!anyDuplicated(X))\n",
    "  stopifnot(length(X)==length(Y))\n",
    "  n = length(X)\n",
    "\n",
    "  # Step 1. \n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^n.\n",
    "  m = Variable(n)\n",
    "\n",
    "  # Step 2. \n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum(m^2)                     ## CHANGE THIS\n",
    "\n",
    "  # Step 3. \n",
    "  # We specify our constraints.\n",
    "  constraints = list(m == 0)         ## CHANGE THIS\n",
    "\n",
    "  # Step 4. \n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R. \n",
    "  #  1. we record the vector of inputs, X, and the vector of corresponding outputs, mu.hat, in a list\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  #  3. we return the list\n",
    "  model = list(X=X, mu.hat=mu.hat)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg = function(X,Y) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(!anyDuplicated(X))\n",
    "  stopifnot(length(X)==length(Y))\n",
    "  n = length(X)\n",
    "\n",
    "  # Step 1. \n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^n.\n",
    "  m = Variable(n)\n",
    "\n",
    "  # Step 2. \n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y-m)^2)/n                                       ## CHANGED \n",
    "\n",
    "  # Step 3. \n",
    "  # We specify our constraints.\n",
    "  all.pairs = expand.grid(i=1:n, j=1:n)\n",
    "  le = all.pairs$i <= all.pairs$j \n",
    "  le.pairs = all.pairs[le,]\n",
    "  constraints = list(m[le.pairs$i] <= m[le.pairs$j])         ## CHANGED\n",
    "\n",
    "  # Step 4. \n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R. \n",
    "  #  1. we record the vector of inputs, X, and the vector of corresponding outputs, mu.hat, in a list\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  #  3. we return the list\n",
    "  model = list(X=X, mu.hat=mu.hat)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Tips.**\n",
    "\n",
    "1.  `CVXR` takes a list of constraints expressed in terms of a vector of unknowns `m` that it calls a Variable.\n",
    "    - These are expressions that would evaluate to TRUE or FALSE if `m` were a vector of numbers, e.g. `m[i] <= m[j]` for some pair of indices `i` and `j`.  \n",
    "    - It's also happy if these constraints in this list are *vector-valued expressions*, i.e. expressions that would evaluate to a vector with elements that are TRUE or FALSE. \n",
    "      - In this case, it enforces the constraint that all elements of the vector are TRUE when it solves for `m`. \n",
    "      - This means that if you've got *vectors* of indices `i` and `j`, saying `m[i] <= m[j]` will enforce that `m[i[1]] <= m[j[1]]`, `m[i[2]] <= m[j[2]]`, etc.\n",
    "\n",
    "2.  If you want to make a list of all pairs of indices $i$ and $j$ with $X_i \\le X_j$, you can do it like this.\n",
    "    - Make a table of all pairs of indices. \n",
    "    - Find the rows where $X_i \\le X_j$. \n",
    "\n",
    "    Some code that does this is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X = c(-1,-1/2,0,1/2,1)\n",
    "n = length(X)\n",
    "\n",
    "all.pairs=expand.grid(i=1:n,j=1:n)\n",
    "le = X[all.pairs$i] < X[all.pairs$j]\n",
    "le.pairs = all.pairs[le,]\n",
    "\n",
    "i = le.pairs$i\n",
    "j = le.pairs$j\n",
    "\n",
    "i\n",
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Your Solution\n",
    "\n",
    "Here's some code to help you see what yours is doing. It uses the code you've written to fit a function to some data\n",
    "and then plots the function and data together. We'll use data sampled around our old friend from the warm-up, the 'stepline'. \n",
    "It'll plot observations $(X_i, Y_i)$ in as gray dots and the predictions $(X_i, \\hat\\mu(X_i))$ as blue ones. And because \n",
    "this is fake data, we can plot the function we've sampled our points around, too. That'll be a black line.\n",
    "\n",
    "You don't need to change this code at all, just run it after you've written what you need to above. If you run this code\n",
    "without changing *that*, $\\hat\\mu$ will be some increasing function but not necessarily a good one. That said, it might\n",
    "be good in some places and not others---a stopped clock is right twice a day.\n",
    "\n",
    "If you've written the code above correctly, you should get a pretty good estimate of the black line. \n",
    "It is, after all, an increasing function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# data\n",
    "mu = function(x) { x*(x >= .5) }\n",
    "sigma = .1\n",
    "\n",
    "n = 100\n",
    "X = seq(0,1,length.out=n)\n",
    "Y = mu(X) + sigma*rnorm(n)\n",
    "\n",
    "# our fit curve\n",
    "model   = monotonereg(X,Y)\n",
    "\n",
    "# a grid for plotting and a plot\n",
    "x = seq(0,1,by=.001) \n",
    "fit.on.sample = ggplot() + \n",
    "\tgeom_point(aes(x=X,y=Y),  alpha=.2) + \n",
    "\tgeom_line(aes(x=x,y=mu(x)), alpha=1) +\n",
    "\tgeom_point(aes(x=model$X,y=model$mu.hat), color='blue', alpha=.2)\n",
    "fit.on.sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Exploration\n",
    "\n",
    "1. Try changing the function `mu` that we're sampling our data around. Find a 'hard function' one that your estimate $\\hat\\mu$ doesn't fit well.\n",
    "2. Try changing the sampling size `n`. If you make it bigger, does your fit to the 'stepline' improve significantly? What about your 'hard function'?\n",
    "3. Try fitting a version of the data without noise, i.e., `Y.without=mu(X)`. \n",
    "    - Vary `n` and compare to the estimate you get when you fit the noisy data. \n",
    "    - Do this for the 'stepline' and your 'hard function'. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimization\n",
    " \n",
    "Now let's simplify our set of constraints to make things easier for\n",
    "`CVXR`. When we have a constraint for all pairs of indices $i,j$ with\n",
    "$X_i \\le X_j$, many of them are redundant. \n",
    "\n",
    "- This is a consequence of *transitivity*: the constraints $m(X_i) \\le m(X_j)$ and\n",
    "$m(X_j) \\le m(X_k)$ imply the additional constraint $m(X_i) \\le m(X_k)$.\n",
    "- It means that if our points $X_i$ are sorted in increasing order, i.e. if $X_1 \\le X_2 \\le \\ldots \\le X_{n-1} \\le X_n$, the\n",
    "constraints $m(X_{i}) \\le m(X_{i+1})$ for $i \\in 1 \\ldots n-1$ imply the\n",
    "whole set.\n",
    "\n",
    "Here's a template to modify.  I've done the sorting for you so\n",
    "you can focus on formulating your optimization in terms of the sorted\n",
    "data. What you need to do is fix the mse and monotonicity constraint in lines 19 and 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution-template"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg.fast = function(X,Y) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(!anyDuplicated(X))\n",
    "  stopifnot(length(X)==length(Y))\n",
    "  n = length(X)\n",
    "  # and reorder pairs (Xi,Yi) so Xs are sorted: X[i] <= X[i+1] <= ...\n",
    "  X.orig = X\n",
    "  increasing.order=order(X)\n",
    "  Y = Y[increasing.order]\n",
    "  X = X[increasing.order]\n",
    "  \n",
    "  # Step 1. \n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^n.\n",
    "  m = Variable(n)\n",
    "\n",
    "  # Step 2. \n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum(m^2)                     ## CHANGE THIS. X AND Y ARE IN CORRESPONDENCE AND X IS SORTED IN INCREASING ORDER\n",
    "\n",
    "  # Step 3. \n",
    "  # We specify our constraints.\n",
    "  constraints = list(m == 0)         ## CHANGE THIS. X AND Y ARE IN CORRESPONDENCE AND X IS SORTED IN INCREASING ORDER\n",
    "\n",
    "  # Step 4. \n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  \n",
    "  # Step 5: a little boilerplate to make it idiomatic R. \n",
    "  #  1. we record X and mu.hat, in correspondence and sorted in increasing order of X, and X in its original order, in a list\n",
    "  #  2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  #  3. we return the list\n",
    "  model = list(X=X, mu.hat=mu.hat, X.original.order=X.orig) \n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test that your new code does the same thing as the old stuff by running the block below. If you've done everything right, \n",
    "you should get essentially the same answer from both versions of the code. There might be some small differences due to rounding error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "model = monotonereg(X,Y)\n",
    "model.fast = monotonereg.fast(X,Y)\n",
    "max.difference = max(abs(as.vector(model$mu.hat)-as.vector(model.fast$mu.hat)))\n",
    "max.difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Generalization\n",
    "\n",
    "The code you're written above has a few limitations relative to what you might want in practice.\n",
    "\n",
    "1. There's no option to fit a decreasing function instead of an increasing one.\n",
    "2. It requires the observations $X_1 \\ldots X_n$ to be distinct.\n",
    "\n",
    "Let's address these so we get code that's a little more broadly applicable. \n",
    "\n",
    "The decreasing bit is easy. You'll need to tweak to the constraints you're passing to `CVXR`, but it's probably not too hard to figure out what the change should be.\n",
    "\n",
    "Dealing with non-distinct $X_i$ is a little trickier, so let's take a minute to prepare. The issue is that, if $X_1 \\ldots X_n$ contains duplicates,\n",
    "we can't think of $m(X_1) \\ldots m(X_n)$ as $n$ unknowns: if $X_i=X_j$, then $m(X_i)$ and $m(X_j)$ have to be the same. Handling this is just a matter\n",
    "of bookkeeping. If $X_1 \\ldots X_n$ has $p$ distinct values, then that's how many unknowns we need to solve for. And what we need is to know, for each $X_i$,\n",
    "which of these unknowns it corresponds to. To help out, I'll give you a function `invert.unique` that takes a vector $X$ and returns a list of two things.\n",
    "\n",
    "1. A vector `elements` cointaining the positions of the unique elements of $X$, so that `X[elements]` is vector of unique elements of $X$ sorted in increasing order. \n",
    "    - This vector has length $p$ where $p$ is the number of unique elements in $X$.\n",
    "2. A vector `inverse` that tells you, for each $i$, the position of $X[i]$ in $X[elements]$. \n",
    "    - This vector has length $n$ where $n$ is the length of $X$.\n",
    "    - `X[elements][inverse]` is the same as `X`.\n",
    "\n",
    "A lot of programming languages have a function like this built in---*Matlab* does and *Python* (NumPy) does---but `R` doesn't. It's a bit fussy to write, so I've saved you the trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "a = c(1,2,3,3,4,5,5,5,6)\n",
    "unique.a=invert.unique(a)\n",
    "stopifnot(a[unique.a$elements][unique.a$inverse] == a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and write a function `monotonereg` that works with non-distinct $X_i$. This isn't a big change to the code you just wrote above, but you may need to think a little bit about how to use `invert.unique` to make it work.\n",
    "When you think you've got it, test it out and make sure it works as you expect. You can use the block above to check that it does the right thing *without* duplicates, but you're on your own for checking that it works *with* duplicates.\n",
    "\n",
    "When you've got that working, test it out and make sure it works as you expect. You can use the block above to check that it does the right thing *without* duplicates, but you're on your own for checking that it works *with* duplicates.\n",
    "Then, tweak it so it fits a decreasing function instead of an increasing one if you pass `decreasing=TRUE`. Go ahead and test that out, too.\n",
    "\n",
    "I'm not going to give you a template for this one because I can't think of a way to do it that doesn't totally give it away, but I'll talk you through it . You'll want to tell `CVXR` that you're thinking about a vector $m$ of $p$ unknowns, where $p$ is the number of unique elements in $X$. Go ahead an think of $m[1]$ as the function's value at the smallest level of $X_i$, $m[2]$ as the function's value at the second smallest level of $X_i$, etc. It should be pretty easy to work out what the right constraints are in terms of the vector $m$. From there, all you need to know is which position in $m$ corresponds to each *observation* $(X_i,Y_i)$, so you can use the right value of $m$ in the loss function. The `inverse` you get from `invert.unique(X)` should help you with that.\n",
    "\n",
    "All that said, while it's useful to be able to do little programming tasks like this, that's not really what this class is supposed to be about. If you're having trouble with this or you don't feel motivated, go ahead and skip it. \n",
    "It's not going to be something you'll need later on in class and I'll give you code that does all this in the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Extending Your Solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Clearly there are functions that are increasing on the sample that\n",
    "aren't, in fact, increasing. Draw one on this sample.\n",
    "\n",
    "```{r}\n",
    "set.seed(1)\n",
    "x=seq(.15,.85,by=.1)\n",
    "y=x+rnorm(x)\n",
    "ggplot() + geom_point(aes(x=x, y=y)) + xlab('') + ylab('')\n",
    "```\n",
    "\n",
    "However, given any function $m$ that is increasing on the sample, there\n",
    "is an everywhere increasing function that agrees with it *on the\n",
    "sample*. In fact, there are infinitely many. Draw one.\n",
    "\n",
    "The first curve you drew probably wasn't a solution to the optimization\n",
    "problem above. That's not something I can just draw, anyway. I need the\n",
    "code we're writing in this lab to do it. But if you had drawn a solution\n",
    "to the optimization problem above, then drawn an increasing curve that\n",
    "agrees with it on the sample, you'd be set. Since the optimization\n",
    "problem above depends only on the values of $m$ on the sample, that\n",
    "increasing curve would be another solution to that problem. And because\n",
    "it's an increasing solution to our increasing-on-the-sample least\n",
    "squares problem, it's a solution to the outright-increasing least\n",
    "squares problem we set out to solve.\n",
    "\n",
    "That means we can solve the increasing least squares problem in two\n",
    "steps. First, we'll find a solution $\\hat\\mu_{sample}$ to the\n",
    "increasing-on-the-sample problem. Then, we'll draw an increasing curve\n",
    "$\\hat \\mu$ that agrees with $\\hat\\mu_{sample}$ on the sample.\n",
    "\n",
    "Finding an Increasing Solution\n",
    "\n",
    "## Sketching\n",
    "\n",
    "Now we're going we're going to use our solution $\\hat\\mu_{sample}$ from\n",
    "the previous part to find a curve $\\hat \\mu$ that solves the increasing\n",
    "least squares problem. By this, we mean a function $\\hat\\mu$ that we can\n",
    "evaluate at any $x \\in \\mathbb{R}$. Then we'll plot the curve\n",
    "$\\hat\\mu(x)$ for $x \\in [0,1]$. This will connect the points you plotted\n",
    "in the previous part.\n",
    "\n",
    "In Section \\@ref(breaking-the-problem-down), we argued that any\n",
    "increasing curve $\\hat \\mu$ that goes through the points\n",
    "$\\{(X_i,\\hat\\mu_{sample}(X_i) \\}$ solves the increasing least squares\n",
    "problem. So all we need is code that takes an increasing point-set, i.e.\n",
    "a set $\\{(X_i, Y_i) : i\\le n\\}$ for which $Y_i \\le Y_j$ whenever\n",
    "$X_i \\le X_j$, and evaluates $m(x)$ for an increasing curve $m$ that\n",
    "goes through these points.\n",
    "\n",
    "We did this by hand in Section \\@ref(breaking-the-problem-down). Now we\n",
    "need to come up with an algorithm that does it. Writing code than runs\n",
    "on computers is hard, so to ease into this, let's write code that runs\n",
    "on your classmates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down an algorithm for drawing an increasing curve that goes\n",
    "through an increasing point-set $\\{(X_i, Y_i) : i \\le n\\}$. You don't\n",
    "have to write pseudocode. Think of it as giving instructions two\n",
    "different people could follow to draw the curve on top of a plot of the\n",
    "points $\\{(X_i, Y_i)\\}$. Make sure that your curve extends past the\n",
    "edges of your data. We want our increasing curve defined for all $x$ on\n",
    "the real line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a test, use your algorithm to draw a curve through the following two\n",
    "plots of an increasing point-set $\\{(X_i, Y_i)\\}$. Then give your\n",
    "algorithm to a classmate and ask them to use it to do the same. If there\n",
    "are any differences between what you draw and they do, one of two things\n",
    "went wrong. Either your algorithm was too vague to have a well-defined\n",
    "outcome or somebody made an error following it. Figure out which and\n",
    "eliminate any vagueness. Soon you'll be implementing this on a computer,\n",
    "which can't handle vague code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x=seq(.05,.95,by=.1)\n",
    "\n",
    "y=1-cos(pi*x/2)\n",
    "ggplot(data.frame(x=x,y=y), aes(x=x, y=y)) + geom_point() + ylim(0,1)\n",
    "\n",
    "y=((1+x)/2)*(.3 + .5*(x >= .5) + .2*(x >= .8))\n",
    "ggplot(data.frame(x=x,y=y), aes(x=x, y=y)) + geom_point() + ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Implementation\n",
    "\n",
    "Once you've got that working, implement a `predict` function like\n",
    "`predict.linearreg` to fill in the gaps. Then add the curve to your plot\n",
    "from the last exercise.\n",
    "\n",
    "**Tips.**\n",
    "\n",
    "1.  It's likely that your algorithm for drawing an increasing curve\n",
    "    through a point-set $\\{(X_i, Y_i) : i \\le n\\}$ involves knowing the\n",
    "    closest point $X_i$ to the left of each $x \\in \\mathbb{R}$, or to\n",
    "    the right of it, or both. To make this easier, I recommend you sort\n",
    "    everything so $X_1 \\le X_2 \\le \\ldots \\le X_n$ throughout.\n",
    "\n",
    "2.  When making your predictions, you can use the built-in `R` function\n",
    "    `findInterval` to find the index $i$ of the closest point to the\n",
    "    left of $x$. In particular, if $X$ is the sorted vector\n",
    "    $[X_1 X_2 \\ldots X_n]$ and $x$ is a vector of query points,\n",
    "    `findInterval(X,x)` will return a vector $i$ such that $X_{i_k}$ is\n",
    "    the closest point to the left of $x_k$. Naturally $X_{i_k+1}$ will\n",
    "    be the closest point to the right of $x_k$. You may have to handle\n",
    "    some edge cases. If there is no observation $X_i$ left of $x_k$,\n",
    "    then the entry $i_k$ will be zero; if there is no observation\n",
    "    $X_{i}$ to the right of it, $i_{k}+1$ will be $n+1$.\n",
    "\n",
    "Here's a template. To help you out, I'll give you code to put your\n",
    "observations $X_i$ and corresponding predictions $\\hat\\mu(X_i)$ in\n",
    "sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution-template"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predict.monotonereg = function(model, newdata=data.frame(X=model$X)) {\n",
    "  # Get X and mu.hat in sorted order\n",
    "  increasing.order = order(model$X) \n",
    "  X = model$X[increasing.order]\n",
    "  mu.hat = model$mu.hat[increasing.order]\n",
    "  \n",
    "  # Do some stuff to compute predictions for each observation in newdata$X\n",
    "  \n",
    "  # Return these predictions as a vector\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented this predict method, let's use it to plot a curve.\n",
    "We'll add this curve to the plot of our observations and our predictions\n",
    "$\\hat\\mu(X_i)$ that we made in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x = seq(0,1,by=.001)\n",
    "muhat.x = predict(model, newdata=data.frame(X=x))\n",
    "fit.on.sample + geom_line(aes(x=x, y=muhat.x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Application\n",
    "\n",
    "Now let's see what this tells us about the effect of class sizes on 5th\n",
    "graders' test scores in Israel. Almost all of the code in this section\n",
    "is all written for you. Your job is interpreting the results.\n",
    "\n",
    "We'll start by grabbing some data from my website and plotting it.\n",
    "\n",
    "```{r load-israel-schools-data}\n",
    "school=read.csv('https://davidahirshberg.bitbucket.io/data/israel-schools-enrollment-model-1200-points.csv')\n",
    "ggplot() + geom_point(aes(x=enrollment, y=score), alpha=.2, data=school)\n",
    "```\n",
    "\n",
    "Recall from our first lecture that class sizes are capped at 40, so\n",
    "there are $x$ students per class in schools with enrollment $x \\le 40$,\n",
    "an average of $x/2$ in schools with enrollment $x \\in [41,80]$, an\n",
    "average of $x/3$ in schools with enrollment $x \\in [81,120]$, and so on.\n",
    "To keep things simple, we'll just be comparing schools with enrollment\n",
    "$x \\le 80$. We'd expect test scores to *decrease* as a function of class\n",
    "size and therefore, if we stick to one of the ranges $[1,40]$ and\n",
    "$[41,80]$, with enrollment. That means we'll need to alter our code\n",
    "above so we can fit decreasing curves as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the function `monotonereg` that you've written above and\n",
    "generalize it so that it fits a decreasing curve rather than an\n",
    "increasing one if passed the argument `decreasing=TRUE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's our estimate, $\\hat \\mu_{right}(40) - \\hat \\mu_{left}(40)$, of\n",
    "the effect if decreasing classes from size $40$ to size $40/2=20$. Since\n",
    "I'm not giving you monotone regression code, I'll use linear regression.\n",
    "You'll change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "linearreg = function(X,Y) { lm(Y~X) }\n",
    "\n",
    "estimate.effect = function(fit.model) { \n",
    "  data.left  = school[school$enrollment <= 40, ]\n",
    "  data.right = school[40 < school$enrollment & school$enrollment <= 80, ]\n",
    "\n",
    "  model.left  = fit.model(data.left$enrollment,   data.left$score)\n",
    "  model.right = fit.model(data.right$enrollment,  data.right$score)\n",
    "\n",
    "  effect.estimate = predict(model.right, newdata = data.frame(X=40)) - \n",
    "                    predict(model.left, newdata  = data.frame(X=40))\n",
    "  \n",
    "  list(model.left  = model.left, \n",
    "       model.right = model.right, \n",
    "       data.left   = data.left,\n",
    "       data.right  = data.right,\n",
    "       estimate=effect.estimate)\n",
    "}\n",
    "\n",
    "estimate = estimate.effect(linearreg)\n",
    "estimate$estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "And let's plot our fits $\\hat\\mu_{left}$ and $\\hat\\mu_{right}$ over the\n",
    "data to get a sense of what our estimate is based on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot.effect.estimate = function(estimate) { \n",
    "  x.left  = seq(0,40, by=.1)\n",
    "  x.right = seq(40,80,by=.1)\n",
    "  plot.left  = data.frame(enrollment=x.left,  \n",
    "                          score=predict(estimate$model.left,  newdata=data.frame(X=x.left)))\n",
    "  plot.right = data.frame(enrollment=x.right, \n",
    "                          score=predict(estimate$model.right, newdata=data.frame(X=x.right)))\n",
    "  ggplot() + geom_point(aes(x=enrollment, y=score), alpha=.2, \n",
    "                        data=rbind(estimate$data.left, estimate$data.right)) +\n",
    "\t           geom_line(aes(x=enrollment,  y=score), color='blue',  data=plot.left) +\n",
    "\t           geom_line(aes(x=enrollment,  y=score), color='blue',  data=plot.right) +\n",
    "\t           geom_vline(aes(xintercept=40)) \n",
    "}\n",
    "plot.effect.estimate(estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you've got your monotone regression code code written, use it to estimate to treatment effect. Then illustrate what it's based on by plotting it as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the effect estimates we get using linear regression and\n",
    "decreasing regression. Which estimate are you inclined to trust? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's check that we're getting the same answer we did before.\n",
    "Because we're using computer arithmetic, which is a bit imprecise, you\n",
    "shouldn't expect exact equality of the vectors\n",
    "$\\hat\\mu(X_1) \\ldots \\hat\\mu(X_n)$ you get with these different\n",
    "implementations, but they should be close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "That speaks to a more general approach we can use for any model.\n",
    "\n",
    "1. Work out the *set of functions on the sample*, $\\mathcal{X}|_{\\mathcal{X}}$, that agree with functions in the model.\n",
    "   $$ \\mathcal{M}|_{\\mathcal{X}} = \\{ m|_{\\mathcal{X}} : m \\in \\mathcal{M} \\} $$\n",
    "   Because there are only finitely many observations, this is a finite-dimensional set. Often, we can optimize over it using CVXR.\n",
    "2. Extend your solution to a function that's actually in the model however you like.\n",
    "    - This can be pretty mechanical, but it is possible to screw it up. Let's look at how.\n",
    "agree with functions in the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# A Variation\n",
    "\n",
    "If you have extra time, check out [nearly-monotone\n",
    "regression](https://www.stat.cmu.edu/~ryantibs/papers/neariso.pdf).[^1]\n",
    "The nearly-monotone model adds a bit of slack so you can fit curves that\n",
    "aren't monotone, but are close. This is a version for nearly-increasing\n",
    "curves. The parameter $B$, which is up to you, controls how much slack\n",
    "is allowed. \n",
    "\n",
    "$$\n",
    "\\hat \\mu = \\operatorname*{argmin}_{\\substack{m \\\\ \\sum_{i=1}^{n-1} \\{m(X_{i})-m(X_{i+1})\\}_+ \\le B}} \\frac{1}{n}\\sum_{i=1}^n \\left\\{Y_i - m(X_i)\\right\\}^2 \n",
    "\\quad \\text{ where } \\quad \\{x\\}_+ = \\begin{cases} x &\\text{if} \\  x \\ge 0 \\\\ 0 &\\text{ if } x < 0. \\end{cases}. \n",
    "$$ \n",
    "\n",
    "You should be able to implement this with a very small change to your `CVXR` code.\n",
    "\n",
    "[^1]: It's called nearly-isotonic regression there. Monotone and isotonic are synonyms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
