{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Review of Robinson's Decomposition and the R-Learner\n",
    "\n",
    "The starting point for the [R-Learner](https://arxiv.org/abs/1712.04912)\n",
    "is *Robinson's decomposition* of the treatment-specific conditional mean\n",
    "outcome , $\\mu(w,x) = E[Y_i \\mid W_i=w, X=x]$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "\\mu(W_i, X_i) &= \\beta(X_i) + \\{W_i-\\pi(X_i)\\} \\tau(X_i) && \\text{ where } \\\\\n",
    "\\beta(X_i)   &= E[Y_i \\mid X_i], && \\\\\n",
    "\\pi(X_i) &= P(W_i=1 \\mid X_i), && \\text{ and } \\\\ \n",
    "\\tau(X_i) &= E[ Y_i \\mid W_i=1, X_i] - E[Y_i \\mid W_i=0, X_i]. &&\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The use of this decomposition goes back at least to Robinson's\n",
    "[root-n-consistent semiparametric regression (Econometrica\n",
    "1988)](https://econpapers.repec.org/article/ecmemetrp/v_3a56_3ay_3a1988_3ai_3a4_3ap_3a931-54.htm).\n",
    "We derived it in class last time.\n",
    "\n",
    "## The R-Learner Idea\n",
    "\n",
    "Imagine, for a moment, that you know $\\pi(x)$ and $\\beta(x)$. Using\n",
    "Robinson's decomposition, we see that this implies that if we know\n",
    "$\\tau(x)$, then we know $\\mu(w,x)$ and vice-versa. \n",
    "\n",
    "- It's often the case that you know $\\pi$, as if you're an experimenter randomizing treatment,\n",
    "it's part of your experimental design.\n",
    "- It's essentially never the case that you know $\\beta(x)$, but just go with it for now. \n",
    "\n",
    "Because $\\tau(x)=\\mu(1,x)-\\mu(0,x)$, estimating it has got to boil down to some\n",
    "way of estimating $\\mu$, and that's what the R-Learner does. But to do\n",
    "it, it uses a clever model for $\\mu$ that takes advantage of our\n",
    "knowledge of $\\pi$ and $\\beta$. In particular, because we'd know $\\mu$\n",
    "if we knew $\\tau$ (in addition to $\\pi$ and $\\beta$), we use a model\n",
    "that expresses $\\mu(w,x)$ as a function of $\\tau(x)$ and known\n",
    "quantities. Given a model $\\mathcal{M}_\\tau$ meant to contain $\\tau$,\n",
    "this is what our model for $\\mu$ looks like.\n",
    "$$ \n",
    "\\mathcal{M}_\\mu = \\{ m_t(w,x) : t \\in \\mathcal{M}_\\tau \\} \\quad \\text{ where } \\quad  m_t(w,x) = \\beta(x) + \\{w-\\pi(x)\\}t(x). \n",
    "$$\n",
    "Note that $\\mu(w,x)=m_{\\tau}(w,x)$, i.e., $\\mu$ is the function we get when we plug in $t=\\tau$ to our formula for $m_t$. \n",
    "\n",
    "To estimate $\\tau$, we'll fit a\n",
    "curve curve to our observations $Y_i$ by least squares over this model\n",
    "$\\mathcal{M}_\\mu$. \n",
    "$$\n",
    "\\hat\\mu = \\operatorname*{argmin}_{\\substack{m \\in \\mathcal{M}_\\mu}} \\frac{1}{n}\\sum_{i=1}^n \\left\\{ m(W_i,X_i) - Y_i \\right\\}^2.\n",
    "$$\n",
    "The resulting estimate $\\hat\\tau$ of $\\tau$ will be the curve for which $\\hat\\mu=m_{\\hat\\tau}$. Or equivalently, plugging in our formula for $m_t$, \n",
    "$$\n",
    "\\hat\\tau = \\operatorname*{argmin}_{\\substack{t \\in \\mathcal{M}_\\tau}} \\frac{1}{n}\\sum_{i=1}^n \\left[ \\beta(X_i) + \\{W_i-\\pi(X_i)\\}t(X_i) - Y_i \\right]^2.\n",
    "$$\n",
    "\n",
    "### The Oracle Estimator\n",
    "\n",
    "Before we move on, let's give a name to this estimator of $\\tau$. It's\n",
    "not an estimator we can use, as it requires knowledge (of $\\pi$ and\n",
    "$\\beta$) that we don't have, but it's also not our estimation\n",
    "target either. It's the estimator we'd use if someone, who is\n",
    "conventionally called an *oracle*, told us what $\\beta$ and $\\pi$ were. \n",
    "It's common practice to call estimators like this *oracle\n",
    "estimators*. We'll write $\\hat\\tau_{\\star}$ for this oracle estimator\n",
    "from now on.\n",
    "\n",
    "Thinking about this oracle estimator lets us break down the \n",
    "question of how accurate the estimator $\\hat\\tau$ into two parts.\n",
    "\n",
    "1. The question of how close the estimator we actually use is to the oracle estimator.\n",
    "2. The question of how close the oracle estimator is to the actual treatment effect.\n",
    "\n",
    "That amounts to thinking of our estimator $\\hat\\tau$'s error as a sum of two terms.\n",
    "\n",
    "$$\n",
    "\\hat\\tau - \\tau = \\left\\{\\hat\\tau - \\hat\\tau_{\\star}\\right\\} + \\left\\{\\hat\\tau_{\\star} - \\tau\\right\\}.\n",
    "$$ \n",
    "\n",
    "\n",
    "People often talk about conditions under which the first term is much smaller than the second, \n",
    "so we can think of the actual and oracle estimators as being essentially the same. It turns \n",
    "out that you don't need to estimate $\\beta$ or $\\pi$ particularly well for this to happen, \n",
    "which people call the *quasi-oracle property* of the R-Learner. \n",
    "\n",
    "Working with fake data, we'll actually calculate all three terms in the decomposition above to see where\n",
    "the R-Learner's error comes from and looks like. \n",
    "\n",
    "## Using the R-Learner\n",
    "\n",
    "To use this in practice, we'll take a two stage approach. \n",
    "\n",
    "### First Stage\n",
    "\n",
    "We'll estimate $\\beta$ and $\\pi$.^[If we know $\\pi$, we'll just use it: $\\hat\\pi=\\pi$.] Each of these is, itself, a least squares regression problem.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat\\beta &= \\operatorname*{argmin}_{b \\in \\mathcal{M}_\\beta} \\frac{1}{\\tilde n}\\sum_{i=1}^n \\{ b(\\tilde X_i) - \\tilde Y_i \\}^2 && \\text{ estimates } \\quad \\beta(x) = E[Y_i \\mid X_i=x] \\\\\n",
    "    \\hat\\pi &=   \\operatorname*{argmin}_{p \\in \\mathcal{M}_\\pi} \\frac{1}{\\tilde n}\\sum_{i=1}^n \\{ p(\\tilde X_i) - \\tilde W_i \\}^2 && \\text{ estimates } \\quad \\pi(x) = E[W_i \\mid X_i=x] = P(W_i=1 \\mid X_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here $\\mathcal{M}_\\beta$ and $\\mathcal{M}_\\pi$ are models meant\n",
    "to contain $\\beta$ and $\\pi$ respectively. \n",
    "\n",
    "To simplify our analysis, we'll use different samples from the same distribution for each stage. We can just use roughly half of our data for each.\n",
    "$$\n",
    "\\begin{aligned}\n",
    " &\\{(\\tilde W_i, \\tilde X_i, \\tilde Y_i) : i \\in 1 \\ldots \\tilde n \\}  && \\text{ is our first stage sample} \\\\\n",
    " &\\{( W_i, X_i, Y_i) : i \\in 1 \\ldots n \\} && \\text{ is our second stage sample}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "[^1]: When we've actually randomized treatment, we should know the\n",
    "    conditional treatment probability $\\pi$, so we'll use the 'perfect\n",
    "    estimate' $\\hat\\pi=\\pi$ in that case. In observational studies, we\n",
    "    often make the hopeful assumption that treatment was selected solely\n",
    "    based on the covariate $X_i$, which justifies analyzing the data as\n",
    "    if it came from an experiment but we do not to know $\\pi$ so we have\n",
    "    to estimate it.\n",
    "\n",
    "### Second Stage\n",
    "\n",
    "We'll plug our estimates $\\hat\\beta$ and $\\hat\\pi$ into our decomposition of $\\mu$ to get a special model,\n",
    "$$\n",
    "\\mathcal{M_\\mu} = \\{ m_t(w,x) : t \\in \\mathcal{M}_{\\tau} \\} \\quad \\text{ where } \\quad m_t(w,x) = \\hat \\beta(x) + \\{w - \\hat \\pi(x)\\}t(x).\n",
    "$$ \n",
    "Then we'll estimate $\\hat \\mu$, or equivalently $\\hat \\tau$, by least squares as usual. \n",
    "\n",
    "$$ \\hat \\tau = \\operatorname*{argmin}_{t \\in \\mathcal{\\mathcal{M}_\\tau}}\\frac{1}{n}\\sum_{i=1}^n\\left[\\hat\\beta(X_i) + \\{W_i-\\hat\\pi(X_i)\\}t(X_i) - Y_i \\right]^2. $$\n",
    "\n",
    "We can use whatever model we want for $\\mathcal{M}_\\tau$. \n",
    "\n",
    "  - Today, we'll focus on a simple version, in which we model the treatment effect $\\tau(x)$ to be constant: $\\mathcal{M}_\\tau = \\{ t(x) = c : c \\in \\mathbb{R}\\}$. \n",
    "  - Next time, we'll do a fancier version, in which we use the bounded variation model $\\mathcal{M}_\\tau = \\{ t(x) : \\rho_{TV}(t) \\le B \\}$.\n",
    "\n",
    "## Data \n",
    "\n",
    "Throughout, we'll be thinking about the National Supported Work (NSW)\n",
    "program we discussed in the last lecture. To get a sense of how things\n",
    "are working, we'll start with the fake data we were using in class. But \n",
    "once we've looked into the estimator's behavior, we'll apply it to the real stuff, too. \n",
    "\n",
    "## Notation\n",
    "\n",
    "Throughout, we'll use the following notation.\n",
    "\n",
    "1.  We'll use $w$ to denote the value of a *binary-valued treatment and*\n",
    "    $W_i$ for the specific value received by participant $i$. Typically\n",
    "    in controlled experiments, we'll let $W_i = 1$ if participant $i$\n",
    "    receives the real treatment and $W_i=0$ if they receive the control\n",
    "    treatment.\n",
    "2.  We'll use $x$ to denote the value of a covariate and $X_i$ the\n",
    "    specific value associated with participant \\$i\\$. In our NSW data,\n",
    "    $X_i$ will be income in 1974, a year that precedes treatment.\n",
    "3.  We'll use $y$ to denote the value of an outcome and $Y_i$ the\n",
    "    specific value associated with participant \\$i\\$. In our NSW data,\n",
    "    $Y_i$ will be income in 1978, a year that follows treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Before we get into the lab, let's set up some code we'll be using. In the three blocks below, we'll do the following.\n",
    "\n",
    "1. We'll import CVXR and the `tidyverse` libraries and do a little configuration. \n",
    "2. We'll copy some convenient little functions we've used in previous labs: the functions `invert.unique` and `prediction.function`.^[We'll modify `prediction.function` so that if you do give it a function, it just returns it. Otherwise, it assumes you're giving it a model object and returns a function thatuses `predict` to get predictions.This makes it easier to plug in $\\pi$ for $\\hat\\pi$ if we happen to know it, or plug in both $\\pi$ and $\\beta$ when we're talking about the oracle estimator. ]\n",
    "3. We'll copy our monotone regression and bounded variation regression code from previous labs, too. \n",
    "  - We'll use the fast version of monotone regression from the convergence rates lab. \n",
    "  - We'll use `bvreg` and `predict.bvreg` from the bounded variation lab. And also our code for automatically selecting the variation budget, `select.budget` and `bvselected`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "})\n",
    "\n",
    "# OSQP claims some feasible problems aren't, so we'll tell CVXR not to use it\n",
    "CVXR::add_to_solver_blacklist('OSQP')  \n",
    "\n",
    "# And we'll style our plots   \n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t\t\t\t\tpanel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\tpanel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t      panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t    axis.ticks.x = element_blank(),\n",
    "\t\t    axis.ticks.y = element_blank(),\n",
    "\t\t    axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t    axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "prediction.function = function(model) { \n",
    "  if(class(model)=='function') { model }\n",
    "  else{ function(x) { as.numeric(predict(model, newdata=data.frame(X=x))) } }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "monotonereg = function(X,Y, decreasing=FALSE) {\n",
    "  input = list(X=X,Y=Y)\n",
    "\n",
    "  isoreg.model = if(decreasing) { isoreg(X,-Y) } else { isoreg(X,Y) }\n",
    "  mu.hat = array(dim=length(X))\n",
    "  mu.hat[isoreg.model$ord] = isoreg.model$yf * if(decreasing) { -1 } else { 1 }\n",
    "  \n",
    "  unique.X = invert.unique(X) \n",
    "  model = list(X=X[unique.X$elements], mu.hat=mu.hat[unique.X$elements], input=input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "bvreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list( sum(abs(diff(m))) <= B )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.bvreg \n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"bvreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.piecewise.constant = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  y = model$mu.hat; X=model$X; x=newdata$X\n",
    "  # for each new data point x in newdata$X, \n",
    "  # find the closest observed X[k] left of x\n",
    "  # i.e., the largest k for which X[k] <= x \n",
    "  # this covers cases 1 and 2\n",
    "  # i will be a vector of these numbers k, with one for each x in newdata$X\n",
    "  i = findInterval(x, X) \n",
    "  # if there is no X[k] < x, findInterval tells us k=0\n",
    "  # to cover case 3, we want X[k] for k=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  y[i]\n",
    "}\n",
    "predict.bvreg = predict.piecewise.constant\n",
    "predict.monotonereg = predict.piecewise.constant\n",
    "\n",
    "select.budget = function(X,Y,Bs) {\n",
    "  set.seed(1)\n",
    "  train = sample(1:length(X)) <= length(X)/2 \n",
    "  Bs = c(.1,.25,.5,1,2,4,10)\n",
    "  test.errors = Bs |> map(\\(B) {\n",
    "    muhat = bvreg(X[train], Y[train], B=B) |> prediction.function()\n",
    "    mean((Y[!train] - muhat(X[!train]))^2)\n",
    "  })\n",
    "  Bs[which.min(test.errors)]\n",
    "}\n",
    "\n",
    "bvselected = function(Bs) {\n",
    "  function(X,Y) { \n",
    "    B = select.budget(X,Y,Bs)\n",
    "    bvreg(X,Y,B=B)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Parametric R-Learner\n",
    "\n",
    "One of the nice things about using parametric models is that it's often\n",
    "pretty straightforward to get explicit characterizations of our\n",
    "estimator. This one is no exception. The least squares estimator\n",
    "$\\hat \\tau$ in the model with constant treatment effect satisfies the\n",
    "zero-derivative condition \n",
    "$$ \n",
    "\\begin{aligned}\n",
    "0 &= \\frac{d}{dt}\\bigg|_{t=\\hat\\tau} \\frac{1}{2n}\\sum_{i=1}^{n} \\left[ \\hat \\beta(X_i) + \\{ W_i - \\hat \\pi(X_i) \\}t - Y_i \\right]^2 \\\\\n",
    "  &= \\frac{1}{n}\\sum_{i=1}^n \\left[ \\hat \\beta(X_i) + \\{ W_i - \\hat \\pi(X_i) \\}\\hat\\tau - Y_i \\right]\\{W_i - \\hat\\pi(X_i)\\} \\\\\n",
    "  &= \\frac{1}{n} \\sum_{i=1}^n \\{\\hat \\beta(X_i) - Y_i\\}\\{ W_i - \\hat \\pi(X_i) \\} + \\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\hat \\pi(X_i)\\}^2 \\hat\\tau. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By a little subtraction and division, we can derive an explicit formula\n",
    "for our estimate $\\hat\\tau$.\n",
    "\n",
    "$$ \n",
    "\\begin{aligned} \n",
    "\\hat \\tau &= \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{ Y_i - \\hat \\beta(X_i) \\} \\{ W_i - \\hat \\pi(X_i) \\}}{\\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\hat \\pi(X_i)\\}^2}. \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## The Behavior of the Oracle Estimator\n",
    "\n",
    "To make sense of what this does, let's start with the oracle estimator. Let's plug in $\\hat\\beta=\\beta$ and $\\hat\\pi=\\pi$. \n",
    "And think about what $Y_i$ is in terms of all this stuff. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y_i &= \\mu(W_i,X_i) + \\varepsilon_i \\quad \n",
    "&& \\text{ where } \\quad E[\\varepsilon_i \\mid W_i, X_i] = 0 \\\\\n",
    "&=\\beta(X_i) + \\{W_i-\\pi(X_i)\\}\\tau(X_i) + \\varepsilon_i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Plugging these into the formula for $\\hat\\tau$ we derived above, we get a nice formula for the oracle estimator $\\hat\\tau_{\\star}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat\\tau_{\\star}\n",
    "&= \\frac{\\frac{1}{n} \\sum_{i=1}^n \\left[ \\beta(X_i) + \\{W_i-\\pi(X_i)\\}\\tau(X_i) + \\varepsilon_i - \\beta(X_i) \\right] \\{ W_i - \\pi(X_i) \\}}{\\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2} \\\\\n",
    "&= \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{W_i-\\pi(X_i)\\}^2 \\ \\tau(X_i) }{\\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2} \n",
    "+  \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{W_i-\\pi(X_i)\\} \\varepsilon_i }{\\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "1. Our first term is a weighted average of the conditional average treatment effect $\\tau(X_i)$ with weights $\\alpha_i = \\{W_i-\\pi(X_i)\\}^2/\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2$. \n",
    "2. Our second term is a weighted average of the noise $\\varepsilon_i$. \n",
    "\n",
    "::: {.callout-exercise}\n",
    "Suppose our treatment effect were actually constant, so $\\tau(X_i)=\\tau$ for all $i$. \n",
    "What would the mean (expected value) of the oracle estimator $\\hat\\tau_{\\star}$ be?\n",
    "\n",
    "I'll give a proof in the solution, but no need for that here.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "::: {.callout-solution}\n",
    "If $\\tau(X_i)$ were $\\tau$ for all $i$, then our oracle estimator's mean would be $\\tau$.^[That's true as long as $E[\\varepsilon_i \\mid (W_1,X_1) \\ldots (W_n,X_n)]=0$, which happens in particular if $(W_1,X_1,Y_1) \\ldots (W_n,X_n,Y_n)$ are independent and $E[\\varepsilon_i \\mid W_i,X_i]=0$.]  Let's prove it. \n",
    "\n",
    "Let's start with the first term in our decomposition of $\\hat\\tau_{\\star}$, which is a weighted average of the conditional average treatment effect $\\tau(X_i)$. If $\\tau(X_i)$ were $\\tau$ for all $i$, that would just be $\\tau$.\n",
    "$$\n",
    "\\sum_i \\alpha_i \\tau(X_i) = \\sum_i \\alpha_i \\tau = \\tau \\sum_i \\alpha_i = \\tau. \n",
    "$$\n",
    "\n",
    "The second term has mean zero.  To prove it, we'll show that it has mean zero conditional on $(W_1,X_1),...,(W_n,X_n)$. That implies it has unconditional mean zero, as by the law of iterated expectations, $E[Z] = E[E[Z | W]]$ is zero if $E[Z \\mid W] = 0$.\n",
    "How do we show it has conditional mean zero? It basically boils down to linearity of conditional expectations. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "&E\\left[ \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{W_i-\\pi(X_i)\\} \\ \\varepsilon_i}{ \\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2} \\mid (W_1,X_1),...,(W_n,X_n) \\right] \\\\\n",
    "&= \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{W_i-\\pi(X_i)\\} E\\left[ \\varepsilon_i \\mid (W_1,X_1),...,(W_n,X_n)\\right]}{ \\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2 } \\\\\n",
    "&= \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{W_i-\\pi(X_i)\\} \\ 0}{ \\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\pi(X_i)\\}^2} \\\\\n",
    "&= 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty straightforward to calculate the standard deviation of the oracle estimator given that constancy assumption, too. \n",
    "Assuming that $(W_1,X_1,Y_1),...,(W_n,X_n,Y_n)$ are independent and identically distributed, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Var[\\hat\\tau_{\\star}] \n",
    "&= \\frac{1}{n} \\times E\\left[ \\{W_i - \\pi(X_i)\\}^2 \\ \\sigma^2(W_i,X_i) \\right] \n",
    "&& \\text{ for } \\ \\sigma^2(w,x)=Var[Y_i \\mid W_i=w,X_i=x] \\\\\n",
    "&\\approx \\frac{\\sigma^2}{n} \\times E \\ Var[W_i \\mid X_i] \n",
    "&& \\text{ if } \\sigma^2(w,x) \\approx \\sigma \\ \\text{ for all } \\ w,x \n",
    "\\end{aligned}\n",
    "$$\n",
    "If $\\tau(X_i)$ *isn't* constant, then its variance is at least this large.^[This isn't hard to show using the law of total variance conditioning on $(W_1,X_1) \\ldots (W_n,X_n)$. This formula is the expected value of the conditional variance. The total variance is the sum of this and the variance of the conditional mean, which is zero when $\\tau(x)$ is constant, but otherwise positive.]\n",
    "\n",
    "One important conclusion we can draw is that the standard deviation of this estimator is on the order of $1/\\sqrt{n}$. Like a sample average's variance. I'll spare you the details of the calculations, but it's not a bad exercise to do it yourself if you want practice working with expected values. \n",
    "\n",
    "Let's take a look at this oracle estimator's sampling distribution. We'll generate 10,000 replications of the data at sample size $n=50$ and $n=200$ and plot histograms of the resulting estimates. We can see that the distributions are both centered around the true value of $\\tau$ and that the width of the distribution basically halves when we quadruple the sample size. That's what happens when you have an unbiased estimator with standard deviation proportional to $1/\\sqrt{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pi = function(x) { .5 }\n",
    "beta = function(x) { x }\n",
    "tau  = function(x) { 1 }\n",
    "\n",
    "oracle.estimates = function(n, reps=10000) { \n",
    "  1:reps |> map_vec(\\(ii) {\n",
    "    X = runif(n)\n",
    "    W = rbinom(n,1, pi(X))\n",
    "    epsilon = rnorm(n,0,.2)\n",
    "    Y = beta(X) + (W - pi(X))*tau(X) + epsilon\n",
    "  \n",
    "    tau.hat = mean( (Y - beta(X)) * (W - pi(X)) ) / mean( (W - pi(X))^2 )\n",
    "    tau.hat\n",
    "  })\n",
    "}\n",
    "\n",
    "estimates.50 = oracle.estimates(50)\n",
    "estimates.200 = oracle.estimates(200)\n",
    "\n",
    "ggplot() + geom_histogram(aes(x=estimates.50, y=after_stat(density)), bins=100, \n",
    "                          alpha=.5, fill='blue', color='black', linewidth=.05) +\n",
    "           geom_histogram(aes(x=estimates.200, y=after_stat(density)), bins=100, \n",
    "                          alpha=.5, fill='magenta', color='black', linewidth=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "To emphasize that our oracle estimator's standard deviation is some constant times $1/\\sqrt{n}$, which'll be important later, let's calculate the standard deviations our our 10,000 draws at both sample sizes and multiply by $\\sqrt{n}$. We'll get basically the same number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sd(estimates.50) * sqrt(50)\n",
    "sd(estimates.200) * sqrt(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Happens When the Effect is Not Constant?\n",
    "\n",
    "Generally, the mean of the oracle estimator is roughly this *treatment-variance-weighted* average of the conditional average treatment effect.^[This is what we get when we take the expectation of the numerator and denominator in our decomposition of $\\hat\\tau_{\\star}$, then divide. That's not the same as dividing and taking the expectation, but when the the numerator and denominator are close to their means (which the law of large numbers tells us they should be), it's a good approximation. You can use Taylor expansion to get a better approximation or bound the approximation error. [This homework exercise](https://qtm285-1.github.io/assets/homework/week5-homework.html#exr-ratio-bias) from another class I teach will talk you through it.]\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "E[\\hat\\tau_{\\star}] &\\approx \\frac{E\\left[ \\alpha(X_i) \\tau(X_i) \\right]}{ E\\left[ \\alpha(X_i) \\right]} \n",
    "&& \\text{where } \\alpha(X_i) = E[\\{W_i-\\pi(X_i)\\}^2 \\mid X_i] = Var[W_i \\mid X_i].\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It's an average of the conditional average treatment effect $\\tau(X_i)$\n",
    "that prioritizes units which, in our sample, could plausibly have\n",
    "received either treatment $W_i=1$ or $W_i=0$.\n",
    "Some people call this the [Overlap-Weighted Treatment\n",
    "Effect](https://academic.oup.com/aje/article/188/1/250/5090958). It is\n",
    "easier to estimate than the usual unweighted average treatment effect,\n",
    "in the sense that it's possible to do it with lower variance, because it\n",
    "downweights comparisons of potential outcomes that we rarely observe.\n",
    "\n",
    "If you're curious, go ahead and change the code in the cell above so $\\tau(X_i)$ is not constant and see what happens.\n",
    "\n",
    "If you want to get the unweighted average treatment effect, you can \n",
    "use *weighted least squares regression* in Stage 2. Working out the right weights isn't a bad \n",
    "exercise.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "Optional. Suppose we want to use an approach like this to estimate the\n",
    "unweighted average treatment effect. How can we do it?\n",
    "\n",
    "Hint: Find a formula for $\\hat\\tau_{\\star}$ when you use *weighted least squares regression* in Stage 2 with\n",
    "some arbitrary weights $w(W_i,X_i)$ and work out what choice of $w$ cancels out the $\\alpha$ s. \n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Parametric R-Learner\n",
    "\n",
    "Let's take a look at how the actual estimator behaves. Here's code for generating fake\n",
    "data as in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pi.equal   = function(x) { rep(.5, length(x)) }\n",
    "pi.unequal = function(x) { .2 + .6*(1-x) }\n",
    "mu = function(w,x) { w*(1+sqrt(x))/2  + (1-w)*(1+x)/2 }\n",
    "fake.nsw.data = function(pi, n=200) {\n",
    "      X = runif(n)\n",
    "      epsilon = runif(n,min=-1,max=1)*.2\n",
    "      W = rbinom(n,1, pi(X))\n",
    "      Y = mu(W,X) + epsilon\n",
    "      data.frame(X=X,W=W,Y=Y)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's use our formula for the parametric R-Learner to estimate the\n",
    "overlap-weighted average treatment effect.\n",
    "\n",
    "$$\n",
    "\\hat \\tau = \\frac{\\frac{1}{n} \\sum_{i=1}^n \\{\\hat \\beta(X_i) - Y_i\\}\\{ W_i - \\hat \\pi(X_i) \\}}{\\frac{1}{n}\\sum_{i=1}^n \\{W_i - \\hat \\pi(X_i)\\}^2}. \n",
    "$$\n",
    "\n",
    "We'll use monotone variation regression to estimate $\\beta$ and compare\n",
    "our estimate to the corresponding oracle. As in lecture, we'll use the actual value of $\\pi$, like we would in an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pi = pi.equal\n",
    "beta = function(x) { pi(x)*mu(1,x) + (1-pi(x))*mu(0,x) }\n",
    "\n",
    "set.seed(0)\n",
    "data = fake.nsw.data(pi, n=400)\n",
    "W = data$W\n",
    "Y = data$Y\n",
    "X = data$X\n",
    "\n",
    "beta.hat =  prediction.function(monotonereg(X, Y))\n",
    "pi.hat = prediction.function(monotonereg(X, W)) \n",
    "\n",
    "tau.hat         = mean( (Y - beta.hat(X)) * (W - pi.hat(X)) ) / mean( (W - pi.hat(X))^2 )\n",
    "tau.hat.oracle  = mean( (Y - beta(X))     * (W - pi(X)) ) / mean( (W - pi(X))^2 )\n",
    "\n",
    "tau.hat\n",
    "tau.hat.oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are pretty similar. \n",
    "\n",
    "Let's do a visual comparison of the two estimators. We'll plot the\n",
    "corresponding curves $\\hat\\mu(w,x) = \\hat\\beta(x) + \\{w-\\hat\\pi(x)\\}\\hat\\tau$ and\n",
    "$\\hat\\mu_{\\star}(w,x) = \\beta(x) + \\{w-\\pi(x)\\}\\tau_{\\star}$\n",
    "on top of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu.hat = function(w,x) { beta.hat(x) + (w-pi(x))*tau.hat }\n",
    "mu.hat.oracle = function(w,x) { beta(x) + (w-pi(x))*tau.hat.oracle }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "grid = expand.grid(w=c(0,1), x=seq(0,1,by=.001))\n",
    "data.plot = ggplot(data) + \n",
    "  geom_point(aes(x=X, y=Y, color=factor(W)), alpha=.3) + \n",
    "  geom_line(aes(x=x, y=mu(w,x), color=factor(w), group=w), alpha=.3, data=grid) + \n",
    "  geom_line(aes(x=x, y=beta(x)), alpha=.3, data=grid)\n",
    "\n",
    "data.plot + \n",
    "  geom_line(aes(x=x, y=mu.hat(w,x), color=factor(w)), data=grid, linetype='dashed') + \n",
    "  geom_line(aes(x=x, y=beta.hat(x)), data=grid,  linetype='dashed')\n",
    "\n",
    "data.plot + \n",
    "  geom_line(aes(x=x, y=mu.hat.oracle(w,x), color=factor(w)), data=grid,  linetype='dashed') +\n",
    "  geom_line(aes(x=x, y=beta(x)), data=grid, linetype='dashed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Comparing to the Oracle\n",
    "\n",
    "### The Rate of Convergence\n",
    "Now let's get a sense of how close our estimate is to the oracle. We'll\n",
    "plot the absolute difference between the two, $\\lvert \\hat\\tau - \\tau_\\star \\rvert$, \n",
    "as a function of sample size. \n",
    "\n",
    "Because this is random, we'll do it 40 times. \n",
    "\n",
    "  - Each time, we'll generate a new dataset of size $n=3200$ and estimate $\\hat\\tau$ and $\\tau_\\star$\n",
    "    at sample sizes 100,200,...,3200. We'll see these 40 error curves as thin 'spaghetti'. \n",
    "  - We'll also plot the average of these 40 error curves as a thicker black line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "reps = 40\n",
    "ns = c(100,200,400,800,1600,3200)\n",
    "set.seed(0)\n",
    "estimates =1:reps |> map_dfr(.id='rep', \\(rr) {\n",
    "  all.data = fake.nsw.data(pi, n=max(ns))\n",
    "  ns |> map_dfr(\\(n) {\n",
    "    data = all.data[1:n,]\n",
    "    W = data$W\n",
    "    Y = data$Y\n",
    "    X = data$X\n",
    "    \n",
    "    beta.hat = prediction.function(monotonereg(X, Y))\n",
    "    pi.hat = pi\n",
    "    tau.hat = mean( (Y - beta.hat(X)) * (W - pi.hat(X)) ) / mean( (W - pi.hat(X))^2 )\n",
    "    tau.hat.oracle = mean( (Y - beta(X))     * (W - pi(X)) ) / mean( (W - pi(X))^2 )\n",
    "    \n",
    "    data.frame( estimator = tau.hat, oracle = tau.hat.oracle, n=n)\n",
    "  })\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "convergence.plot = \n",
    "  ggplot(estimates) + \n",
    "    geom_line(aes(x=n,    y=abs(estimator-oracle), color=rep, group=rep), linewidth=.5, alpha=.5) + \n",
    "    stat_summary(aes(x=n, y=abs(estimator-oracle)), fun.data=mean_se, geom='pointrange', linewidth=.5) + \n",
    "    stat_summary(aes(x=n, y=abs(estimator-oracle)), fun=mean, geom='line') + guides(color='none')\n",
    "convergence.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize this curve, we'll estimate a rate of convergence like \n",
    "we did in our convergence rates lab. We'll use `nls` to fit a model like this.\n",
    "\n",
    "$$\n",
    "\\lvert \\hat\\tau - \\tau_\\star \\rvert \\approx \\hat\\alpha n^{-\\hat\\rho} \\quad \\text{ for some } \\hat\\alpha \\text{ and } \\hat\\rho.\n",
    "$$\n",
    "\n",
    "We'll take a look at the rate and, to check that it makes sense, plot what it predicts, i.e. $\\hat\\alpha n^{-\\hat\\rho}$, on top of the actual error curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "convergence.model = nls(abs(estimator-oracle) ~ a * n^(-r),\n",
    "                        data=estimates,\n",
    "                        start=list(a=1, r=1/2))\n",
    "convergence.model \n",
    "\n",
    "error.predictions = data.frame(n=ns) \n",
    "error.predictions$error.prediction = predict(convergence.model, newdata=error.predictions)\n",
    "convergence.plot + \n",
    "  geom_line(aes(x=n, y=error.prediction), data=error.predictions, \n",
    "            color='blue', linewidth=2, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n^{-\\hat\\rho}$ is our estimate of the rate of convergence of $\\hat\\tau$ to $\\tau_\\star$.\n",
    "Note that we're converging to the oracle at a rate **faster than** $n^{-1/2}$, which is the rate at which the oracle estimator\n",
    "converges to **its limit**. This tells us the difference between our\n",
    "estimate and the oracle is asymptotically negligible in the sense that\n",
    "the actual estimate converges to the oracle estimator's limit\n",
    "essentially just as fast as the oracle estimator itself does. Let's take\n",
    "a look. We'll use the value of the oracle estimator at a large sample\n",
    "size as a proxy for its limit. In the plot below, we see...\n",
    "\n",
    "-   dashed lines showing the estimator at multiple sample sizes\n",
    "-   solid lines showing the oracle estimator at the same sample sizes\n",
    "-   a green horizontal line showing the oracle estimator's limit\n",
    "\n",
    "We do show this, using lines of different colors, for a few different replications of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "data = fake.nsw.data(pi, n=100000)\n",
    "W = data$W\n",
    "X = data$X \n",
    "Y = data$Y\n",
    "\n",
    "tau.hat.oracle.limit  = mean( (Y - beta(X)) * (W - pi(X)) ) / mean( (W - pi(X))^2 )\n",
    "\n",
    "ggplot(estimates[estimates$rep %in% 1:3,]) + \n",
    "           geom_line(aes(x=n, y=estimator, color=rep, group=rep), linetype='dashed') +  \n",
    "           geom_point(aes(x=n, y=estimator, color=rep, group=rep)) +  \n",
    "           geom_line(aes(x=n, y=oracle, color=rep, group=rep)) +\n",
    "           geom_point(aes(x=n, y=oracle, color=rep, group=rep)) +  \n",
    "           geom_segment(aes(x=n, xend=n, y=estimator, yend=oracle, color=rep, group=rep), alpha=.2) + \n",
    "           geom_hline(aes(yintercept=tau.hat.oracle.limit), color='green', linewidth=2, alpha=.5) + \n",
    "           guides(color='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional Implications\n",
    "\n",
    "What does this equivalence mean for the *sampling distribution* of our estimator?\n",
    "Let's take a look. We'll use 10,000 replications of our data at sample size n=400\n",
    "to histogram the sampling distributions of the oracle estimator (blue) and our actual estimator (red).\n",
    "The green line shows the oracle estimator's limit. What do you see?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 400\n",
    "reps = 10000\n",
    "\n",
    "tau.hat = rep(NA, reps)\n",
    "tau.hat.oracle = rep(NA, reps)\n",
    "\n",
    "set.seed(0)\n",
    "for(ii in 1:reps) {\n",
    "  data = fake.nsw.data(pi, n=n)\n",
    "  W = data$W\n",
    "  Y = data$Y\n",
    "  X = data$X\n",
    "\n",
    "  beta.hat = prediction.function(monotonereg(X, Y))\n",
    "  pi.hat   = pi\n",
    "  tau.hat[ii]         = mean( (Y - beta.hat(X)) * (W - pi.hat(X)) ) / mean( (W - pi.hat(X))^2 )\n",
    "  tau.hat.oracle[ii]  = mean( (Y - beta(X))     * (W - pi(X)) )     / mean( (W - pi(X))^2 )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "nbins = 100\n",
    "histogram.comparison = ggplot() + \n",
    "           geom_histogram(aes(x=tau.hat, y=after_stat(density)), bins=nbins, alpha=.5, fill='red', color='black', linewidth=.05) + \n",
    "           geom_histogram(aes(x=tau.hat.oracle, y=after_stat(density)), bins=nbins, alpha=.5, fill='blue', color='black', linewidth=.05) + \n",
    "           geom_vline(aes(xintercept=tau.hat.oracle.limit), color='green', linewidth=2, alpha=.5)\n",
    "histogram.comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're pretty similar, right?  To see why, let's look at the actual *differences* between the two estimators in each sample.\n",
    "Here are two visualizations. \n",
    "\n",
    "1. We plot two little dots for each sample, one red and one blue, showing the two estimators. With a little stick connecting them. \n",
    "2. We histogram the *differences* between the two estimators. \n",
    "\n",
    "I've plotted these on top of the sampling distributions above to give a sense of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "max.y = dnorm(0,sd=sd(tau.hat))\n",
    "ys = seq(0,max.y,length.out=length(tau.hat))\n",
    "\n",
    "histogram.comparison + \n",
    "  geom_point(aes(x=tau.hat, y=ys),alpha=.5, color='red', size=.1) + \n",
    "  geom_point(aes(x=tau.hat.oracle, y=ys), alpha=.5, color='blue', size=.1) + \n",
    "  geom_segment(aes(x=tau.hat, xend=tau.hat.oracle, y=ys, yend=ys), alpha=.2, linewidth=.1)\n",
    "\n",
    "histogram.comparison +  geom_histogram(aes(x=tau.hat-tau.hat.oracle, y=after_stat(density)), \n",
    "                          bins=nbins, alpha=.5, fill='purple', color='black', linewidth=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The R-Learner in Observational Studies \n",
    "\n",
    "In observational studies, we don't know $\\pi$, so we have to estimate\n",
    "it. We can do this by fitting a model for $\\pi$ to our data and using\n",
    "the resulting estimate $\\hat\\pi$ in place of $\\pi$ in our estimator.\n",
    "Here's a theoretical claim. It's presented as an exercise, but it's a\n",
    "hard one if you're not used to thinking about these things.\n",
    "I'll be happy to talk you through it during office hours, but you might\n",
    "want to skip it for now.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "*Optional*. Show that the difference between $\\hat\\tau$ and\n",
    "$\\hat\\tau_{\\star}$ is negligible if the errors $\\hat \\pi - \\pi$\n",
    "and $\\hat \\beta - \\beta$ converge to zero at the rates $n^{-\\rho_\\pi}$\n",
    "and $n^{-\\rho_\\beta}$ respectively if \n",
    "\n",
    "1. $\\rho_\\beta > 0$\n",
    "2. $\\rho_\\pi > 1/4$\n",
    "3. $\\rho_\\beta + \\rho_\\pi > 1/2$.\n",
    ":::\n",
    "\n",
    "[^2]: You may assume that this means\n",
    "    $\\lVert \\hat \\pi - \\pi \\rVert \\le a_\\pi n^{-r_\\pi}$ for whatever\n",
    "    norm you find convenient and similarly for $\\hat\\beta-\\beta$.\n",
    "\n",
    "*Tip.* To show this, I suggest again starting by expanding $\\hat \\tau$\n",
    "around $\\hat\\tau_{\\star}$. This isn't quite as easy as the case\n",
    "in which $\\pi$ is known, as unlike $\\hat\\beta$, $\\hat\\pi$ occurs in the\n",
    "denominator of of $\\hat\\tau$ as well as in the numerator. To work around\n",
    "this straightforwardly, you can think of our estimators as functions $f$\n",
    "of the vectors $\\beta(X)=\\beta(X_1)\\ldots\\beta(X_n)$ and\n",
    "$\\pi(X)=\\pi(X_1)\\ldots \\pi(X_n)$, and look at the first order Taylor\n",
    "series expansion of $\\hat\\tau = f(\\hat\\beta(X), \\hat\\pi(X))$ around\n",
    "$\\hat\\tau_{\\star} = f(\\beta(X), \\pi(X))$ and that\n",
    "expansion's remainder.\n",
    "\n",
    "That said, we don't need to know the theory to try it out. Here's\n",
    "another exercise.\n",
    "\n",
    "::: {.callout-exercise}\n",
    "*Optional*. Repeat the experiments above, but use an estimate $\\hat\\pi$ in\n",
    "place of $\\pi$ itself when calculating $\\hat\\tau$. You can estimate $\\pi$\n",
    "using monotone or bounded variation regression. If you like, repeat\n",
    "with $\\pi=\\text{pi.unequal}$ as well.\n",
    ":::\n",
    "\n",
    "### Implications {#implications}\n",
    "\n",
    "If you think about it, this is a very cool result. It says that you don't\n",
    "need particularly accurate estimates of $\\pi$ and $\\beta$ to get an accurate \n",
    "estimate of $\\tau$. In particular, if we use monotone or bounded variation regression to estimate $\\pi$ and $\\beta$, \n",
    "we'll get $n^{-1/3}$ rates of convergence for both $\\hat\\beta$ and $\\hat\\pi$, and that's good enough to get almost exactly the same estimate of $\\tau$ as if we knew $\\pi$ and $\\beta$ exactly. The practical take-away is that we don't \n",
    "need to consider fitting simpler models (e.g. lines) when estimating $\\pi$ and $\\beta;\n",
    "these offer no improvement when they're (approximately) correct, but can mess things \n",
    "up when they're not.\n",
    "\n",
    "Later in the semester, I'll sketch a partial generalization of this result\n",
    "that applies when we use a nonparametric model for $\\tau$ later in the\n",
    "semester. Nie and Wager call this the *Quasi-Oracle Property* of the\n",
    "R-Learner. In short, it says the rate of convergence of $\\hat\\tau(x)$ to\n",
    "$\\tau(x)$ will be the same as the rate of convergence we'd get doing\n",
    "least squares on direct observations\n",
    "$Y^\\tau_i = \\tau(X_i) + \\varepsilon_i$ of $\\tau$ as long as this is rate\n",
    "is slower than $n^{-(\\rho_\\pi+\\rho_\\beta)}$ and $n^{-2\\rho_\\pi}$. For example, if\n",
    "$\\hat\\beta$ and $\\hat\\pi$ converge at rates faster than $n^{-1/6}$, then\n",
    "if we use an R-Learner version of monotone regression we'll get the same\n",
    "$n^{-1/3}$ rate we'd get via least squares monotone regression on these\n",
    "direct observations. So far we don't have any methods for estimating\n",
    "$\\beta$ and $\\pi$ that converge this slow, but we will see this happen\n",
    "later in the semester when we start working with data in which $X_i$ is\n",
    "higher-dimensional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Nonparametric R-Learner\n",
    "\n",
    "Now let's implement a version that uses a nonparametric model for\n",
    "$\\tau(x)$ in a model that actually allows it to vary with $x$. We'll use the \n",
    "bounded variation model.\n",
    "\n",
    "$$ \\hat \\tau = \\operatorname*{argmin}_{\\substack{t \\\\ \\rho_{TV}(t) \\le B}} \\frac{1}{n}\\sum_{i=1}^n\\left[\\hat\\beta(X_i) + \\{W_i-\\hat\\pi(X_i)\\}t(X_i) - Y_i \\right]^2. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-exercise}\n",
    "Modify the code in the block below so it fits a bounded variation model for $\\tau$ using\n",
    "the R-Learner. \n",
    "\n",
    "To test your code, run the block after it to fit it to some simulated data \n",
    "and compare it to $\\tau$ itself.\n",
    ":::\n",
    "\n",
    "Here's a template. I've handled the boilerplate stuff for you, e.g. naming and saving of\n",
    "variables, but all the code does as-is is fit a model to predict Y from X. What do you need to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "bvrlearner = function(W, X, Y, pi.hat, beta.hat, B=1) {\n",
    "  # check that the inputs make sense\n",
    "  stopifnot(length(W) == length(X)) \n",
    "  stopifnot(length(X) == length(Y)) \n",
    "  input = list(W=W, X=X, Y=Y, pi.hat=pi.hat, beta.hat=beta.hat)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "   \n",
    "  # We tell CVXR we're thinking about a vector of unknowns, t, in R^p. \n",
    "  #   - t will have one element for each unique level of X.\n",
    "  #   - t[i] will correspond to the ith smallest element of X. \n",
    "  # We use invert.unique to duplicate and shuffle these into a vector of unknowns in correspondence with Y_1 ... Y_n.\n",
    "  t = Variable(length(unique.X$elements))\n",
    "  t.WX = t[unique.X$inverse]\n",
    "  \n",
    "  # And we tell it how to predict Y from W and X. \n",
    "  # in terms of our placeholder t for the unknown vector tau(X1)...tau(Xn)\n",
    "  # Here's where you come in. This does something, but isn't right. Change it.\n",
    "  m.WX = beta.hat(X) + (W - pi.hat(X))*t.WX\n",
    "\n",
    "  # We tell CVXR that we're interested in mean squared error and specify the constraints.\n",
    "  mse = sum((Y - m.WX)^2 / n)\n",
    "  constraints = list( sum(abs(diff(t))) <= B )\n",
    "\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector tau.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  tau.hat = solved$getValue(t)\n",
    "\n",
    "  # A  little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and tau.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.bvrlearner \n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], tau.hat = tau.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"bvrlearner\"\n",
    "  model\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "pi.equal   = function(x) { rep(.5, length(x)) }\n",
    "pi.unequal = function(x) { .2 + .6*(1-x) }\n",
    "mu = function(w,x) { w*(1+sqrt(x))/2  + (1-w)*(1+x)/2 }\n",
    "fake.nsw.data = function(pi, n=200) {\n",
    "      X = runif(n)\n",
    "      epsilon = runif(n,min=-1,max=1)*.2\n",
    "      W = rbinom(n,1, pi(X))\n",
    "      Y = mu(W,X) + epsilon\n",
    "      data.frame(X=X,W=W,Y=Y)\n",
    "}\n",
    "\n",
    "pi = pi.equal\n",
    "beta = function(x) { pi(x)*mu(1,x) + (1-pi(x))*mu(0,x) }\n",
    "tau = function(x) { mu(1,x) - mu(0,x) }\n",
    "data = fake.nsw.data(pi, n=200) \n",
    "\n",
    "beta.hat = prediction.function( monotonereg(data$X, data$Y) )\n",
    "pi.hat = prediction.function( monotonereg(data$X, data$W) )\n",
    "\n",
    "tau.model = bvrlearner(data$W, data$X, data$Y, pi.hat, beta.hat, B=.2)\n",
    "ggplot() + geom_point(aes(x=tau.model$X, y=tau.model$tau.hat), color='red') + \n",
    "           geom_point(aes(x=tau.model$X, y=tau(tau.model$X))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions using the R-Learner\n",
    "\n",
    "We can use piecewise-constant extension to calculate $\\hat\\tau(x)$ at new points $x$. \n",
    "Here's a function that does it using our old friend `predict.piecewise.constant`.\n",
    "That function expects a model with a `mu.hat` field, so we'll rename `tau.hat` to `mu.hat` before we call it.\n",
    "\n",
    "It takes an argument 'type' to specify whether you want a prediction \n",
    "of the response (i.e. an estimate of $\\mu(w,x)$) or the effect (i.e. an estimate of $\\tau(x)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "predict.bvrlearner = function(model, newdata=data.frame(W=model$input$W, X=model$input$X), type='response') {\n",
    "  model$mu.hat = model$tau.hat\n",
    "  tau.hat = predict.piecewise.constant(model, newdata)\n",
    "  if(type == 'effect') { \n",
    "    tau.hat \n",
    "  } else { \n",
    "    beta.hat = model$input$beta.hat\n",
    "    pi.hat = model$input$pi.hat\n",
    "    beta.hat(newdata$X) + (newdata$W-pi.hat(newdata$X))*tau.hat\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's calculate the real and oracle estimates and do a visual\n",
    "comparison. We'll plot the curves\n",
    "$\\hat\\mu(w,x) = \\hat\\beta(x) + \\{w-\\pi(x)\\}\\hat\\tau(x)$ and\n",
    "$\\hat\\mu_{\\star}(w,x) = \\beta(x) + \\{w-\\pi(x)\\}\\hat\\tau_{\\star}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "data = fake.nsw.data(pi, n=800)\n",
    "beta.hat = prediction.function( monotonereg(data$X, data$Y) )\n",
    "pi.hat = prediction.function( monotonereg(data$X, data$W) )\n",
    "\n",
    "tau.model = bvrlearner(data$W, data$X, data$Y, pi.hat, beta.hat, B=1/2)\n",
    "tau.model.oracle = bvrlearner(data$W, data$X, data$Y, pi, beta, B=1/2)\n",
    "\n",
    "mu.hat = function(w,x) { predict(tau.model, newdata=data.frame(W=w, X=x), type='response') }\n",
    "mu.hat.oracle = function(w,x) { predict(tau.model.oracle, newdata=data.frame(W=w, X=x), type='response') }\n",
    "tau.hat = function(x) { predict(tau.model, newdata=data.frame(X=x), type='effect') }\n",
    "tau.hat.oracle = function(x) { predict(tau.model.oracle, newdata=data.frame(X=x), type='effect') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the two treatment effect estimators. I'll plot the\n",
    "oracle as a solid line and the actual one as a dashed line. They're very\n",
    "close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "xx = seq(0,1,by=.01)\n",
    "ggplot() + geom_line(aes(x=xx, y=tau.hat.oracle(xx)), color='blue')  +\n",
    "           geom_line(aes(x=xx, y=tau.hat(xx)), color='red')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what this looks like in the context of the data, we\n",
    "can look at the corresponding treatment specific means $\\hat\\mu(w,x)$.\n",
    "Here's what those looks like. We'll plot $\\hat\\beta(x)$ with the actual\n",
    "estimates and $\\beta(x)$ with the oracle ones, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "grid = expand.grid(w=c(0,1), x=seq(0,1,by=.001))\n",
    "data.plot = ggplot(data) + \n",
    "  geom_point(aes(x=X, y=Y, color=factor(W)), alpha=.3) + \n",
    "  geom_line(aes(x=x, y=mu(w,x), color=factor(w), group=w), alpha=.3, data=grid, linewidth=1) +\n",
    "  geom_line(aes(x=x, y=beta(x)), data=grid, alpha=.5) \n",
    "\n",
    "data.plot + \n",
    "  geom_line(aes(x=x, y=beta.hat(x)), data=grid, alpha=.5) + \n",
    "  geom_line(aes(x=x, y=mu.hat(w,x), color=factor(w)), data=grid) \n",
    "\n",
    "data.plot + \n",
    "  geom_line(aes(x=x, y=beta(x)), data=grid, alpha=.3, linewidth=1) + \n",
    "  geom_line(aes(x=x, y=mu.hat.oracle(w,x), color=factor(w)), data=grid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence to the Oracle\n",
    "\n",
    "Now let's get a sense of how close our estimate is to the oracle. We'll\n",
    "plot the root-mean squared distance\n",
    "$\\lVert \\hat \\tau - \\hat \\tau_{\\star} \\rVert_{L_2(P)}$ as a\n",
    "function of sample size. And, for reference, the same for the difference\n",
    "$\\lVert \\hat\\tau_{\\star} - \\lim_{n \\to \\infty} \\hat\\tau_{\\star}\\rVert_{L_2(P)}$\n",
    "between the oracle and its limit. Or, due to computational restrictions,\n",
    "a 'limit' that's actually just the value of the oracle fit to a\n",
    "large-ish sample independent of the one we're using elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(2)\n",
    "B = 1/2 \n",
    "\n",
    "data = fake.nsw.data(pi, n=3200)\n",
    "tau.model.oracle.limit = bvrlearner(data$W, data$X, data$Y, pi, beta, B=B)\n",
    "tau.hat.oracle.limit = function(x) { predict(tau.model.oracle.limit, newdata=data.frame(X=x), type='effect') }\n",
    "\n",
    "ns = c(200,400,800,1600,3200)\n",
    "set.seed(0)\n",
    "all.data = fake.nsw.data(pi, n=max(ns))\n",
    "xx = seq(0,1,by=.01)\n",
    "\n",
    "rms.difference = rep(NA, length(ns))\n",
    "rms.oracle.error = rep(NA, length(ns))\n",
    "for(ii in 1:length(ns)) {\n",
    "  n = ns[ii] \n",
    "  data = all.data[1:n,]\n",
    "  beta.hat = prediction.function( monotonereg(data$X, data$Y) )\n",
    "  pi.hat = prediction.function( monotonereg(data$X, data$W) )\n",
    "\n",
    "  tau.model = bvrlearner(data$W, data$X, data$Y, pi.hat, beta.hat, B=B)\n",
    "  tau.model.oracle = bvrlearner(data$W, data$X, data$Y, pi, beta, B=B)\n",
    "\n",
    "  tau.hat = function(x) { predict(tau.model, newdata=data.frame(X=x), type='effect') }\n",
    "  tau.hat.oracle = function(x) { predict(tau.model.oracle, newdata=data.frame(X=x), type='effect') }\n",
    "\n",
    "  rms.difference[ii] = sqrt(mean((tau.hat(xx) - tau.hat.oracle(xx))^2))\n",
    "  rms.oracle.error[ii] = sqrt(mean((tau.hat.oracle(xx) - tau.hat.oracle.limit(xx))^2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here's a comparison of the RMS actual-oracle difference (dashed) and the\n",
    "RMS oracle-'limit' difference (solid). What see is what the theory predicts. \n",
    "The difference between $\\hat\\tau$ and $\\hat\\tau_{\\star}$ is much smaller than the difference between $\\hat\\tau_{\\star}$ and its limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "convergence.plot.tv = \n",
    "  ggplot() + geom_line(aes(x=ns, y=rms.difference), linetype='dashed') + \n",
    "             geom_point(aes(x=ns, y=rms.difference)) + \n",
    "             geom_line(aes(x=ns, y=rms.oracle.error)) +\n",
    "             geom_point(aes(x=ns, y=rms.oracle.error)) + ylim(0,max(rms.oracle.error))\n",
    "convergence.plot.tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To summarize what's going on, we can approximate each by a multiple of a\n",
    "power of $n$. You know the drill. \n",
    "\n",
    "$$\n",
    "\\lVert \\text{ difference } \\rVert \\approx \\alpha n^{-\\rho} \\quad \\text{ for some } \\alpha \\text{ and } \\rho.\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "difference.model.tv = nls(abs(rms.difference) ~ a * ns^(-r),\n",
    "                        data=data.frame(ns=ns, rms.difference=rms.difference),\n",
    "                        start=list(a=1, r=1/2))\n",
    "difference.model.tv.predictions = predict(difference.model.tv, newdata=data.frame(ns=ns))\n",
    "\n",
    "convergence.model.tv = nls(abs(rms.oracle.error) ~ a * ns^(-r),\n",
    "                        data=data.frame(ns=ns, rms.oracle.error=rms.oracle.error),\n",
    "                        start=list(a=1, r=1/2))\n",
    "convergence.model.tv.predictions = predict(convergence.model.tv, newdata=data.frame(ns=ns))\n",
    "\n",
    "convergence.plot.tv + geom_line(aes(x=ns, y=convergence.model.tv.predictions), color='blue') +\n",
    "                      geom_line(aes(x=ns, y=difference.model.tv.predictions), color='blue', linetype='dashed')\n",
    "\n",
    "summary(difference.model.tv)\n",
    "summary(convergence.model.tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These approximations are pretty accurate. And one way of summarizing what they say is that,\n",
    "if we increase the sample size enough that the oracle estimator gets one digit of precision \n",
    "closer to its limit, then the R-Learner estimator gets two digits of precision closer to the oracle.\n",
    "This is essentially what the theory tell us to expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Application\n",
    "\n",
    "Let's use what we've got to estimate the effect of the National\n",
    "Supported Work (NSW) program on 1978 income as a function of 1974\n",
    "income.\n",
    "\n",
    "Let's think about how we estimate $\\pi$ and $\\beta$. \n",
    "  \n",
    "- This is a randomized experiment, so it should be the case that $\\pi(x)$ is constant. We can try to estimate that constant using `lm`. \n",
    "- You'd think higher income in '74 predicts higher income in '78, so $\\beta(x)$ should be increasing, so let's estimate $\\beta$ using monotone regression. \n",
    "\n",
    "We can see what happens when we change these, too.\n",
    "\n",
    "This data is hard to work with, so don't expect the results to feel great no matter what we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "nsw = read.csv('https://davidahirshberg.bitbucket.io/teaching/regression/fall2021/data/nsw.csv')\n",
    "nsw.experimental = nsw[nsw$dataset=='nsw', ]\n",
    "real.nsw.data =  data.frame(W=as.numeric(0+nsw.experimental$treated), \n",
    "                            X=as.numeric(nsw.experimental$income74), \n",
    "                            Y=as.numeric(nsw.experimental$income78))\n",
    "data = real.nsw.data\n",
    "data$Y = data$Y/max(data$X)\n",
    "data$X = data$X/max(data$X)\n",
    "beta.hat = prediction.function( monotonereg(data$X, data$Y) )\n",
    "pi.hat = prediction.function( lm(W ~ 1, data=data) )\n",
    "\n",
    "tau.model = bvrlearner(data$W, data$X, data$Y, pi.hat, beta.hat, B=1/2)\n",
    "\n",
    "mu.hat = function(w,x) { predict(tau.model, newdata=data.frame(W=w, X=x), type='response') }\n",
    "tau.hat = function(x) { predict(tau.model, newdata=data.frame(X=x), type='effect') }\n",
    "\n",
    "grid = expand.grid(w=c(0,1), x=seq(0,1,by=.001))\n",
    "ggplot(data) + \n",
    "  geom_point(aes(x=X, y=Y, color=factor(W)), alpha=.3) + \n",
    "  geom_line(aes(x=x, y=mu.hat(w,x), color=factor(w)), data=grid) + \n",
    "  guides(color='none') \n",
    "\n",
    "xx = seq(0,1,by=.001)\n",
    "ggplot() + geom_line(aes(x=xx, y=tau.hat(xx))) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
