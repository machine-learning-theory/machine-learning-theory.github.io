{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rates for Univariate Sobolev Regression\n",
    "\n",
    "Why would we prefer a model with a bounded second derivative over one with a bounded first derivative? For example,\n",
    "the $p=2$ version of this Sobolev model below over the $p=1$ version?\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{M}^p \n",
    "&= \\left\\{ m : \\lVert m^{(p)} \\rVert_{L_2} \\le B \\right\\} \\\\\n",
    "&=\\left\\{ \\sum_{j = 0}^\\infty m_j \\phi_j : \\sum_{j=0}^\\infty \\lambda_j^p m_j^2 \\le B^2 \\right\\} \\\\\n",
    "& \\text{ for } \\phi_{j}(x) = \\sqrt{2}\\cos(\\pi j x) \\ \\text{ and } \\ \\lambda_{j}=\\pi^2 j^2. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It overfits less. Let's compare the $p=1$ and $p=2$ models fit the data with the signal \n",
    "$\\mu(x)=\\sqrt{2}\\cos(\\pi x)$. Note that $\\mu=\\phi_1$, so it's in the $p=1$ model for $B \\ge \\lambda_1=\\pi^2$\n",
    "and the $p=2$ model for $B \\ge \\lambda_1^{2}=\\pi^4$. We'll use those values for $B$ below.\n",
    "\n",
    "You'll have to run a few code blocks, since we haven't yet loaded any packages or implemented sobolev regression.\n",
    "The next two blocks are copied from the sobolev regression lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "    CVXR::add_to_solver_blacklist('OSQP')  \n",
    "})\n",
    "\n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t  panel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\t  panel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t              panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t          axis.ticks.x = element_blank(),\n",
    "\t\t          axis.ticks.y = element_blank(),\n",
    "\t\t          axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sobolevreg = function(X,Y, p=1, B=1, epsilon=.001){\n",
    "  N = ceiling((1/pi)*(B/epsilon)^(1/p))\n",
    "  Phi = outer(X, 0:(N-1), function(x,j) cos(pi*j*x)) \n",
    "  lambda = pi^2 * (0:(N-1))^(2*p)\n",
    "  # specify the parameters and constraints.\n",
    "  b = Variable(N)\n",
    "  m = Phi %*% b\n",
    "  mse = sum((Y - m)^2)/length(Y) \n",
    "  constraints = list(sum(lambda * b^2) <= B^2)\n",
    "\n",
    "  # solve and extract the solution\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  beta.hat = solved$getValue(b)\n",
    "  \n",
    "  # package up the results in a model object and return it\n",
    "  model = list(beta.hat=beta.hat, p=p, input=list(X=X, Y=Y)) \n",
    "  attr(model, \"class\") = \"sobolevreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "predict.sobolevreg = function(model, newdata=model$input) { \n",
    "  x = newdata$X\n",
    "  beta.hat = model$beta.hat\n",
    "  N = length(beta.hat)\n",
    "  Phi.x = outer(x, 0:(N-1), function(x,j) cos(pi*j*x)) \n",
    "  \n",
    "  Phi.x %*% beta.hat \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "sigma = .2  \n",
    "\n",
    "mu = function(x) { sqrt(2)*cos(pi*x) } \n",
    "X = runif(n)\n",
    "Y = mu(X) + sigma*rnorm(length(X))\n",
    "\n",
    "fit.model.1 = \\(X,Y)sobolevreg(X,Y,p=1,B=pi^2)\n",
    "fit.model.2 = \\(X,Y)sobolevreg(X,Y,p=2,B=pi^4)\n",
    "\n",
    "x = seq(0,1,by=.01)\n",
    "newdata = data.frame(X=x)\n",
    "ggplot() + \n",
    "  geom_line(aes(x=x, y=mu(x))) + \n",
    "  geom_point(aes(x=X,y=Y), alpha=.1) + \n",
    "  geom_line(aes(x=x, y=predict(fit.model.1(X,Y), newdata)), color=\"blue\") + \n",
    "  geom_line(aes(x=x, y=predict(fit.model.2(X,Y), newdata)), color=\"red\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Even though we've chosen $B$ for each model so that $\\mu$ is just barely in it, i.e. we've chosen $B_1$\n",
    "and $B_2$ so that $\\lVert\\mu' \\rVert_{L_2}  = B_1$ and $\\lVert\\mu'' \\rVert_{L_2} = B_2$, the $p=2$ model is still\n",
    "giving us a better fit. The overall shape is the same, and it's not perfect, but it jumps around a lot less. \n",
    "\n",
    "To get a quantitative sense of the difference, let's vary sample size, plot some error curves, and estimate rates of convergence. We have code for that. It does take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ns = c(50,100,200,400,800)\n",
    "\n",
    "models  = list(p.is.1=fit.model.1,\n",
    "               p.is.2=fit.model.2)\n",
    "\n",
    "tabulate.errors.for.sample = function(X,Y,models,ns) { \n",
    "  ns |> map(function(n) {\n",
    "    models |> map(function(model.fit) {\n",
    "      model = model.fit(X[1:n], Y[1:n])\n",
    "      muhat = function(x) { predict(model, newdata=data.frame(X=x)) }\n",
    "      error = sqrt(mean((mu(X[1:n]) - muhat(X[1:n]))^2))\n",
    "      data.frame(n=n, error=error)\n",
    "    }) |> list_rbind(names_to='model')\n",
    "  }) |> list_rbind()\n",
    "}\n",
    "tabulate.errors = function(replications=10) { \n",
    "  1:replications |> map(function(rep) {\n",
    "    X           = runif(max(ns))\n",
    "    epsilon     = sigma*rnorm(length(X))\n",
    "    Y = mu(X) + epsilon\n",
    "    tabulate.errors.for.sample(X, Y, models, ns)\n",
    "  }) |> list_rbind(names_to='rep')\n",
    "}\n",
    "\n",
    "tab = tabulate.errors(replications=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Here are the error curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.curves = ggplot(tab, aes(x=n, y=error, color=model)) + \n",
    "    stat_summary(geom='line', fun=mean) +  \n",
    "    stat_summary(geom='pointrange', fun.data=mean_se,\n",
    "    \t\t position=position_dodge(5)) \n",
    "\n",
    "error.curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the rates for each model. They're $n^{-\\beta}$ where $\\beta$ is in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.model = function(group) { \n",
    "\treg.data = group |> group_by(n) |> summarize(error=mean(error))\n",
    "\tnls(formula = error ~ a*n^(-b), data=reg.data, \n",
    "\t    start=list(a=1, b=1/2))\n",
    "}\n",
    "\n",
    "rates = tab |> \n",
    "  group_by(model) |>\n",
    "\tgroup_modify(function(group,...) {\n",
    "\t    rate.model = error.model(group)\n",
    "\t\t  data.frame(alpha=coef(rate.model)[1], beta=coef(rate.model)[2])\n",
    "\t})\n",
    "\n",
    "rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, as evidence that the rates are meaningful, are the error curves this $\\alpha n^{-\\beta}$ model predicts overlaid on the actual error curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.predictions = tab |> group_by(model) |>\n",
    "\tgroup_modify(function(group,...) {\n",
    "\t\trate.model = error.model(group)\n",
    "\t\tdata.frame(n=ns, error = predict(rate.model, newdata=data.frame(n=ns)))\n",
    "\t})\n",
    "\n",
    "error.curves + \n",
    "    geom_line(aes(x=n, y=error, color=model), alpha=.4, linewidth=2, data=error.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rates for Bivariate Sobolev Regression\n",
    "\n",
    "A cube-root rate isn't bad, so maybe we don't need the $p=2$ model to get a good fit when $X$ is one-dimensional. \n",
    "Let's check out the bivariate case using the *isotropic* Sobolev model.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{M}^p \n",
    "&= \\left\\{ m \\ : \\ \\left\\langle -\\Delta^p m, \\ m \\right\\rangle_{L_2} \\le B \\right\\} \\\\\n",
    "&= \\left\\{ \\sum_{j \\in \\mathbb{N}^2} m_j \\phi_j  \\ : \\sum_{j \\in \\mathbb{N}^2} \\lambda_j^p \\ m_j^2  \\le B^2 \\right\\} \n",
    "&&\\text{ for } \\quad \\phi_j(x) =  \\cos(\\pi j_1 x_1) \\cos(\\pi j_2 x_2) \\\\ \n",
    "&&&\\text{ and } \\quad \\lambda_j = \\pi^2 \\lVert j \\rVert_2^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We'll use this model to fit noisy image data. To make the computation manageable, we'll use pretty small images.\n",
    "We'll make them $10 \\times 10$, $14\\times14$, and $20\\times20$ giving us sample sizes (number of pixes) of $100$, $196$, and $400$ respectively. We'll use two different signals:\n",
    "\n",
    "- A smooth bump: the bivariate normal density function.\n",
    "- A sharp checkerboard pattern.\n",
    "\n",
    "Here are what these images look like with and without added noise.\n",
    "We'll use the `gridExtra` package to plot the images side by side. You may need to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(gridExtra)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "image.from.function = function(f, noise.level=.2, px=10){\n",
    "  grid = expand.grid(x1=seq(0, 1, length.out = px), \n",
    "                     x2=seq(0, 1, length.out = px))\n",
    "  eps = noise.level*matrix(runif(px*px, -1, 1), nrow=px)  \n",
    "  matrix(1 + f(grid$x1, grid$x2) + eps, nrow=px, ncol=px)\n",
    "}\n",
    "bump = function(noise.level=.2, px=10) {\n",
    "  f = function(x1,x2) { dnorm(3*sqrt((x1-0.5)^2 + (x2-0.5)^2)) }\n",
    "  f |> image.from.function(noise.level, px)\n",
    "}\n",
    "checkerboard = function(noise.level=.2, px=10){\n",
    "  f = function(x1,x2) { as.numeric((x1 <= .5) == (x2 <= .5)) }\n",
    "  f |> image.from.function(noise.level, px)\n",
    "}\n",
    "\n",
    "image.to.df = function(image) {\n",
    "  grid = expand.grid(x1=seq(0, 1, length.out = nrow(image)), \n",
    "                     x2=seq(0, 1, length.out = ncol(image)))\n",
    "  grid$Y = as.vector(image)\n",
    "  grid\n",
    "}\n",
    "image.from.df = function(df, dim=sqrt(nrow(df))*c(1,1)) {\n",
    "  matrix(df$Y, nrow=dim[1], ncol=dim[2])\n",
    "}\n",
    "\n",
    "show.image = function(image) {\n",
    "  df = image.to.df(image)\n",
    "  ggplot(df, aes(x = x1, y = x2, fill = Y)) +\n",
    "    geom_raster() +\n",
    "    scale_fill_gradient(low = \"black\", high = \"white\") +\n",
    "    coord_equal() + guides(fill='none')\n",
    "}\n",
    "\n",
    "multires.plot = function(im) { \n",
    "  grid.arrange(im(noise.level=0,  px=10) |> show.image(),\n",
    "               im(noise.level=.2, px=10) |> show.image(),\n",
    "               im(noise.level=0,  px=14) |> show.image(),\n",
    "               im(noise.level=.2, px=14) |> show.image(),\n",
    "               im(noise.level=0,  px=20) |> show.image(),\n",
    "               im(noise.level=.2, px=20) |> show.image(),\n",
    "               ncol=2)\n",
    "}\n",
    "multires.plot(bump)\n",
    "multires.plot(checkerboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's some code to fit our Sobolev model to these images. Once we've got our basis functions $\\phi_j$ and the corresponding eigenvalues $\\lambda_j$, the code is the same as in the univariate case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cosine.basis.2d = function(p, B=1, epsilon=.1) {\n",
    "  N.1 = ceiling((B / epsilon)^(1/p) / pi) \n",
    "  frequencies   =  expand.grid(j1=0:N.1, j2=0:N.1)\n",
    "  j = cbind(frequencies$j1, frequencies$j2)\n",
    "  lambda = (pi^2 * (j[,1]^2 + j[,2]^2) )^p\n",
    "  Phi = function(data) {\n",
    "    phi = matrix(nrow=nrow(data), ncol=nrow(j))\n",
    "    for(ij in 1:nrow(j)) {\n",
    "\t    phi[,ij] = 2*cos(pi*j[ij,1]*data$x1)*cos(pi*j[ij,2]*data$x2)\n",
    "    }\n",
    "    phi\n",
    "  }\n",
    "  list(lambda=lambda, Phi=Phi)\n",
    "}\n",
    "\n",
    "sobolevreg2d = function(data, p=1, epsilon=.1, B=1,\n",
    "                        basis=cosine.basis.2d(p, B, epsilon)) {\n",
    "  lambda = basis$lambda\n",
    "  Phi = basis$Phi(data)\n",
    "  Y = data$Y\n",
    "\n",
    "  # specify the parameters and constraints.\n",
    "  b = Variable(length(lambda))\n",
    "  m = Phi %*% b\n",
    "  mse = sum((Y - m)^2)/length(Y) \n",
    "  constraints = list(sum(lambda * b^2) <= B^2)\n",
    "\n",
    "  # solve and extract the solution\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  beta.hat = solved$getValue(b)\n",
    "  \n",
    "  # package up the results in a model object and return it\n",
    "  model = list(beta.hat=beta.hat, basis=basis, input=data) \n",
    "  attr(model, \"class\") = \"sobolevreg2d\"\n",
    "  model\n",
    "}\n",
    "\n",
    "predict.sobolevreg2d = function(model, newdata=model$input) { \n",
    "  Phi.x = model$basis$Phi(newdata)\n",
    "  Phi.x %*% model$beta.hat \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll see what happens when we denoise the bump image using the $p=1$ and $p=2$ Sobolev models. In the plot below:\n",
    "\n",
    "  - The top left image is the signal $\\mu$\n",
    "  - The top right image is the image $\\mu + \\varepsilon$ that we're denoising.\n",
    "  - The bottom left image is the denoised image using the $p=1$ model.\n",
    "  - The bottom right image is the denoised image using the $p=2$ model.\n",
    "\n",
    "To get a fair comparison, we'll use cross-validation to choose the regularization parameter $B$ for each model.\n",
    "Both do pretty well, which is interesting given how little the noisy image looks like the signal to the naked eye.\n",
    "But the $p=2$ model does a bit better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "px = 20\n",
    "im = bump\n",
    "noisy.image = im(noise.level=.2, px=px)\n",
    "clean.image = im(noise.level=0, px=px) \n",
    "\n",
    "denoise.image = function(image, p=1, B=seq(.25,10,by=.25)) {\n",
    "  denoised = image.to.df(image)\n",
    "  \n",
    "  if(length(B) > 1) {\n",
    "    cv.errors = map_dbl(B, function(b) {\n",
    "      train = sample(1:nrow(denoised)) <= nrow(denoised)/2\n",
    "      model = sobolevreg2d(denoised[train,], p=p, B=b) \n",
    "      prediction = predict(model, denoised[!train,])\n",
    "      mean((prediction - denoised$Y[!train])^2)\n",
    "    })\n",
    "    best.B = B[which.min(cv.errors)]\n",
    "  } else {\n",
    "    best.B = B\n",
    "  }\n",
    "  model = sobolevreg2d(denoised, p=p, B=best.B) \n",
    "  denoised$Y = predict(model)\n",
    "  im = image.from.df(denoised)\n",
    "  \n",
    "  attr(im, 'B') = best.B\n",
    "  im\n",
    "}\n",
    "\n",
    "\n",
    "denoised.1 = noisy.image |> denoise.image(p=1)\n",
    "denoised.2 = noisy.image |> denoise.image(p=2) \n",
    "\n",
    "c(mean((denoised.1-clean.image)^2), mean((denoised.2-clean.image)^2))\n",
    "\n",
    "grid.arrange(clean.image |> show.image(),\n",
    "             noisy.image |> show.image(), \n",
    "             denoised.1 |> show.image(), \n",
    "             denoised.2 |> show.image(), \n",
    "             ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's just one signal. If you swap out the bump for the checkerboard, which is much less smooth, you'll see the $p=1$ model does better. To do that, change line 3 above from `im=bump` to `im=checkerboard`. And it's just one random draw from the distribution of noisy images with that signal. \n",
    "\n",
    "Let's plot the root-mean-square error of each estimator as a function of sample size, like we did in the univariate case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(0)\n",
    "pxs = c(10,14,20)\n",
    "models  = list(p.is.1=\\(im)denoise.image(im, p=1),\n",
    "               p.is.2=\\(im)denoise.image(im, p=2))\n",
    "signals = list(bump=bump, checkerboard=checkerboard)\n",
    "\n",
    "tabulate.errors.2d = function(replications=10) {\n",
    "    pxs |> map(function(px) {\n",
    "      signals |> map(function(im) { \n",
    "        clean.image = im(noise.level=0, px=px) \n",
    "        1:replications |> map(function(rep) {\n",
    "          noisy.image = im(noise.level=.2, px=px)\n",
    "          models |> map(function(denoise) {\n",
    "            denoised.image = denoise(noisy.image)\n",
    "            data.frame(n=px^2,\n",
    "                       B=attr(denoised.image, 'B'), \n",
    "                       error=mean((denoised.image - clean.image)^2))\n",
    "          }) |> list_rbind(names_to='model')\n",
    "        }) |> list_rbind(names_to='rep')\n",
    "      }) |> list_rbind(names_to='signal')\n",
    "    }) |> list_rbind()\n",
    "}\n",
    "\n",
    "tab.2d = tabulate.errors.2d(replications=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you make of these curves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "error.curves = ggplot(tab.2d, aes(x=n, y=error, color=model)) + \n",
    "    stat_summary(geom='line', fun=mean) +  \n",
    "    stat_summary(geom='pointrange', fun.data=mean_se,\n",
    "    \t\t position=position_dodge(5)) +\n",
    "    geom_line(aes(group=interaction(rep,model)), linewidth=.05) +\n",
    "    facet_wrap(.~signal, scales='free_y')\n",
    "error.curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plot of the average regularization parameter $B$ chosen by cross-validation.\n",
    "\n",
    "Why are they increasing with sample sizefor the checkerboard signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "B.curves = ggplot(tab.2d, aes(x=n, y=B, color=model)) + \n",
    "    stat_summary(geom='line', fun=mean) +  \n",
    "    stat_summary(geom='pointrange', fun.data=mean_se,\n",
    "    \t\t position=position_dodge(5)) +\n",
    "    geom_line(aes(group=interaction(rep,model)), linewidth=.05) +\n",
    "    facet_wrap(.~signal)\n",
    "B.curves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
