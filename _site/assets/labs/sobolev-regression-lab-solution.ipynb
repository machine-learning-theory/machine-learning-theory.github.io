{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "## Summary\n",
    "\n",
    "Today, we're going to implementing least squares regression subject to a\n",
    "*Sobolev smoothness* constraint. That is, we'll talk about this\n",
    "estimator.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\mu &= \\operatorname*{argmin}_{m \\in \\mathcal{M}^p} \\frac{1}{n}\\sum_{i=1}^n \\{ Y_i - m(X_i) \\}^2 \n",
    "&& \\text{ for } \\quad  \\mathcal{M}^p = \\left\\{ m : \\lVert m^{(p)} \\rVert_{L_2} \\le B \\right\\} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As usual, we'll use `CVXR` to fit our model and the `tidyverse` package for plotting and other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(CVXR)\n",
    "    CVXR::add_to_solver_blacklist('OSQP')  \n",
    "})\n",
    "\n",
    "## Styles\n",
    "\n",
    "lab.theme = theme(plot.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "\t\t    \t  panel.background = element_rect(fill = \"transparent\", colour = NA),\n",
    "                  legend.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.box.background = element_rect(fill=\"transparent\", colour = NA),\n",
    "                  legend.key = element_rect(fill=\"transparent\", colour = NA),\n",
    "\t\t\t\t  panel.grid.major=element_line(color=rgb(0,0,0,.05,   maxColorValue=1)),\n",
    "\t              panel.grid.minor=element_line(color=rgb(0,0,0,.02,   maxColorValue=1)),\n",
    "\t\t          axis.ticks.x = element_blank(),\n",
    "\t\t          axis.ticks.y = element_blank(),\n",
    "\t\t          axis.text.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.text.y  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.title.x  = element_text(colour = \"#aaaaaa\"),\n",
    "\t\t          axis.title.y  = element_text(colour = \"#aaaaaa\", angle=90))\n",
    "theme_set(lab.theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Fourier Series and Approximation \n",
    "\n",
    "Recall the Fourier series characterization of this model we worked out in the [Sobolev Regression lecture](https://machine-learning-theory.github.io/assets/lectures/sobolev-regression-lecture.pdf).\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\mathcal{M}^p &= \\left\\{ m(x) = \\sum_{j = 0}^\\infty m_j \\phi_j(x) : \\sum_{j=0}^\\infty \\lambda_j^p m_j^2 \\le B^2 \\right\\} \\\\\n",
    "& \\text{ for } \\phi_{j}(x) = \\sqrt{2}\\cos(\\pi j x) \\ \\text{ and } \\ \\lambda_{j}=\\pi^2 j^2. \n",
    "\\end{aligned}\n",
    "$$ \n",
    "There are a few computational tricks we can use to fit this model,\n",
    "but today we're going to use finite dimensional approximation.\n",
    "\n",
    "\n",
    "Consider a model that differs only in that we exclude terms for $j \\ge N$,\n",
    "\n",
    "$$ \\mathcal{M}^{p,N} = \\left\\{ m(x) = \\sum_{j = 0}^{N-1} m_j \\phi_j(x) : \\sum_{j=0}^{N-1} \\lambda_j^p m_j^2 \\le B^2 \\right\\}. \n",
    "$$\n",
    "\n",
    "This is what we're actually going to fit. To have confidence that it\n",
    "makes sense to fit the finite-dimensional model $\\mathcal{M}^{p,K}$ but\n",
    "still think as if we've fit the infinite-dimensional model\n",
    "$\\mathcal{M}^p$, we'll want to make sure that this model contains an accurate\n",
    "approximation to every curve in $\\mathcal{M}^p$. That's what Exercises 3 and 4\n",
    "from the [homework on Sobolev models and finite-dimensional approximation](https://machine-learning-theory.github.io/assets/homework/sobolev-models-homework.pdf) are about. Let's review them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Approximation Error \n",
    "\n",
    "### Non-Uniform Approximation\n",
    "Let's start with non-uniform approximation. If we have a function $m(x)=\\sum_{j=0}^{\\infty} m_j \\phi_j(x)$ and we truncate the series after $N$ terms, we get an approximation $m_N(x)=\\sum_{j=0}^{N-1} m_j \\phi_j(x)$. Exercise 3 asks you to prove that this is the best approximation to $m$ spanned by $\\phi_0, \\ldots, \\phi_{N-1}$ and calculate the error of this approximation. Let's do both at once by looking at the squared error of an arbitrary function $a_N = \\sum_{j=0}^{N-1} a_j \\phi_j$ spanned by this truncated basis.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\lVert m - a_N \\right\\rVert^2 \n",
    "&= \\left\\lVert \\sum_{j=0}^{\\infty} m_j \\phi_j - \\sum_{j=0}^{N-1} a_j \\phi_j \\right\\rVert^2 \\\\\n",
    "&= \\left\\lVert \\sum_{j=0}^{N-1} (m_j - a_j) \\phi_j + \\sum_{j=N}^{\\infty} m_j \\phi_j \\right\\rVert^2 \n",
    "\\end{aligned}\n",
    "$$\n",
    "We can simplify this by observing that, because our basis functions are orthonormal, the squared norm of a linear combination of them is just the sum of the squared coefficients.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\lVert \\sum_{j=0}^{\\infty} b_j \\phi_j \\right\\rVert^2 \n",
    "&= \\left\\langle \\sum_{j=0}^{\\infty} b_j \\phi_j, \\ \\sum_{k=0}^{\\infty} b_k \\phi_k \\right\\rangle \\\\\n",
    "&= \\sum_{j=0}^{\\infty}\\sum_{k=0}^{\\infty} b_j b_k \\left\\langle \\phi_j, \\phi_k \\right\\rangle \\\\\n",
    "&= \\sum_{j=0}^{\\infty} b_k^2 \\quad \\text{because} \\quad \\left\\langle \\phi_j, \\phi_k \\right\\rangle = \\begin{cases} 1 & \\text{ if } j=k \\\\ 0 & \\text{ otherwise } \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Applying this to our approximation error, we get this.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\lVert m - a_N \\right\\rVert^2 \n",
    "&= \\sum_{j=0}^{N-1} (m_j - a_j)^2 + \\sum_{j=N}^{\\infty} m_j^2 \n",
    "\\end{aligned}\n",
    "$$\n",
    "When we use the truncated fourier series, i.e. when we take $a_j = m_j$ for $j=0 \\ldots N-1$,\n",
    "this error is $\\sum_{j=N}^{\\infty} m_j^2$. And because our error for arbitrary $a_0 \\ldots a_{N-1}$ is the sum of this quantity and another sum of squares, we can do no better.\n",
    "\n",
    "### Uniform Approximation\n",
    "\n",
    "How big does this approximation error get for functions $m \\in \\mathcal{M}^p$ to approximate? For the sake of intuition, \n",
    "let's start by looking for the hardest 'one-term function' $m_j \\phi_j \\in \\mathcal{M}^p$ to approximate. We can approximate the ones with $j < N$ perfectly, so it has to be one of the others. And among these, it's the one with the largest coefficient $m_j$. Because the model dictates that the coefficients satisfy $m_j^2 \\lambda_j^{p} \\le B^2$ (i.e. $\\lvert m_j\\rvert \\le \\lambda_j^{-p/2} B$), that's the one where $\\lambda_j$ is smallest, and because the sequence $\\lambda_0, \\lambda_1, \\ldots$ is increasing, that's $\\lambda_{N}$. The maximal error over all 'one-term functions' is $\\lambda_j^{-p/2} B$.\n",
    "\n",
    "This turns out to be the maximal error for all functions $m \\in \\mathcal{M}^p$, too. Thinking of the sum $\\sum_{j=N}^{\\infty}m_j^2$ as the dot product between two sequences $\\lambda_N^p m_N^2, \\lambda_{N+1}^p m_{N+1}^2, \\ldots$ and $\\lambda_{N}^p, \\lambda_{N+1}^p, \\ldots$, we can prove it using HÃ¶lder's inequality.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{j=N}^{\\infty} m_j^2 \n",
    "&= \\sum_{j=N}^{\\infty} \\lambda_j^{p} m_j^2 \\times \\lambda_j^{-p}  \\\\\n",
    "&= \\left\\langle \\lambda_N^p m_N^2, \\lambda_{N+1}^p m_{N+1}^2, \\ldots \\ , \\ \\lambda_{N}^{-p}, \\lambda_{N+1}^{-p}, \\ldots  \\right\\rangle_2 \\\\\n",
    "&\\le \\left\\lVert \\lambda_N^p m_N^2, \\lambda_{N+1}^p m_{N+1}^2, \\ldots \\right\\rVert_1 \\times \\left\\lVert \\lambda_N^{-p}, \\lambda_{N+1}^{-p}, \\ldots \\right\\rVert_\\infty \\\\\n",
    "&= \\sum_{j=N}^{\\infty} \\lambda_j^{-p} m_j^2 \\lambda_j^{p} \\times \\max_{j \\ge N} \\lambda_j^{-p} \\\\\n",
    "&\\le B^2 \\times \\lambda_N^{-p} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we want to ensure this maximal error is less than $\\epsilon$, then we want to choose $N$ so that $\\lambda_N^{-p} \\le \\epsilon^2/B^2$. In the specific case we're considering here, in which $\\lambda_j = \\pi^2 j^2$, this means we want to choose\n",
    "the smallest $N$ with $N^{-2p} \\le \\pi^{2p} \\epsilon^2/B^2$ or equivalently the smallest $N \\ge \\pi^{-1} (B/\\epsilon)^{1/p}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Use CVXR to implement the approximate Sobolev regression, \n",
    "$$ \n",
    "\\hat\\mu = \\operatorname*{argmin}_{m \\in \\mathcal{M}^{p,N}} \\frac{1}{n}\\sum_{i=1}^n \\{Y_i - m(X_i)\\}^2 \n",
    "$$\n",
    "choosing $N$ so that maximal approximation error is less than $\\epsilon=.001$. \n",
    "\n",
    "Here's a template. In it, I use the function `outer` to compute a matrix $\\Phi$ with $\\Phi_{ij} = \\phi_j(X_i)$. You'll want to make some changes in lines 2-13 and 26. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sobolevreg = function(X,Y, p=1, B=1, epsilon=.001){\n",
    "  N = ceiling((1/pi)*(B/epsilon)^(1/p))\n",
    "  Phi = outer(X, 0:(N-1), function(x,j) cos(pi*j*x)) \n",
    "  lambda = pi^2 * (0:(N-1))^(2*p)\n",
    "  # specify the parameters and constraints.\n",
    "  b = Variable(N)\n",
    "  m = Phi %*% b\n",
    "  mse = sum((Y - m)^2)/length(Y) \n",
    "  constraints = list(sum(lambda * b^2) <= B^2)\n",
    "\n",
    "  # solve and extract the solution\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  beta.hat = solved$getValue(b)\n",
    "  \n",
    "  # package up the results in a model object and return it\n",
    "  model = list(beta.hat=beta.hat, p=p, input=list(X=X, Y=Y)) \n",
    "  attr(model, \"class\") = \"sobolevreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "predict.sobolevreg = function(model, newdata=model$input) { \n",
    "  x = newdata$X\n",
    "  beta.hat = model$beta.hat\n",
    "  N = length(beta.hat)\n",
    "  Phi.x = outer(x, 0:(N-1), function(x,j) cos(pi*j*x)) \n",
    "  \n",
    "  Phi.x %*% beta.hat \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a quick test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = function(x) { x^2*sin(pi*x) }\n",
    "X = seq(0,1,by=.01)\n",
    "Y = mu(X) + .1*rnorm(length(X))\n",
    "\n",
    "model1 = sobolevreg(X,Y,p=1, B=1)\n",
    "model2 = sobolevreg(X,Y,p=2, B=1)\n",
    "ggplot() + \n",
    "    geom_line(aes(x=X, y=mu(X)), color='black') + \n",
    "    geom_point(aes(x=X, y=Y), alpha=.2) + \n",
    "    geom_line(aes(x=X, y=predict(model1)), color='blue') + \n",
    "    geom_line(aes(x=X, y=predict(model2)), color='red')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Let's take a minute to build our intuition about what Sobolev smoothness\n",
    "means by comparing the Sobolev seminorm to others we've been using.\n",
    "Consider the total variation, ($p=1$) Sobolev, and Lipschitz seminorms\n",
    "on functions on $[0,1]$. \n",
    "$$ \n",
    "\\rho_{TV}(m) = \\int_0^1 \\lvert m'(x) \\rvert dx, \\quad \n",
    "\\rho_{-\\Delta}(m) = \\sqrt{\\int_{0}^1 \\lvert m'(x)\\rvert^2 dx}, \\quad \n",
    "\\rho_{Lip}(m) = \\max_{x \\in [0,1]} \\lvert m'(x)\\rvert.  \n",
    "$$\n",
    "\n",
    "::: exercise\n",
    "1.  What are these for the polynomial functions $m(x)=x^k$?\n",
    "2.  Name a function for which these are all equal.\n",
    "3.  Name a function for which none of these are finite.\n",
    "4.  Name a function for which the first is finite and the others are\n",
    "    not.\n",
    "5.  Name a function for which the first two are finite and the last is\n",
    "    not.\n",
    ":::\n",
    "\n",
    "Go ahead and swap these signals into the block above and see how well the Sobolev model fits them.\n",
    "Try out different values of $p$ and $B$ to see how it changes the fit. And compare to Lipschitz and/or bounded variation regression, too.\n",
    "\n",
    "Here are a couple more signals to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## polynomial of order k normalized to have p=1 sobolev norm=1\n",
    "mu = function(x,k=2) { abs(x)^k * sqrt(2*k-1)/k } \n",
    "## cosine of period 2/k normalized to have p=p sobolev norm=1\n",
    "mu =  function(x, k=3, p=2) { sqrt(2)*cos(pi*k*x)/(pi * k)^p }\n",
    "## step function\n",
    "mu = function(x) {  .5*(x > 1/3) + .4*(x > 2/3) }\n",
    "## step line\n",
    "mu = function(x) { (x-.5)*(x >= .5) }\n",
    "## damped high-frequency oscillations near 0\n",
    "mu = function(x) { x*sin(pi/x)/2 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "To do our comparisons, we'll need regression code for these other methods. Here's code copied from previous labs and homeworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## A function we'll want. unique with an inverse mapping.\n",
    "\n",
    "invert.unique = function(x) { \n",
    "  o = order(x)\n",
    "  dup = duplicated(x[o])\n",
    "  inverse = rep(NA, length(x))\n",
    "  inverse[o] = cumsum(!dup)\n",
    "  list(elements=o[!dup], inverse=inverse)\n",
    "}\n",
    "\n",
    "a = c(1,2,3,3,4,5,5,5,6)\n",
    "unique.a=invert.unique(a)\n",
    "stopifnot(a[unique.a$elements][unique.a$inverse] == a)\n",
    "\n",
    "## Fitting Functions\n",
    "\n",
    "monotonereg = function(X, Y, decreasing = FALSE) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list(if (decreasing) { diff(m) <= 0 } else { diff(m) >= 0 })\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.monotonereg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"monotonereg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "convexreg = function(X, Y, concave = FALSE, monotone = NULL) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  # Interpretation (rearrange): secant slopes are increasing\n",
    "  uX = X[unique.X$elements]\n",
    "  ii = 1:(n-2)\n",
    "  constraints = \n",
    "    list(((m[ii+1]-m[ii])  * (uX[ii+2]-uX[ii]) - \n",
    "          (m[ii+2]-m[ii]) *  (uX[ii+1]-uX[ii])) * (-1)^concave <= 0)\n",
    "  if(!is.null(monotone)) { \n",
    "       decreasing = monotone == 'decreasing'\n",
    "       constraints = c(constraints, diff(m) * (-1)^decreasing >= 0)\n",
    "  }\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.convexreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, input = input)\n",
    "  attr(model, \"class\") = \"convexreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "bvreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  constraints = list( sum(abs(diff(m))) <= B )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.bvreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"bvreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "lipreg = function(X, Y, B=1) {\n",
    "  # Step 0.\n",
    "  # We check that the inputs satisfy our assumptions.\n",
    "  stopifnot(length(X) == length(Y))\n",
    "  input = list(X=X, Y=Y)\n",
    "  n = length(X)\n",
    "  # and find the unique elements of X and the inverse mapping\n",
    "  unique.X = invert.unique(X)\n",
    "\n",
    "  # Step 1.\n",
    "  # We tell CVXR we're thinking about a vector of unknowns m in R^p.\n",
    "  m = Variable(length(unique.X$elements))\n",
    "  # and permute and duplicate these into a vector mX with n elements in correspondence with (X_1,Y_1)...(X_n,Y_n)\n",
    "  mX = m[unique.X$inverse]\n",
    "\n",
    "  # Step 2.\n",
    "  # We tell CVXR that we're interested in mean squared error.\n",
    "  mse = sum((Y - mX)^2 / n)\n",
    "\n",
    "  # Step 3.\n",
    "  # We specify our constraints.\n",
    "  uX = X[unique.X$elements]\n",
    "  constraints = list( abs(diff(m)) <= B * diff(uX) )\n",
    "\n",
    "  # Step 4.\n",
    "  # We ask CVXR to minimize mean squared error subject to our constraints.\n",
    "  # And we ask for vector mu.hat that does it.\n",
    "  solved = solve(Problem(Minimize(mse), constraints))\n",
    "  mu.hat = solved$getValue(m)\n",
    "\n",
    "  # Step 5: a little boilerplate to make it idiomatic R.\n",
    "  # 1. we record the unique levels of X and mu.hat, in correspondence and sorted in increasing order of X, in a list. We also record the input data. \n",
    "  # 2. we assign that list a class, so R knows predict should delegate to predict.lipreg\n",
    "  # 3. we return the list\n",
    "  model = list(X = X[unique.X$elements], mu.hat = mu.hat, B=B, input = input)\n",
    "  attr(model, \"class\") = \"lipreg\"\n",
    "  model\n",
    "}\n",
    "\n",
    "## Prediction Functions\n",
    "\n",
    "# make predictions based on piecewise-constant interpolation\n",
    "# we use the curve that jumps at each observation and is otherwise constant\n",
    "# that is, if X[1] < X[2] < ..., \n",
    "#   mu.hat(x) for x between X[k] and X[k+1] is mu.hat(X[k])   [case 1]\n",
    "#             for x > X[k]  is mu.hat(X[k])                   [case 2]\n",
    "#             for x < X[1]  is mu.hat(X[1])                   [case 3]\n",
    "predict.piecewise.constant = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  Y = model$mu.hat; X=model$X; x=newdata$X\n",
    "  # for each new data point x in newdata$X, \n",
    "  # find the closest observed X[k] left of x\n",
    "  # i.e., the largest k for which X[k] <= x \n",
    "  # this covers cases 1 and 2\n",
    "  # i will be a vector of these numbers k, with one for each x in newdata$X\n",
    "  i = findInterval(x, X) \n",
    "  # if there is no X[k] < x, findInterval tells us k=0\n",
    "  # to cover case 3, we want X[k] for k=1 when this happens.\n",
    "  i[i==0] = 1\n",
    "  # report the values of mu.hat(X[k]), one for each x\n",
    "  Y[i]\n",
    "}\n",
    "\n",
    "predict.piecewise.linear = function(model, newdata=data.frame(X=model$input$X)) {\n",
    "  Y = model$mu.hat; X=model$X; x=newdata$X; n = length(X) \n",
    "  # for each new data point x[k]\n",
    "  # find the closest observed X[i[k]] left of x[k]\n",
    "  # i.e., i[k] is the largest integer i for which X[i] <= x[k] \n",
    "  i = findInterval(newdata$X, X) \n",
    "  # If there is no X[i] < x[k], findInterval sets i[k]=0\n",
    "  #  and we'll want to act as if we'd gotten 1 so we use the\n",
    "  #  line through (X[1], Y[1])  and (X[2], Y[2])\n",
    "  # If that k is n, we'll want to act as if we'd gotten n-1 so we use \n",
    "  #  the line through (X[n-1], Y[n-1])  and (X[n], Y[n])\n",
    "  i[i==0] = 1; i[i==n] = n-1\n",
    "  # make a prediction using the formula y - y0 = (x-x0) * slope \n",
    "  Y[i] + (x-X[i]) * (Y[i+1]-Y[i])/(X[i+1]-X[i])\n",
    "}\n",
    "\n",
    "predict.monotonereg = predict.piecewise.constant\n",
    "predict.convexreg = predict.piecewise.linear\n",
    "predict.bvreg = predict.piecewise.constant\n",
    "predict.lipreg = predict.piecewise.linear\n",
    "\n",
    "## Conveniences\n",
    "\n",
    "prediction.function = function(model) { \n",
    "  if(class(model)=='function') { model }\n",
    "  else { function(x) { predict(model, newdata=data.frame(X=x)) } }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Now for the comparisons themselves. I'll start with the signal in the 'quick test' block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "models = list(bv  = \\(X,Y)bvreg(X,Y,B=1),\n",
    "              lip = \\(X,Y)lipreg(X,Y,B=1),\n",
    "              sobolev1 = \\(X,Y)sobolevreg(X,Y, B=1.5, p=1),\n",
    "              sobolev2 = \\(X,Y)sobolevreg(X,Y, B=1, p=2))\n",
    "\n",
    "plot.predictions = function(X,Y,models) {\n",
    "  x = seq(-1,1,by=.001)\n",
    "  predictions = models |> map(function(reg) { \n",
    "    model = reg(X,Y)\n",
    "    muhat.x = predict(model, newdata=data.frame(X=x))\n",
    "    data.frame(x=x, y=muhat.x)\n",
    "  }) |> list_rbind(names_to = \"model\")\n",
    "\n",
    "  reflected = function(f) { \\(x) ifelse(x >= 0, f(x), f(-x)) }\n",
    "  ggplot() + geom_line(aes(x=x, y=reflected(mu)(x)), alpha=.2, linewidth=1) + \n",
    "             geom_point(aes(x=X, y=Y), alpha=.1) + \n",
    "             geom_line(aes(x=x, y=y, color=model), alpha=.8, data=predictions) + \n",
    "             coord_cartesian(xlim=c(0,1), ylim=range(Y)) + theme(legend.position = \"bottom\") + \n",
    "             labs(x=\"\", y=\"\")\n",
    "}\n",
    "\n",
    "mu = function(x) { x^2*sin(pi*x) }\n",
    "X = seq(0,1,by=.01)\n",
    "Y = mu(X) + .1*rnorm(length(X))\n",
    "plot.predictions(X,Y,models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Here's an interesting one: $\\mu(x)=x$. Even though the second derivative $\\mu''$ is zero, the second order Sobolev model\n",
    "doesn't fit it well near $x=0$. You can even get rid of the noise entirely. What's going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mu = function(x) { x }\n",
    "X = seq(0,1,by=.01)\n",
    "epsilon = .1*rnorm(length(X))\n",
    "plot.predictions(X,mu(X)+epsilon,models)\n",
    "plot.predictions(X,mu(X),models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "It's a reflection thing. When you take a look at what's going on to the left of $x=0$ when we extend $\\mu(x)$ by reflection,\n",
    "it makes a little more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = .1*rnorm(length(X))\n",
    "plot.predictions(X,mu(X)+epsilon,models) + coord_cartesian(xlim=c(-1,1))\n",
    "plot.predictions(X,mu(X),models) + coord_cartesian(xlim=c(-1,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
